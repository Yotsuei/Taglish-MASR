{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torchaudio\n",
    "import pandas as pd\n",
    "from transformers import Wav2Vec2ForCTC, WavLMForCTC, WhisperForConditionalGeneration, Wav2Vec2Processor, WhisperProcessor, Wav2Vec2CTCTokenizer, Wav2Vec2FeatureExtractor\n",
    "import torch\n",
    "import jiwer\n",
    "import warnings\n",
    "import numpy as np\n",
    "from google.colab import drive  # For Google Drive mounting in Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set dataset paths in Google Drive\n",
    "audio_dir = \"/content/drive/MyDrive/path/to/common-voice/clips\"  # Update with your actual path\n",
    "tsv_file = \"/content/drive/MyDrive/path/to/common-voice/validated.tsv\"  # Update with your actual path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore specific warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\".*transcription using a multilingual Whisper will default to language detection.*\")\n",
    "warnings.filterwarnings(\"ignore\", message=\".*Passing a tuple of `past_key_values` is deprecated.*\")\n",
    "warnings.filterwarnings(\"ignore\", message=\".*The attention mask is not set and cannot be inferred from input.*\")\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model and processor parameters\n",
    "models = {\n",
    "    \"wavlm\": {\n",
    "        \"model\": None,\n",
    "        \"processor\": None,\n",
    "    },\n",
    "    \"whisper\": {\n",
    "        \"model\": None,\n",
    "        \"processor\": None,\n",
    "    },\n",
    "    \"wav2vec2\": {\n",
    "        \"model\": None,\n",
    "        \"processor\": None,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"\\nUsing device: {device}\")  # Print the device being used\n",
    "print(\"\\n\" + \"=\" * 50 + \"\\n\")  # Divider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load models and processors\n",
    "def load_models():\n",
    "    try:\n",
    "        print(\"Loading models and processors...\\n\")\n",
    "        for model_name in models.keys():\n",
    "            print(f\"Loading {model_name}...\\n\")\n",
    "            try:\n",
    "                if model_name == \"wav2vec2\":\n",
    "                    models[model_name][\"model\"] = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-large-960h\").to(device)\n",
    "                    models[model_name][\"processor\"] = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-large-960h\")\n",
    "                elif model_name == \"wavlm\":\n",
    "                    models[model_name][\"model\"] = WavLMForCTC.from_pretrained(\"patrickvonplaten/wavlm-libri-clean-100h-large\").to(device)\n",
    "                    models[model_name][\"processor\"] = Wav2Vec2Processor.from_pretrained(\"patrickvonplaten/wavlm-libri-clean-100h-large\")\n",
    "                elif model_name == \"whisper\":\n",
    "                    models[model_name][\"model\"] = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-large\").to(device)\n",
    "                    models[model_name][\"processor\"] = WhisperProcessor.from_pretrained(\"openai/whisper-large\")\n",
    "                print(f\"\\n{model_name} loaded successfully.\\n\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {model_name}: {e}\\n\")\n",
    "        print(\"All models and processors loaded.\\n\\n\" + \"=\" * 50 + \"\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading models: {e}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Common Voice dataset (TSV and mp3)\n",
    "def load_common_voice_data(tsv_file, audio_dir, max_samples=100):\n",
    "    audio_files = []\n",
    "    transcripts = []\n",
    "    count = 0\n",
    "\n",
    "    try:\n",
    "        print(\"Loading dataset...\\n\\n\" + \"=\" * 50 + \"\\n\")\n",
    "        df = pd.read_csv(tsv_file, sep='\\t').sample(frac=1).reset_index(drop=True)  # Shuffle rows\n",
    "\n",
    "        for index, row in df.iterrows():\n",
    "            audio_file = row['path'] if row['path'].endswith(\".mp3\") else row['path'] + \".mp3\"\n",
    "            transcript = row['sentence']  # Extract transcript from 'sentence' column\n",
    "\n",
    "            audio_files.append(os.path.join(audio_dir, audio_file))\n",
    "            transcripts.append(transcript)\n",
    "            count += 1\n",
    "\n",
    "            if count >= max_samples:\n",
    "                print(f\"Finished loading {count} audio files and transcripts from dataset.\\n\\n\" + \"=\" * 50 + \"\\n\")\n",
    "                break\n",
    "\n",
    "        return audio_files, transcripts\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading Common Voice data: {e}\\n\")\n",
    "        return [], []  # Return empty lists on error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate evaluation metrics\n",
    "def calculate_metrics(reference, hypothesis):\n",
    "    reference, hypothesis = reference.lower(), hypothesis.lower()\n",
    "    reference_words, hypothesis_words = reference.split(), hypothesis.split()\n",
    "\n",
    "    wer = jiwer.wer(reference, hypothesis)\n",
    "    cer = jiwer.cer(reference, hypothesis)\n",
    "    true_positives = sum(1 for word in hypothesis_words if word in reference_words)\n",
    "    false_positives = len(hypothesis_words) - true_positives\n",
    "    false_negatives = len(reference_words) - true_positives\n",
    "\n",
    "    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "    recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "    f1_score = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    correct_predictions = sum(1 for i in range(min(len(reference_words), len(hypothesis_words))) if reference_words[i] == hypothesis_words[i])\n",
    "    accuracy = correct_predictions / len(reference_words) if reference_words else 0\n",
    "\n",
    "    return {\n",
    "        \"wer\": wer,\n",
    "        \"cer\": cer,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1_score\": f1_score,\n",
    "        \"accuracy\": accuracy\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad or truncate audio input to a target length\n",
    "def pad_or_truncate(array, target_length):\n",
    "    current_length = array.shape[1]\n",
    "    if current_length > target_length:\n",
    "        return array[:, :target_length]\n",
    "    elif current_length < target_length:\n",
    "        pad_width = ((0, 0), (0, target_length - current_length))\n",
    "        return np.pad(array, pad_width, mode='constant')\n",
    "    else:\n",
    "        return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate a model on Common Voice data\n",
    "def evaluate_model(model, processor, audio_files, transcripts):\n",
    "    results, total_loss = {}, 0\n",
    "    last_transcription, last_audio_file = \"\", \"\"\n",
    "    total_metrics = {\"wer\": 0, \"cer\": 0, \"precision\": 0, \"recall\": 0, \"f1_score\": 0, \"accuracy\": 0}\n",
    "    num_samples = len(audio_files)\n",
    "\n",
    "    try:\n",
    "        print(f\"Evaluating {model.__class__.__name__}...\\n\\n\" + \"=\" * 50 + \"\\n\")\n",
    "\n",
    "        for i, audio_file in enumerate(audio_files):\n",
    "            try:\n",
    "                audio, sample_rate = torchaudio.load(audio_file)\n",
    "                if sample_rate != 16000:\n",
    "                    audio = torchaudio.transforms.Resample(sample_rate, 16000)(audio)\n",
    "\n",
    "                if isinstance(model, WhisperForConditionalGeneration):\n",
    "                    input_features = processor(audio.squeeze().numpy(), sampling_rate=16000, return_tensors=\"pt\").input_features.to(device)\n",
    "                    with torch.no_grad():\n",
    "                        output = model.generate(input_features, language='en')\n",
    "                        transcription = processor.batch_decode(output, skip_special_tokens=True)[0]\n",
    "                else:\n",
    "                    inputs = processor(audio.squeeze().numpy(), sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
    "                    input_values = torch.tensor(pad_or_truncate(inputs.input_values.cpu().numpy(), 200000)).to(device)\n",
    "                    with torch.no_grad():\n",
    "                        logits = model(input_values).logits\n",
    "                        predicted_ids = torch.argmax(logits, dim=-1)\n",
    "                        transcription = processor.batch_decode(predicted_ids)[0]\n",
    "\n",
    "                metrics = calculate_metrics(transcripts[i], transcription)\n",
    "                for key in total_metrics:\n",
    "                    total_metrics[key] += metrics[key]\n",
    "\n",
    "                last_audio_file, last_transcription = audio_file, transcription\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error evaluating file {audio_file}: {e}\\n\\n\" + \"=\" * 50 + \"\\n\")\n",
    "\n",
    "        print(f\"Finished evaluating {model.__class__.__name__}.\\n\\n\" + \"=\" * 50 + \"\\n\")\n",
    "        avg_metrics = {key: value / num_samples for key, value in total_metrics.items()} if num_samples > 0 else {key: 0 for key in total_metrics}\n",
    "        results = avg_metrics\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error evaluating model: {e}\\n\")\n",
    "\n",
    "    results['last_transcription'], results['last_audio_file'] = last_transcription, last_audio_file\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function to run evaluation\n",
    "if __name__ == \"__main__\":\n",
    "    load_models()  # Load models and processors\n",
    "    audio_files, transcripts = load_common_voice_data(tsv_file, audio_dir, max_samples=100)\n",
    "\n",
    "    results = {}\n",
    "    for model_name in models.keys():\n",
    "        results[model_name] = evaluate_model(models[model_name][\"model\"], models[model_name][\"processor\"], audio_files, transcripts)\n",
    "\n",
    "    print(f\"\\nFinal evaluation results: {results}\\n\\n\" + \"=\" * 50 + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "proj-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
