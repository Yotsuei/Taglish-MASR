{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vuG8NFgwl9WT",
        "outputId": "98cf049c-3d12-4131-cbb6-2d5d4d9fa18c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.5.0+cu121)\n",
            "Requirement already satisfied: jiwer in /usr/local/lib/python3.10/dist-packages (3.0.5)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.0+cu121)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: click<9.0.0,>=8.1.3 in /usr/local/lib/python3.10/dist-packages (from jiwer) (8.1.7)\n",
            "Requirement already satisfied: rapidfuzz<4,>=3 in /usr/local/lib/python3.10/dist-packages (from jiwer) (3.10.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n"
          ]
        }
      ],
      "source": [
        "!pip install torchaudio jiwer torch transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "T4-ZmUZsl9WT"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torchaudio\n",
        "import pandas as pd\n",
        "from transformers import Wav2Vec2ForCTC, WavLMForCTC, WhisperForConditionalGeneration, Wav2Vec2Processor, WhisperProcessor, Wav2Vec2CTCTokenizer, Wav2Vec2FeatureExtractor\n",
        "import torch\n",
        "import jiwer\n",
        "import warnings\n",
        "import numpy as np\n",
        "from google.colab import drive  # For Google Drive mounting in Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BeLDugsLl9WU",
        "outputId": "7fbca61f-de2a-4f3f-f1a3-2811fbc47876"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "-RLVLKW6l9WV"
      },
      "outputs": [],
      "source": [
        "# Set dataset paths in Google Drive\n",
        "audio_dir = \"/content/drive/Shareddrives/CS307-Thesis/Dataset/common-voice/clips\"  # Update with your actual path\n",
        "tsv_file = \"/content/drive/Shareddrives/CS307-Thesis/Dataset/common-voice/validated.tsv\"  # Update with your actual path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "QhA3yEVhl9WV"
      },
      "outputs": [],
      "source": [
        "# Ignore specific warnings\n",
        "warnings.filterwarnings(\"ignore\", message=\".*transcription using a multilingual Whisper will default to language detection.*\")\n",
        "warnings.filterwarnings(\"ignore\", message=\".*Passing a tuple of `past_key_values` is deprecated.*\")\n",
        "warnings.filterwarnings(\"ignore\", message=\".*The attention mask is not set and cannot be inferred from input.*\")\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "4NYPK0ojl9WV"
      },
      "outputs": [],
      "source": [
        "# Define model and processor parameters\n",
        "models = {\n",
        "    \"wavlm\": {\n",
        "        \"model\": None,\n",
        "        \"processor\": None,\n",
        "    },\n",
        "    \"whisper\": {\n",
        "        \"model\": None,\n",
        "        \"processor\": None,\n",
        "    },\n",
        "    \"wav2vec2\": {\n",
        "        \"model\": None,\n",
        "        \"processor\": None,\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PtWAw0zZl9WV",
        "outputId": "46ea12c5-283d-4f50-f702-a16315536e65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Using device: cuda\n",
            "\n",
            "==================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"\\nUsing device: {device}\")  # Print the device being used\n",
        "print(\"\\n\" + \"=\" * 50 + \"\\n\")  # Divider"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "ZX4--Udel9WV"
      },
      "outputs": [],
      "source": [
        "# Load models and processors\n",
        "def load_models():\n",
        "    try:\n",
        "        print(\"Loading models and processors...\\n\")\n",
        "        for model_name in models.keys():\n",
        "            print(f\"Loading {model_name}...\\n\")\n",
        "            try:\n",
        "                if model_name == \"wav2vec2\":\n",
        "                    models[model_name][\"model\"] = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-large-960h\").to(device)\n",
        "                    models[model_name][\"processor\"] = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-large-960h\")\n",
        "                elif model_name == \"wavlm\":\n",
        "                    models[model_name][\"model\"] = WavLMForCTC.from_pretrained(\"patrickvonplaten/wavlm-libri-clean-100h-large\").to(device)\n",
        "                    models[model_name][\"processor\"] = Wav2Vec2Processor.from_pretrained(\"patrickvonplaten/wavlm-libri-clean-100h-large\")\n",
        "                elif model_name == \"whisper\":\n",
        "                    models[model_name][\"model\"] = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-large\").to(device)\n",
        "                    models[model_name][\"processor\"] = WhisperProcessor.from_pretrained(\"openai/whisper-large\")\n",
        "                print(f\"\\n{model_name} loaded successfully.\\n\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading {model_name}: {e}\\n\")\n",
        "        print(\"All models and processors loaded.\\n\\n\" + \"=\" * 50 + \"\\n\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading models: {e}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "MOkyQn4el9WW"
      },
      "outputs": [],
      "source": [
        "# Load Common Voice dataset (TSV and mp3)\n",
        "def load_common_voice_data(tsv_file, audio_dir, max_samples=1000):\n",
        "    audio_files = []\n",
        "    transcripts = []\n",
        "    count = 0\n",
        "\n",
        "    try:\n",
        "        print(\"Loading dataset...\\n\\n\" + \"=\" * 50 + \"\\n\")\n",
        "        df = pd.read_csv(tsv_file, sep='\\t').sample(frac=1).reset_index(drop=True)  # Shuffle rows\n",
        "\n",
        "        for index, row in df.iterrows():\n",
        "            audio_file = row['path'] if row['path'].endswith(\".mp3\") else row['path'] + \".mp3\"\n",
        "            transcript = row['sentence']  # Extract transcript from 'sentence' column\n",
        "\n",
        "            audio_files.append(os.path.join(audio_dir, audio_file))\n",
        "            transcripts.append(transcript)\n",
        "            count += 1\n",
        "\n",
        "            if count >= max_samples:\n",
        "                print(f\"Finished loading {count} audio files and transcripts from dataset.\\n\\n\" + \"=\" * 50 + \"\\n\")\n",
        "                break\n",
        "\n",
        "        return audio_files, transcripts\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading Common Voice data: {e}\\n\")\n",
        "        return [], []  # Return empty lists on error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "TRmIa4PDl9WW"
      },
      "outputs": [],
      "source": [
        "# Function to calculate evaluation metrics\n",
        "def calculate_metrics(reference, hypothesis):\n",
        "    reference, hypothesis = reference.lower(), hypothesis.lower()\n",
        "    reference_words, hypothesis_words = reference.split(), hypothesis.split()\n",
        "\n",
        "    wer = jiwer.wer(reference, hypothesis)\n",
        "    cer = jiwer.cer(reference, hypothesis)\n",
        "    true_positives = sum(1 for word in hypothesis_words if word in reference_words)\n",
        "    false_positives = len(hypothesis_words) - true_positives\n",
        "    false_negatives = len(reference_words) - true_positives\n",
        "\n",
        "    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
        "    recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
        "    f1_score = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "    correct_predictions = sum(1 for i in range(min(len(reference_words), len(hypothesis_words))) if reference_words[i] == hypothesis_words[i])\n",
        "    accuracy = correct_predictions / len(reference_words) if reference_words else 0\n",
        "\n",
        "    return {\n",
        "        \"wer\": wer,\n",
        "        \"cer\": cer,\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "        \"f1_score\": f1_score,\n",
        "        \"accuracy\": accuracy\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "SArLONhkl9WW"
      },
      "outputs": [],
      "source": [
        "# Pad or truncate audio input to a target length\n",
        "def pad_or_truncate(array, target_length):\n",
        "    current_length = array.shape[1]\n",
        "    if current_length > target_length:\n",
        "        return array[:, :target_length]\n",
        "    elif current_length < target_length:\n",
        "        pad_width = ((0, 0), (0, target_length - current_length))\n",
        "        return np.pad(array, pad_width, mode='constant')\n",
        "    else:\n",
        "        return array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "choL1Zaol9WW"
      },
      "outputs": [],
      "source": [
        "# Function to evaluate a model on Common Voice data\n",
        "def evaluate_model(model, processor, audio_files, transcripts):\n",
        "    results, total_loss = {}, 0\n",
        "    last_transcription, last_audio_file = \"\", \"\"\n",
        "    total_metrics = {\"wer\": 0, \"cer\": 0, \"precision\": 0, \"recall\": 0, \"f1_score\": 0, \"accuracy\": 0}\n",
        "    num_samples = len(audio_files)\n",
        "\n",
        "    try:\n",
        "        print(f\"Evaluating {model.__class__.__name__}...\\n\\n\" + \"=\" * 50 + \"\\n\")\n",
        "\n",
        "        for i, audio_file in enumerate(audio_files):\n",
        "            try:\n",
        "                audio, sample_rate = torchaudio.load(audio_file)\n",
        "                if sample_rate != 16000:\n",
        "                    audio = torchaudio.transforms.Resample(sample_rate, 16000)(audio)\n",
        "\n",
        "                if isinstance(model, WhisperForConditionalGeneration):\n",
        "                    input_features = processor(audio.squeeze().numpy(), sampling_rate=16000, return_tensors=\"pt\").input_features.to(device)\n",
        "                    with torch.no_grad():\n",
        "                        output = model.generate(input_features, language='en')\n",
        "                        transcription = processor.batch_decode(output, skip_special_tokens=True)[0]\n",
        "                else:\n",
        "                    inputs = processor(audio.squeeze().numpy(), sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
        "                    input_values = torch.tensor(pad_or_truncate(inputs.input_values.cpu().numpy(), 200000)).to(device)\n",
        "                    with torch.no_grad():\n",
        "                        logits = model(input_values).logits\n",
        "                        predicted_ids = torch.argmax(logits, dim=-1)\n",
        "                        transcription = processor.batch_decode(predicted_ids)[0]\n",
        "\n",
        "                metrics = calculate_metrics(transcripts[i], transcription)\n",
        "                for key in total_metrics:\n",
        "                    total_metrics[key] += metrics[key]\n",
        "\n",
        "                last_audio_file, last_transcription = audio_file, transcription\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error evaluating file {audio_file}: {e}\\n\\n\" + \"=\" * 50 + \"\\n\")\n",
        "\n",
        "        print(f\"Finished evaluating {model.__class__.__name__}.\\n\\n\" + \"=\" * 50 + \"\\n\")\n",
        "        avg_metrics = {key: value / num_samples for key, value in total_metrics.items()} if num_samples > 0 else {key: 0 for key in total_metrics}\n",
        "        results = avg_metrics\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error evaluating model: {e}\\n\")\n",
        "\n",
        "    results['last_transcription'], results['last_audio_file'] = last_transcription, last_audio_file\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h8ZdOwPBl9WW",
        "outputId": "be9f21e3-7ae1-40aa-f9a4-9d403f1d826f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading models and processors...\n",
            "\n",
            "Loading wavlm...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at patrickvonplaten/wavlm-libri-clean-100h-large were not used when initializing WavLMForCTC: ['wavlm.encoder.pos_conv_embed.conv.weight_g', 'wavlm.encoder.pos_conv_embed.conv.weight_v']\n",
            "- This IS expected if you are initializing WavLMForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing WavLMForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of WavLMForCTC were not initialized from the model checkpoint at patrickvonplaten/wavlm-libri-clean-100h-large and are newly initialized: ['wavlm.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wavlm.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "wavlm loaded successfully.\n",
            "\n",
            "Loading whisper...\n",
            "\n",
            "\n",
            "whisper loaded successfully.\n",
            "\n",
            "Loading wav2vec2...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at facebook/wav2vec2-large-960h were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n",
            "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-960h and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.masked_spec_embed']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "wav2vec2 loaded successfully.\n",
            "\n",
            "All models and processors loaded.\n",
            "\n",
            "==================================================\n",
            "\n",
            "Loading dataset...\n",
            "\n",
            "==================================================\n",
            "\n",
            "Evaluating WavLMForCTC...\n",
            "\n",
            "==================================================\n",
            "\n",
            "Finished evaluating WavLMForCTC.\n",
            "\n",
            "==================================================\n",
            "\n",
            "Evaluating WhisperForConditionalGeneration...\n",
            "\n",
            "==================================================\n",
            "\n",
            "Finished evaluating WhisperForConditionalGeneration.\n",
            "\n",
            "==================================================\n",
            "\n",
            "Evaluating Wav2Vec2ForCTC...\n",
            "\n",
            "==================================================\n",
            "\n",
            "Finished evaluating Wav2Vec2ForCTC.\n",
            "\n",
            "==================================================\n",
            "\n",
            "\n",
            "Final evaluation results: {'wavlm': {'wer': 0.303509791101032, 'cer': 0.08096808523607901, 'precision': 0.7162629073374025, 'recall': 0.7246446578563365, 'f1_score': 0.7198859702973255, 'accuracy': 0.6401309558243863, 'last_transcription': 'there are no convincing explanations about the origins of the words ofpenia and fenya', 'last_audio_file': '/content/drive/Shareddrives/CS307-Thesis/Dataset/common-voice/clips/common_voice_en_41098431.mp3'}, 'whisper': {'wer': 0.09891112537097936, 'cer': 0.03589546667894927, 'precision': 0.9088614548468562, 'recall': 0.9140582248246484, 'f1_score': 0.9110717949147669, 'accuracy': 0.8724382065622941, 'last_transcription': \" There are no convincing explanations about the origins of the words o'fenya and fenya.\", 'last_audio_file': '/content/drive/Shareddrives/CS307-Thesis/Dataset/common-voice/clips/common_voice_en_41098431.mp3'}, 'wav2vec2': {'wer': 0.28102550328827713, 'cer': 0.08733589741451155, 'precision': 0.7395256310885332, 'recall': 0.751206559053274, 'f1_score': 0.7447243363812637, 'accuracy': 0.6742749950779143, 'last_transcription': 'THERE ARE NO CONVINCING EXPLANATIONS ABOUT THE ORIGINS OF THE WORDS AUFENA AND FENA', 'last_audio_file': '/content/drive/Shareddrives/CS307-Thesis/Dataset/common-voice/clips/common_voice_en_41098431.mp3'}}\n",
            "\n",
            "==================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Main function to run evaluation\n",
        "if __name__ == \"__main__\":\n",
        "    load_models()  # Load models and processors\n",
        "    audio_files, transcripts = load_common_voice_data(tsv_file, audio_dir, max_samples=1000)\n",
        "\n",
        "    results = {}\n",
        "    for model_name in models.keys():\n",
        "        results[model_name] = evaluate_model(models[model_name][\"model\"], models[model_name][\"processor\"], audio_files, transcripts)\n",
        "\n",
        "    print(f\"\\nFinal evaluation results: {results}\\n\\n\" + \"=\" * 50 + \"\\n\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.6"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}