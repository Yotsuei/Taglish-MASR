{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X8S0cMJ9-SuW"
   },
   "outputs": [],
   "source": [
    "!pip install wandb transformers torch torchaudio jiwer scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rxIwC0z4-SuX"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import wandb\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchaudio\n",
    "import logging\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration, get_scheduler\n",
    "from jiwer import wer, cer\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim import AdamW\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "import torch.nn.functional as F\n",
    "from google.colab import drive\n",
    "from typing import Optional, Tuple, Dict\n",
    "import soundfile as sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('training.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioValidationError(Exception):\n",
    "    \"\"\"Custom exception for audio validation errors\"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_audio_file(audio_path: str, min_duration: float = 0.1, max_duration: float = 120.0) -> Tuple[bool, str]:\n",
    "    \"\"\"\n",
    "    Validate audio file existence, format, and quality\n",
    "    \n",
    "    Args:\n",
    "        audio_path: Path to audio file\n",
    "        min_duration: Minimum allowed duration in seconds\n",
    "        max_duration: Maximum allowed duration in seconds\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (is_valid, message)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if not os.path.exists(audio_path):\n",
    "            return False, f\"File does not exist: {audio_path}\"\n",
    "            \n",
    "        if not audio_path.lower().endswith(('.wav', '.mp3', '.flac')):\n",
    "            return False, f\"Unsupported audio format: {audio_path}\"\n",
    "            \n",
    "        # Check audio properties\n",
    "        info = sf.info(audio_path)\n",
    "        duration = info.duration\n",
    "        \n",
    "        if duration < min_duration:\n",
    "            return False, f\"Audio too short ({duration:.2f}s < {min_duration}s): {audio_path}\"\n",
    "            \n",
    "        if duration > max_duration:\n",
    "            return False, f\"Audio too long ({duration:.2f}s > {max_duration}s): {audio_path}\"\n",
    "            \n",
    "        if info.samplerate < 8000:\n",
    "            return False, f\"Sample rate too low ({info.samplerate} Hz): {audio_path}\"\n",
    "            \n",
    "        return True, \"Valid\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        return False, f\"Error validating audio file: {str(e)}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_transcript(transcript: str, min_length: int = 1, max_length: int = 10000) -> Tuple[bool, str]:\n",
    "    \"\"\"\n",
    "    Validate transcript content\n",
    "    \n",
    "    Args:\n",
    "        transcript: Input transcript\n",
    "        min_length: Minimum character length\n",
    "        max_length: Maximum character length\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (is_valid, message)\n",
    "    \"\"\"\n",
    "    if not isinstance(transcript, str):\n",
    "        return False, \"Transcript must be a string\"\n",
    "        \n",
    "    if len(transcript.strip()) < min_length:\n",
    "        return False, \"Transcript too short\"\n",
    "        \n",
    "    if len(transcript) > max_length:\n",
    "        return False, \"Transcript too long\"\n",
    "        \n",
    "    return True, \"Valid\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_timestamps(start_time: float, end_time: float, audio_duration: float) -> Tuple[bool, str]:\n",
    "    \"\"\"\n",
    "    Validate timestamp alignment\n",
    "    \n",
    "    Args:\n",
    "        start_time: Start time in seconds\n",
    "        end_time: End time in seconds\n",
    "        audio_duration: Total audio duration in seconds\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (is_valid, message)\n",
    "    \"\"\"\n",
    "    if start_time < 0:\n",
    "        return False, \"Negative start time\"\n",
    "        \n",
    "    if end_time > audio_duration:\n",
    "        return False, \"End time exceeds audio duration\"\n",
    "        \n",
    "    if start_time >= end_time:\n",
    "        return False, \"Start time must be less than end time\"\n",
    "        \n",
    "    if (end_time - start_time) < 0.1:\n",
    "        return False, \"Segment too short (< 0.1s)\"\n",
    "        \n",
    "    return True, \"Valid\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "id": "MuQ_eFZjG5d2"
   },
   "outputs": [],
   "source": [
    "# Add GPU verification at startup\n",
    "def verify_gpu_status():\n",
    "    \"\"\"Verify GPU availability and PyTorch CUDA configuration\"\"\"\n",
    "    print(\"\\n=== GPU Status Check ===\")\n",
    "    print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"Current Device: {torch.cuda.current_device()}\")\n",
    "        print(f\"Device Name: {torch.cuda.get_device_name()}\")\n",
    "        print(f\"Device Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "        print(f\"PyTorch CUDA Version: {torch.version.cuda}\")\n",
    "    print(\"=====================\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "id": "WoqRKUR_-SuX"
   },
   "outputs": [],
   "source": [
    "# Enhanced configurations\n",
    "config = {\n",
    "    \"tsv_file\": \"/content/drive/Shareddrives/CS307-Thesis/Dataset/single-speaker/validated.tsv\",\n",
    "    \"audio_dir\": \"/content/drive/Shareddrives/CS307-Thesis/Dataset/single-speaker/\",\n",
    "    \"batch_size\": 4,\n",
    "    \"learning_rate\": 5e-5,\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"warmup_steps\": 500,\n",
    "    \"epochs\": 3,\n",
    "    \"max_samples\": 13,\n",
    "    \"checkpoint_interval\": 3,\n",
    "    \"validation_split\": 0.1,\n",
    "    \"max_grad_norm\": 1.0,\n",
    "    \"early_stopping_patience\": 3,\n",
    "    \"mixed_precision\": True,\n",
    "    \"gradient_accumulation_steps\": 4\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "id": "zJObwJQdMw2r"
   },
   "outputs": [],
   "source": [
    "def validate_audio_file(audio_path):\n",
    "    if not os.path.exists(audio_path):\n",
    "        return False, \"File does not exist\"\n",
    "    if not audio_path.endswith(('.wav', '.mp3', '.flac')):\n",
    "        return False, \"Unsupported audio format\"\n",
    "    return True, \"Valid\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HeFoIWaN-SuY"
   },
   "outputs": [],
   "source": [
    "def load_data(tsv_file: str, audio_dir: str, max_samples: Optional[int] = None) -> Tuple[list, list, list, list]:\n",
    "    \"\"\"\n",
    "    Enhanced data loading with validation\n",
    "    \"\"\"\n",
    "    audio_files, transcripts, languages, timestamps = [], [], [], []\n",
    "    error_log = []\n",
    "\n",
    "    # Read TSV file\n",
    "    df = pd.read_csv(tsv_file, sep='\\t')\n",
    "    required_columns = ['path', 'start_time', 'end_time', 'language', 'sentence']\n",
    "\n",
    "    if not all(col in df.columns for col in required_columns):\n",
    "        raise ValueError(f\"TSV file must contain columns: {required_columns}\")\n",
    "\n",
    "    # Shuffle and limit samples if specified\n",
    "    df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    if max_samples:\n",
    "        df = df.head(max_samples)\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        try:\n",
    "            audio_file = row['path']\n",
    "            full_audio_path = os.path.join(audio_dir, audio_file)\n",
    "            \n",
    "            # Validate audio file\n",
    "            is_valid, message = validate_audio_file(full_audio_path)\n",
    "            if not is_valid:\n",
    "                error_log.append(f\"Row {idx}: {message}\")\n",
    "                continue\n",
    "                \n",
    "            # Validate transcript\n",
    "            is_valid, message = validate_transcript(row['sentence'])\n",
    "            if not is_valid:\n",
    "                error_log.append(f\"Row {idx}: {message}\")\n",
    "                continue\n",
    "\n",
    "            # Get audio duration for timestamp validation\n",
    "            audio_info = sf.info(full_audio_path)\n",
    "            audio_duration = audio_info.duration\n",
    "\n",
    "            # Parse and validate timestamps\n",
    "            try:\n",
    "                start_time = float(row['start_time'])\n",
    "                end_time = float(row['end_time'])\n",
    "                \n",
    "                is_valid, message = validate_timestamps(start_time, end_time, audio_duration)\n",
    "                if not is_valid:\n",
    "                    error_log.append(f\"Row {idx}: {message}\")\n",
    "                    continue\n",
    "                    \n",
    "            except ValueError as e:\n",
    "                error_log.append(f\"Row {idx}: Invalid timestamp format - {str(e)}\")\n",
    "                continue\n",
    "\n",
    "            # All validations passed, add to dataset\n",
    "            audio_files.append(full_audio_path)\n",
    "            transcripts.append(row['sentence'])\n",
    "            timestamps.append((start_time, end_time))\n",
    "            languages.append(row['language'])\n",
    "\n",
    "        except Exception as e:\n",
    "            error_log.append(f\"Row {idx}: Unexpected error - {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    # Log validation results\n",
    "    logging.info(f\"Processed {len(df)} rows\")\n",
    "    logging.info(f\"Successfully loaded {len(audio_files)} samples\")\n",
    "    logging.info(f\"Failed to load {len(error_log)} samples\")\n",
    "    \n",
    "    if error_log:\n",
    "        with open('data_loading_errors.log', 'w') as f:\n",
    "            f.write('\\n'.join(error_log))\n",
    "        logging.warning(\"See data_loading_errors.log for detailed error information\")\n",
    "\n",
    "    return audio_files, transcripts, languages, timestamps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientAccumulationTrainer:\n",
    "    \"\"\"Helper class for gradient accumulation training\"\"\"\n",
    "    def __init__(self, model, optimizer, scheduler, scaler, config):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        self.scaler = scaler\n",
    "        self.config = config\n",
    "        self.accumulated_loss = 0\n",
    "        self.steps = 0\n",
    "        \n",
    "    def train_step(self, batch, device):\n",
    "        input_features = batch[\"input_features\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        \n",
    "        # Get forced decoder IDs\n",
    "        forced_decoder_ids = self.model.processor.get_decoder_prompt_ids(\n",
    "            language=[\"en\", \"tl\"],\n",
    "            task=\"transcribe\",\n",
    "            no_timestamps=True\n",
    "        )\n",
    "        \n",
    "        # Mixed precision training step\n",
    "        if self.scaler:\n",
    "            with torch.amp.autocast(device_type='cuda', dtype=torch.float16):\n",
    "                outputs = self.model(\n",
    "                    input_features,\n",
    "                    labels=labels,\n",
    "                    forced_decoder_ids=forced_decoder_ids\n",
    "                )\n",
    "                loss = outputs.loss / self.config[\"gradient_accumulation_steps\"]\n",
    "                \n",
    "            self.scaler.scale(loss).backward()\n",
    "            self.accumulated_loss += loss.item()\n",
    "            \n",
    "            # Update weights if we've accumulated enough gradients\n",
    "            if (self.steps + 1) % self.config[\"gradient_accumulation_steps\"] == 0:\n",
    "                self.scaler.unscale_(self.optimizer)\n",
    "                clip_grad_norm_(self.model.parameters(), self.config[\"max_grad_norm\"])\n",
    "                self.scaler.step(self.optimizer)\n",
    "                self.scaler.update()\n",
    "                self.scheduler.step()\n",
    "                self.optimizer.zero_grad()\n",
    "                \n",
    "        else:\n",
    "            outputs = self.model(\n",
    "                input_features,\n",
    "                labels=labels,\n",
    "                forced_decoder_ids=forced_decoder_ids\n",
    "            )\n",
    "            loss = outputs.loss / self.config[\"gradient_accumulation_steps\"]\n",
    "            loss.backward()\n",
    "            self.accumulated_loss += loss.item()\n",
    "            \n",
    "            if (self.steps + 1) % self.config[\"gradient_accumulation_steps\"] == 0:\n",
    "                clip_grad_norm_(self.model.parameters(), self.config[\"max_grad_norm\"])\n",
    "                self.optimizer.step()\n",
    "                self.scheduler.step()\n",
    "                self.optimizer.zero_grad()\n",
    "                \n",
    "        self.steps += 1\n",
    "        return loss.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelCheckpointer:\n",
    "    \"\"\"Handles model checkpointing based on validation metrics\"\"\"\n",
    "    def __init__(self, save_dir: str, metric_name: str = \"WER\", mode: str = \"min\"):\n",
    "        self.save_dir = save_dir\n",
    "        self.metric_name = metric_name\n",
    "        self.mode = mode\n",
    "        self.best_metric = float('inf') if mode == \"min\" else float('-inf')\n",
    "        self.not_improved_count = 0\n",
    "        \n",
    "    def should_save(self, metrics: Dict) -> bool:\n",
    "        current_metric = metrics.get(self.metric_name)\n",
    "        if current_metric is None:\n",
    "            return False\n",
    "            \n",
    "        if self.mode == \"min\":\n",
    "            is_better = current_metric < self.best_metric\n",
    "        else:\n",
    "            is_better = current_metric > self.best_metric\n",
    "            \n",
    "        if is_better:\n",
    "            self.best_metric = current_metric\n",
    "            self.not_improved_count = 0\n",
    "            return True\n",
    "        else:\n",
    "            self.not_improved_count += 1\n",
    "            return False\n",
    "            \n",
    "    def save_checkpoint(self, model, processor, optimizer, scheduler, epoch: int, metrics: Dict):\n",
    "        checkpoint_dir = os.path.join(self.save_dir, f\"checkpoint_epoch_{epoch}\")\n",
    "        os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "        \n",
    "        # Save model and processor\n",
    "        model.save_pretrained(checkpoint_dir)\n",
    "        processor.save_pretrained(checkpoint_dir)\n",
    "        \n",
    "        # Save training state\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'best_metric': self.best_metric,\n",
    "            'metrics': metrics\n",
    "        }, os.path.join(checkpoint_dir, 'training_state.pt'))\n",
    "        \n",
    "        logging.info(f\"Saved checkpoint at epoch {epoch} with {self.metric_name}: {self.best_metric}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "id": "OvHBaGuX-SuY"
   },
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=3, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.should_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "        elif val_loss > self.best_loss - self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.should_stop = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "id": "Dpw6Yh0KESQG"
   },
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function to handle variable length sequences\n",
    "    \"\"\"\n",
    "    # Filter out any None or empty items\n",
    "    batch = [item for item in batch if item is not None]\n",
    "\n",
    "    if len(batch) == 0:\n",
    "        return {}\n",
    "\n",
    "    # Get maximum lengths\n",
    "    max_input_length = max(item['input_features'].size(1) for item in batch)\n",
    "    max_label_length = max(item['labels'].size(0) for item in batch)\n",
    "\n",
    "    # Initialize padded tensors\n",
    "    batch_size = len(batch)\n",
    "    padded_input_features = torch.zeros(batch_size, 80, max_input_length)\n",
    "    padded_labels = torch.full((batch_size, max_label_length), -100, dtype=torch.long)  # -100 is often used for padding in transformers\n",
    "\n",
    "    # Fill padded tensors\n",
    "    for i, item in enumerate(batch):\n",
    "        # Pad input features\n",
    "        input_features = item['input_features']\n",
    "        length = input_features.size(1)\n",
    "        padded_input_features[i, :, :length] = input_features\n",
    "\n",
    "        # Pad labels\n",
    "        labels = item['labels']\n",
    "        length = labels.size(0)\n",
    "        padded_labels[i, :length] = labels\n",
    "\n",
    "    return {\n",
    "        'input_features': padded_input_features,\n",
    "        'labels': padded_labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "id": "9Rd4eaXD-SuZ"
   },
   "outputs": [],
   "source": [
    "class ProcessData(Dataset):\n",
    "    def __init__(self, audio_files, transcripts, timestamps, processor, languages=None, training=True):\n",
    "        self.audio_files = audio_files\n",
    "        self.transcripts = transcripts\n",
    "        self.timestamps = timestamps\n",
    "        self.processor = processor\n",
    "        self.languages = languages or [\"tl-en\"] * len(audio_files)  # Default to tl-en if not provided\n",
    "        self.training = training\n",
    "        self.debug_stats = {\n",
    "            \"processed\": 0,\n",
    "            \"errors\": 0,\n",
    "            \"error_types\": {}\n",
    "        }\n",
    "\n",
    "        print(f\"Initializing dataset with {len(audio_files)} samples\")\n",
    "\n",
    "        if training:\n",
    "            # Initialize audio augmentation transforms\n",
    "            self.time_stretch = torchaudio.transforms.TimeStretch(fixed_rate=0.98)\n",
    "            self.freq_mask = torchaudio.transforms.FrequencyMasking(freq_mask_param=30)\n",
    "            self.time_mask = torchaudio.transforms.TimeMasking(time_mask_param=100)\n",
    "\n",
    "    def apply_audio_transforms(self, audio):\n",
    "        \"\"\"Apply audio augmentation transforms during training\"\"\"\n",
    "        if not self.training:\n",
    "            return audio\n",
    "\n",
    "        try:\n",
    "            # Convert to complex spectrogram for time stretching\n",
    "            spec = torch.stft(\n",
    "                audio,\n",
    "                n_fft=400,\n",
    "                hop_length=100,\n",
    "                win_length=400,\n",
    "                window=torch.hann_window(400),\n",
    "                return_complex=True\n",
    "            )\n",
    "\n",
    "            # Apply time stretch\n",
    "            spec_stretched = self.time_stretch(spec)\n",
    "\n",
    "            # Convert back to time domain\n",
    "            audio = torch.istft(\n",
    "                spec_stretched,\n",
    "                n_fft=400,\n",
    "                hop_length=100,\n",
    "                win_length=400,\n",
    "                window=torch.hann_window(400)\n",
    "            )\n",
    "\n",
    "            # Apply frequency and time masking\n",
    "            spec = torchaudio.transforms.MelSpectrogram()(audio)\n",
    "            spec = self.freq_mask(spec)\n",
    "            spec = self.time_mask(spec)\n",
    "\n",
    "            return audio\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Audio augmentation failed: {str(e)}\")\n",
    "            return audio\n",
    "\n",
    "    def log_processing_step(self, idx, step, info):\n",
    "        \"\"\"Structured logging for processing steps\"\"\"\n",
    "        print(f\"\\n[Sample {idx}][{step}] {info}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            audio_path = self.audio_files[idx]\n",
    "            transcript = self.transcripts[idx]\n",
    "            start_time, end_time = self.timestamps[idx]\n",
    "\n",
    "            self.log_processing_step(idx, \"Start\", \"Beginning processing\")\n",
    "            self.log_processing_step(idx, \"Info\", f\"Path: {audio_path}\")\n",
    "            self.log_processing_step(idx, \"Info\", f\"Transcript: {transcript}\")\n",
    "            self.log_processing_step(idx, \"Info\", f\"Timestamps: {start_time}-{end_time}\")\n",
    "\n",
    "            # Load audio\n",
    "            audio, sample_rate = torchaudio.load(audio_path)\n",
    "            self.log_processing_step(idx, \"Audio Load\", f\"Shape: {audio.shape}, Sample rate: {sample_rate}\")\n",
    "\n",
    "            # Convert to mono if stereo\n",
    "            if audio.shape[0] > 1:\n",
    "                audio = torch.mean(audio, dim=0, keepdim=True)\n",
    "                self.log_processing_step(idx, \"Mono Convert\", \"Converted stereo to mono\")\n",
    "\n",
    "            # Resample if necessary\n",
    "            if sample_rate != 16000:\n",
    "                resampler = torchaudio.transforms.Resample(sample_rate, 16000)\n",
    "                audio = resampler(audio)\n",
    "                self.log_processing_step(idx, \"Resample\", \"Resampled to 16kHz\")\n",
    "\n",
    "            # Apply timestamps\n",
    "            start_frame = int(start_time * 16000)\n",
    "            end_frame = int(end_time * 16000)\n",
    "            audio = audio[:, start_frame:end_frame]\n",
    "            self.log_processing_step(idx, \"Trim\", f\"Trimmed shape: {audio.shape}\")\n",
    "\n",
    "            # Apply audio transforms if in training mode\n",
    "            if self.training:\n",
    "                audio = self.apply_audio_transforms(audio.squeeze())\n",
    "                audio = audio.unsqueeze(0)\n",
    "                self.log_processing_step(idx, \"Augment\", \"Applied audio transforms\")\n",
    "\n",
    "            # Process audio features\n",
    "            input_features = self.processor(\n",
    "                audio.squeeze().numpy(),\n",
    "                sampling_rate=16000,\n",
    "                return_tensors=\"pt\"\n",
    "            ).input_features\n",
    "            self.log_processing_step(idx, \"Features\", f\"Processed features shape: {input_features.shape}\")\n",
    "\n",
    "            # Process labels\n",
    "            labels = self.processor(\n",
    "                text=transcript,\n",
    "                return_tensors=\"pt\"\n",
    "            ).input_ids.squeeze()\n",
    "            self.log_processing_step(idx, \"Labels\", f\"Processed labels shape: {labels.shape}\")\n",
    "\n",
    "            self.debug_stats[\"processed\"] += 1\n",
    "            return {\n",
    "                \"input_features\": input_features.squeeze(),\n",
    "                \"labels\": labels.long(),\n",
    "                \"transcript\": transcript,\n",
    "                \"language\": self.languages[idx]  # Add language information to the batch\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            self.log_error(type(e).__name__, str(e))\n",
    "            self.log_processing_step(idx, \"Error\", f\"Failed: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def log_error(self, error_type, details):\n",
    "        if error_type not in self.debug_stats[\"error_types\"]:\n",
    "            self.debug_stats[\"error_types\"][error_type] = []\n",
    "        self.debug_stats[\"error_types\"][error_type].append(details)\n",
    "        self.debug_stats[\"errors\"] += 1\n",
    "\n",
    "    def get_stats(self):\n",
    "        return {\n",
    "            \"total_items\": len(self),\n",
    "            \"successfully_processed\": self.debug_stats[\"processed\"],\n",
    "            \"errors\": self.debug_stats[\"errors\"],\n",
    "            \"error_breakdown\": self.debug_stats[\"error_types\"]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "id": "g4tHWUHS-SuZ"
   },
   "outputs": [],
   "source": [
    "def create_dataloaders(audio_files, transcripts, timestamps, processor, config, languages=None):\n",
    "    \"\"\"Create training and validation dataloaders with error checking\"\"\"\n",
    "    # Create full dataset\n",
    "    full_dataset = ProcessData(audio_files, transcripts, timestamps, processor, languages=languages, training=True)\n",
    "\n",
    "    # Calculate split sizes with minimum validation size check\n",
    "    total_samples = len(full_dataset)\n",
    "    val_size = max(int(total_samples * config[\"validation_split\"]), 1)\n",
    "    train_size = total_samples - val_size\n",
    "\n",
    "    print(f\"Splitting dataset: {train_size} training samples, {val_size} validation samples\")\n",
    "\n",
    "    # Split dataset\n",
    "    train_dataset, val_dataset = random_split(\n",
    "        full_dataset,\n",
    "        [train_size, val_size],\n",
    "        generator=torch.Generator().manual_seed(42)\n",
    "    )\n",
    "\n",
    "    # Create dataloaders\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config[\"batch_size\"],\n",
    "        shuffle=True,\n",
    "        num_workers=2,\n",
    "        pin_memory=True,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "\n",
    "    val_dataloader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=config[\"batch_size\"],\n",
    "        shuffle=False,\n",
    "        num_workers=2,\n",
    "        pin_memory=True,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "\n",
    "    return train_dataloader, val_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "id": "YhrMhW5p-SuZ"
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, processor, dataloader, device, log_samples=5):\n",
    "    \"\"\"Enhanced evaluation function with multilingual support and detailed prediction logging\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    batch_sizes = []\n",
    "    samples_logged = 0\n",
    "\n",
    "    # Create dictionaries to store metrics by language\n",
    "    metrics_by_language = {\n",
    "        \"en\": {\"preds\": [], \"labels\": []},\n",
    "        \"tl\": {\"preds\": [], \"labels\": []},\n",
    "        \"tl-en\": {\"preds\": [], \"labels\": []}\n",
    "    }\n",
    "\n",
    "    print(\"\\n=== Starting Evaluation ===\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(dataloader):\n",
    "            if not batch:\n",
    "                print(f\"Batch {batch_idx} is empty, skipping...\")\n",
    "                continue\n",
    "\n",
    "            batch_sizes.append(batch[\"input_features\"].size(0))\n",
    "            input_features = batch[\"input_features\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            # Get language information from batch\n",
    "            languages = batch.get(\"language\", [\"tl-en\"] * input_features.size(0))\n",
    "\n",
    "            try:\n",
    "                # Create forced decoder IDs for both English and Tagalog\n",
    "                forced_decoder_ids = processor.get_decoder_prompt_ids(\n",
    "                    language=[\"en\", \"tl\"],\n",
    "                    task=\"transcribe\",\n",
    "                    no_timestamps=True\n",
    "                )\n",
    "\n",
    "                outputs = model(\n",
    "                    input_features,\n",
    "                    labels=labels,\n",
    "                    forced_decoder_ids=forced_decoder_ids\n",
    "                )\n",
    "\n",
    "                total_loss += outputs.loss.item()\n",
    "\n",
    "                # Generate predictions\n",
    "                generated_ids = model.generate(\n",
    "                    input_features,\n",
    "                    forced_decoder_ids=forced_decoder_ids,\n",
    "                    max_length=256\n",
    "                )\n",
    "\n",
    "                # Decode predictions and labels\n",
    "                decoded_preds = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "                labels_clean = torch.where(labels != -100, labels, processor.tokenizer.pad_token_id)\n",
    "                decoded_labels = processor.batch_decode(labels_clean, skip_special_tokens=True)\n",
    "\n",
    "                # Log predictions and store metrics by language\n",
    "                for pred, label, lang in zip(decoded_preds, decoded_labels, languages):\n",
    "                    pred = pred.strip()\n",
    "                    label = label.strip()\n",
    "                    if pred and label:\n",
    "                        # Store in overall metrics\n",
    "                        all_preds.append(pred)\n",
    "                        all_labels.append(label)\n",
    "\n",
    "                        # Store in language-specific metrics\n",
    "                        if lang in metrics_by_language:\n",
    "                            metrics_by_language[lang][\"preds\"].append(pred)\n",
    "                            metrics_by_language[lang][\"labels\"].append(label)\n",
    "\n",
    "                        # Log detailed samples\n",
    "                        if samples_logged < log_samples:\n",
    "                            print(\"\\n--- Sample #{} (Language: {}) ---\".format(samples_logged + 1, lang))\n",
    "                            print(f\"Predicted : {pred}\")\n",
    "                            print(f\"Actual    : {label}\")\n",
    "                            print(f\"WER       : {wer([label], [pred]):.4f}\")\n",
    "                            print(f\"CER       : {cer([label], [pred]):.4f}\")\n",
    "                            print(\"-\" * 50)\n",
    "                            samples_logged += 1\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error in batch {batch_idx}: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "    # Calculate overall metrics\n",
    "    overall_metrics = calculate_metrics(all_preds, all_labels, total_loss, len(dataloader))\n",
    "\n",
    "    # Calculate language-specific metrics\n",
    "    language_metrics = {}\n",
    "    for lang, data in metrics_by_language.items():\n",
    "        if data[\"preds\"]:  # Only calculate if we have predictions for this language\n",
    "            lang_loss = total_loss * (len(data[\"preds\"]) / len(all_preds))  # Approximate loss distribution\n",
    "            lang_metrics = calculate_metrics(data[\"preds\"], data[\"labels\"], lang_loss, len(dataloader))\n",
    "            language_metrics[lang] = lang_metrics\n",
    "\n",
    "    # Print detailed metrics\n",
    "    print(\"\\n=== Overall Evaluation Metrics ===\")\n",
    "    for metric, value in overall_metrics.items():\n",
    "        print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "    print(\"\\n=== Language-Specific Metrics ===\")\n",
    "    for lang, metrics in language_metrics.items():\n",
    "        print(f\"\\n{lang.upper()} Metrics:\")\n",
    "        for metric, value in metrics.items():\n",
    "            print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "    # Combine metrics for return\n",
    "    return {\n",
    "        \"overall\": overall_metrics,\n",
    "        \"by_language\": language_metrics\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "id": "mL84UAtBHLT9"
   },
   "outputs": [],
   "source": [
    "def calculate_metrics(all_preds, all_labels, total_loss, num_batches):\n",
    "    \"\"\"Separate function for metric calculation with error handling\"\"\"\n",
    "    metrics = {\n",
    "        \"loss\": total_loss / num_batches if num_batches > 0 else float('inf')\n",
    "    }\n",
    "\n",
    "    if len(all_preds) > 0 and len(all_labels) > 0:\n",
    "        try:\n",
    "            metrics[\"WER\"] = wer(all_labels, all_preds)\n",
    "            metrics[\"CER\"] = cer(all_labels, all_preds)\n",
    "\n",
    "            tokenized_labels = [text.split() for text in all_labels]\n",
    "            tokenized_preds = [text.split() for text in all_preds]\n",
    "\n",
    "            min_len = min(len(tokenized_labels), len(tokenized_preds))\n",
    "            tokenized_labels = tokenized_labels[:min_len]\n",
    "            tokenized_preds = tokenized_preds[:min_len]\n",
    "\n",
    "            flat_labels = [word for sentence in tokenized_labels for word in sentence]\n",
    "            flat_preds = [word for sentence in tokenized_preds for word in sentence]\n",
    "\n",
    "            if len(flat_labels) == len(flat_preds) and len(flat_labels) > 0:\n",
    "                precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "                    flat_labels,\n",
    "                    flat_preds,\n",
    "                    average='weighted',\n",
    "                    zero_division=0\n",
    "                )\n",
    "                accuracy = accuracy_score(flat_labels, flat_preds)\n",
    "\n",
    "                metrics.update({\n",
    "                    \"Precision\": precision,\n",
    "                    \"Recall\": recall,\n",
    "                    \"F1-Score\": f1,\n",
    "                    \"Accuracy\": accuracy\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating metrics: {str(e)}\")\n",
    "            metrics.update({\n",
    "                \"WER\": 1.0,\n",
    "                \"CER\": 1.0,\n",
    "                \"Precision\": 0.0,\n",
    "                \"Recall\": 0.0,\n",
    "                \"F1-Score\": 0.0,\n",
    "                \"Accuracy\": 0.0\n",
    "            })\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "78_1ayt4-Sua"
   },
   "outputs": [],
   "source": [
    "def train_model(config, checkpoint_path=None):\n",
    "    # Initialize wandb with more configurations\n",
    "    wandb.init(\n",
    "        project=\"taglish-whisper-fine-tuning\",\n",
    "        config={\n",
    "            **config,\n",
    "            \"architecture\": \"whisper-base\",\n",
    "            \"optimizer\": \"AdamW\",\n",
    "            \"scheduler\": \"cosine_with_warmup\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Set device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    if not torch.cuda.is_available():\n",
    "        logging.warning(\"CUDA unavailable - training will proceed on CPU\")\n",
    "        config[\"mixed_precision\"] = False\n",
    "\n",
    "\n",
    "    # Load model and processor with safe checkpoint loading\n",
    "    start_epoch = 0\n",
    "    if checkpoint_path and os.path.exists(checkpoint_path):\n",
    "        print(f\"Loading checkpoint from {checkpoint_path}\")\n",
    "        try:\n",
    "            processor = WhisperProcessor.from_pretrained(checkpoint_path)\n",
    "            model = WhisperForConditionalGeneration.from_pretrained(checkpoint_path)\n",
    "\n",
    "            # Add Tagalog language token if not present\n",
    "            special_tokens = {\"additional_special_tokens\": [\"<|tl|>\"]}\n",
    "            num_added_tokens = processor.tokenizer.add_special_tokens(special_tokens)\n",
    "            if num_added_tokens > 0:\n",
    "                model.resize_token_embeddings(len(processor.tokenizer))\n",
    "\n",
    "            # Safely load training state\n",
    "            training_state_path = os.path.join(checkpoint_path, 'training_state.pt')\n",
    "            if os.path.exists(training_state_path):\n",
    "                try:\n",
    "                    training_state = torch.load(training_state_path, map_location=device)\n",
    "                    start_epoch = training_state.get('epoch', 0) + 1\n",
    "                    print(f\"Resuming from epoch {start_epoch}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Could not load training state: {e}\")\n",
    "                    start_epoch = 0\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading checkpoint: {e}\")\n",
    "            print(\"Starting fresh training with multilingual base model\")\n",
    "            processor = WhisperProcessor.from_pretrained(\"openai/whisper-base\")\n",
    "            model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-base\")\n",
    "\n",
    "            # Add Tagalog language token\n",
    "            special_tokens = {\"additional_special_tokens\": [\"<|tl|>\"]}\n",
    "            num_added_tokens = processor.tokenizer.add_special_tokens(special_tokens)\n",
    "            if num_added_tokens > 0:\n",
    "                model.resize_token_embeddings(len(processor.tokenizer))\n",
    "    else:\n",
    "        print(\"Starting fresh training with multilingual base model\")\n",
    "        processor = WhisperProcessor.from_pretrained(\"openai/whisper-base\")\n",
    "        model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-base\")\n",
    "\n",
    "        # Add Tagalog language token\n",
    "        special_tokens = {\"additional_special_tokens\": [\"<|tl|>\"]}\n",
    "        num_added_tokens = processor.tokenizer.add_special_tokens(special_tokens)\n",
    "        if num_added_tokens > 0:\n",
    "            model.resize_token_embeddings(len(processor.tokenizer))\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    # Load data\n",
    "    print(\"\\nLoading data...\")\n",
    "    audio_files, transcripts, languages, timestamps = load_data(\n",
    "        config[\"tsv_file\"],\n",
    "        config[\"audio_dir\"],\n",
    "        config[\"max_samples\"]\n",
    "    )\n",
    "\n",
    "    print(f\"Total samples loaded: {len(audio_files)}\")\n",
    "\n",
    "    if len(audio_files) == 0:\n",
    "        raise ValueError(\"No audio files loaded. Please check your data paths and file formats.\")\n",
    "\n",
    "    # Create dataloaders with validation size check\n",
    "    total_samples = len(audio_files)\n",
    "    min_val_samples = 1  # Minimum number of validation samples\n",
    "\n",
    "    # Adjust validation split if necessary\n",
    "    if total_samples * config[\"validation_split\"] < min_val_samples:\n",
    "        adjusted_split = min_val_samples / total_samples\n",
    "        print(f\"Warning: Adjusting validation split from {config['validation_split']} to {adjusted_split} to ensure at least {min_val_samples} validation sample(s)\")\n",
    "        config[\"validation_split\"] = adjusted_split\n",
    "\n",
    "    # Create dataloaders with language information\n",
    "    train_dataloader, val_dataloader = create_dataloaders(\n",
    "        audio_files,\n",
    "        transcripts,\n",
    "        timestamps,\n",
    "        processor,\n",
    "        config,\n",
    "        languages=languages  # Pass languages to create_dataloaders\n",
    "    )\n",
    "\n",
    "    print(f\"Training batches: {len(train_dataloader)}\")\n",
    "    print(f\"Validation batches: {len(val_dataloader)}\")\n",
    "\n",
    "\n",
    "    # Setup optimizer and scheduler\n",
    "    optimizer = AdamW(\n",
    "        model.parameters(),\n",
    "        lr=config[\"learning_rate\"],\n",
    "        weight_decay=config[\"weight_decay\"]\n",
    "    )\n",
    "\n",
    "    num_training_steps = config[\"epochs\"] * len(train_dataloader)\n",
    "    scheduler = get_scheduler(\n",
    "        \"cosine\",\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=config[\"warmup_steps\"],\n",
    "        num_training_steps=num_training_steps\n",
    "    )\n",
    "\n",
    "    # Load optimizer and scheduler states if resuming\n",
    "    if checkpoint_path and os.path.exists(checkpoint_path) and 'training_state' in locals():\n",
    "        try:\n",
    "            optimizer.load_state_dict(training_state['optimizer_state_dict'])\n",
    "            scheduler.load_state_dict(training_state['scheduler_state_dict'])\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not load optimizer/scheduler states: {e}\")\n",
    "\n",
    "    # Setup mixed precision training only if CUDA is available\n",
    "    scaler = torch.amp.GradScaler() if config[\"mixed_precision\"] and torch.cuda.is_available() else None\n",
    "\n",
    "    # Setup early stopping\n",
    "    early_stopping = EarlyStopping(patience=config[\"early_stopping_patience\"])\n",
    "\n",
    "# Initialize gradient accumulation trainer\n",
    "    trainer = GradientAccumulationTrainer(\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "        scaler=scaler,\n",
    "        config=config\n",
    "    )\n",
    "\n",
    "    # Initialize checkpointer\n",
    "    checkpointer = ModelCheckpointer(\n",
    "        save_dir=\"/content/drive/Shareddrives/CS307-Thesis/Dataset/whisper_checkpoints\",\n",
    "        metric_name=\"WER\",\n",
    "        mode=\"min\"\n",
    "    )\n",
    "\n",
    "    # Training loop with enhanced monitoring and checkpointing\n",
    "    for epoch in range(start_epoch, config[\"epochs\"]):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        # Training phase with gradient accumulation\n",
    "        for i, batch in enumerate(train_dataloader):\n",
    "            if not batch:\n",
    "                continue\n",
    "                \n",
    "            loss = trainer.train_step(batch, device)\n",
    "            epoch_loss += loss\n",
    "\n",
    "            # Logging\n",
    "            if i % 10 == 0:\n",
    "                wandb.log({\n",
    "                    \"batch_loss\": loss,\n",
    "                    \"learning_rate\": scheduler.get_last_lr()[0],\n",
    "                    \"epoch\": epoch,\n",
    "                    \"step\": i\n",
    "                })\n",
    "\n",
    "        # Validation phase\n",
    "        val_metrics = evaluate_model(model, processor, val_dataloader, device)\n",
    "        \n",
    "        # Log metrics\n",
    "        wandb.log({\n",
    "            \"train_loss\": epoch_loss / len(train_dataloader),\n",
    "            **val_metrics,\n",
    "            \"epoch\": epoch + 1\n",
    "        })\n",
    "\n",
    "        # Checkpointing based on validation metrics\n",
    "        if checkpointer.should_save(val_metrics[\"overall\"]):\n",
    "            checkpointer.save_checkpoint(\n",
    "                model=model,\n",
    "                processor=processor,\n",
    "                optimizer=optimizer,\n",
    "                scheduler=scheduler,\n",
    "                epoch=epoch + 1,\n",
    "                metrics=val_metrics\n",
    "            )\n",
    "\n",
    "        # Early stopping check\n",
    "        early_stopping(val_metrics[\"overall\"][\"loss\"])\n",
    "        if early_stopping.should_stop:\n",
    "            logging.info(f\"Early stopping triggered at epoch {epoch + 1}\")\n",
    "            break\n",
    "\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jPTdgrf2-Sua"
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    drive.mount('/content/drive')\n",
    "    verify_gpu_status()\n",
    "    \n",
    "    try:\n",
    "        checkpoint_path = \"/content/drive/Shareddrives/CS307-Thesis/Dataset/whisper_checkpoints/checkpoint_epoch_2\"\n",
    "        train_model(config, checkpoint_path=checkpoint_path)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Training failed: {str(e)}\", exc_info=True)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "proj-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
