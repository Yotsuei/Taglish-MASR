{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy pandas torch transformers wandb tqdm scikit-learn librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import WhisperFeatureExtractor, WhisperProcessor, WhisperForConditionalGeneration\n",
    "import wandb\n",
    "from tqdm import tqdm\n",
    "from jiwer import wer, cer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Google Drive mounting\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "AUDIO_DIR = '/content/drive/Shareddrives/CS307-Thesis/Dataset/single-speaker/'\n",
    "TSV_FILE = '/content/drive/Shareddrives/CS307-Thesis/Dataset/single-speaker/validated.tsv'\n",
    "CHECKPOINT_DIR = '/content/drive/Shareddrives/CS307-Thesis/Dataset/whisper_checkpoints/'\n",
    "MAX_SAMPLES = 1000\n",
    "BATCH_SIZE = 4\n",
    "NUM_WORKERS = 4\n",
    "EPOCHS = 3\n",
    "LEARNING_RATE = 1e-5\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "audio_files, transcripts, languages, timestamps = load_data(TSV_FILE, AUDIO_DIR, max_samples=MAX_SAMPLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset and dataloader\n",
    "class WhisperDataset(Dataset):\n",
    "    def __init__(self, audio_files, transcripts, languages, timestamps):\n",
    "        self.audio_files = audio_files\n",
    "        self.transcripts = transcripts\n",
    "        self.languages = languages\n",
    "        self.timestamps = timestamps\n",
    "        self.processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        audio_file = self.audio_files[idx]\n",
    "        transcript = self.transcripts[idx]\n",
    "        language = self.languages[idx]\n",
    "        start_time, end_time = self.timestamps[idx]\n",
    "\n",
    "        # Load audio\n",
    "        audio = torchaudio.load(audio_file)[0].squeeze(0)\n",
    "        # Crop audio based on timestamps\n",
    "        audio = audio[int(start_time * 16000):int(end_time * 16000)]\n",
    "\n",
    "        # Preprocess audio and text\n",
    "        pixel_values = self.processor.feature_extractor(audio, sampling_rate=16000, return_tensors=\"pt\").pixel_values\n",
    "        input_ids = self.processor.tokenizer(transcript, return_tensors=\"pt\").input_ids\n",
    "\n",
    "        return {\n",
    "            \"audio\": pixel_values,\n",
    "            \"input_ids\": input_ids,\n",
    "            \"language\": language\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = WhisperDataset(audio_files, transcripts, languages, timestamps)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Weights & Biases\n",
    "wandb.init(project=\"whisper-fine-tuning\", config={\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"learning_rate\": LEARNING_RATE,\n",
    "    \"epochs\": EPOCHS,\n",
    "    \"max_samples\": MAX_SAMPLES\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Whisper model and fine-tune\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\").to(DEVICE)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load checkpoint if available\n",
    "start_epoch = 0\n",
    "if os.path.exists(os.path.join(CHECKPOINT_DIR, \"checkpoint.pt\")):\n",
    "    checkpoint = torch.load(os.path.join(CHECKPOINT_DIR, \"checkpoint.pt\"))\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "    start_epoch = checkpoint[\"epoch\"] + 1\n",
    "    print(f\"Loaded checkpoint from epoch {start_epoch}\")\n",
    "\n",
    "for epoch in range(start_epoch, EPOCHS):\n",
    "    train_loss = 0\n",
    "    train_wer, train_cer, train_acc, train_precision, train_recall, train_f1 = 0, 0, 0, 0, 0, 0\n",
    "    model.train()\n",
    "    for batch in tqdm(dataloader, desc=f\"Epoch {epoch+1}/{EPOCHS}\", unit=\"batch\"):\n",
    "        audio = batch[\"audio\"].to(DEVICE)\n",
    "        input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "        language = batch[\"language\"]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(audio, input_ids=input_ids, return_dict=True)\n",
    "        loss = output.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # Evaluate metrics\n",
    "        predicted_ids = output.logits.argmax(-1)\n",
    "        predicted_text = [model.processor.tokenizer.decode(p, skip_special_tokens=True) for p in predicted_ids]\n",
    "        true_text = [model.processor.tokenizer.decode(t, skip_special_tokens=True) for t in input_ids]\n",
    "        train_wer += wer(true_text, predicted_text)\n",
    "        train_cer += cer(true_text, predicted_text)\n",
    "        train_acc += (np.array(predicted_text) == np.array(true_text)).mean()\n",
    "        train_precision += (np.array(predicted_text) == np.array(true_text)).mean()\n",
    "        train_recall += (np.array(predicted_text) == np.array(true_text)).mean()\n",
    "        train_f1 += 2 * train_precision * train_recall / (train_precision + train_recall)\n",
    "\n",
    "    train_loss /= len(dataloader)\n",
    "    train_wer /= len(dataloader)\n",
    "    train_cer /= len(dataloader)\n",
    "    train_acc /= len(dataloader)\n",
    "    train_precision /= len(dataloader)\n",
    "    train_recall /= len(dataloader)\n",
    "    train_f1 /= len(dataloader)\n",
    "\n",
    "    # Log metrics to Weights & Biases\n",
    "    wandb.log({\n",
    "        \"train_loss\": train_loss,\n",
    "        \"train_wer\": train_wer,\n",
    "        \"train_cer\": train_cer,\n",
    "        \"train_accuracy\": train_acc,\n",
    "        \"train_precision\": train_precision,\n",
    "        \"train_recall\": train_recall,\n",
    "        \"train_f1\": train_f1\n",
    "    })\n",
    "\n",
    "    # Save checkpoint\n",
    "    checkpoint = {\n",
    "        \"epoch\": epoch,\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict()\n",
    "    }\n",
    "    torch.save(checkpoint, os.path.join(CHECKPOINT_DIR, \"checkpoint.pt\"))\n",
    "    print(f\"Checkpoint saved for epoch {epoch+1}\")\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "proj-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
