{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5S8Yq8HwkQkv",
        "outputId": "90713594-d726-423c-a3d7-f6f4fbd25031"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.0+cu121)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.1.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.10/dist-packages (0.10.2.post1)\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.10/dist-packages (0.4.3)\n",
            "Requirement already satisfied: jiwer in /usr/local/lib/python3.10/dist-packages (3.0.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.9.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.6)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.10)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.24.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa) (3.0.1)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.5.2)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.4.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.60.0)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.12.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.5.0.post1)\n",
            "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.1.0)\n",
            "Requirement already satisfied: click<9.0.0,>=8.1.3 in /usr/local/lib/python3.10/dist-packages (from jiwer) (8.1.7)\n",
            "Requirement already satisfied: rapidfuzz<4,>=3 in /usr/local/lib/python3.10/dist-packages (from jiwer) (3.10.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.17.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.1->librosa) (4.3.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->librosa) (3.5.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile>=0.12.1->librosa) (1.17.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.22)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch pandas numpy datasets transformers librosa evaluate jiwer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "id": "ISR3DM1kkQkx",
        "outputId": "2b181fc0-d4a3-4a30-ea20-83ec138c16e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing last run (ID:6pt9exlu) before initializing another..."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">whisper-small-taglish-20241105_154535</strong> at: <a href='https://wandb.ai/idcsalvame-n-a/whisper-taglish/runs/6pt9exlu' target=\"_blank\">https://wandb.ai/idcsalvame-n-a/whisper-taglish/runs/6pt9exlu</a><br/> View project at: <a href='https://wandb.ai/idcsalvame-n-a/whisper-taglish' target=\"_blank\">https://wandb.ai/idcsalvame-n-a/whisper-taglish</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20241105_154535-6pt9exlu/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Successfully finished last run (ID:6pt9exlu). Initializing new run:<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.18.5"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20241105_155215-tg8r7u3s</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/idcsalvame-n-a/whisper-taglish/runs/tg8r7u3s' target=\"_blank\">whisper-small-taglish-20241105_155215</a></strong> to <a href='https://wandb.ai/idcsalvame-n-a/whisper-taglish' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/idcsalvame-n-a/whisper-taglish' target=\"_blank\">https://wandb.ai/idcsalvame-n-a/whisper-taglish</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/idcsalvame-n-a/whisper-taglish/runs/tg8r7u3s' target=\"_blank\">https://wandb.ai/idcsalvame-n-a/whisper-taglish/runs/tg8r7u3s</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:__main__:Error during evaluation: operands could not be broadcast together with remapped shapes [original->remapped]: (2,2)  and requested shape (3,2)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "operands could not be broadcast together with remapped shapes [original->remapped]: (2,2)  and requested shape (3,2)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-1673a038c28e>\u001b[0m in \u001b[0;36m<cell line: 416>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 417\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-29-1673a038c28e>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    402\u001b[0m     \u001b[0;31m# Evaluate pre-training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Evaluating model before training...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 404\u001b[0;31m     \u001b[0mpre_train_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pre_training\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    405\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Pre-training metrics: {pre_train_metrics}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-29-1673a038c28e>\u001b[0m in \u001b[0;36mevaluate_model\u001b[0;34m(self, eval_dataset, split_name)\u001b[0m\n\u001b[1;32m    294\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0meval_dataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m                     \u001b[0;31m# Generate predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                     inputs = self.processor(\n\u001b[0m\u001b[1;32m    297\u001b[0m                         \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'audio'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m                         \u001b[0msampling_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/whisper/processing_whisper.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0maudio\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m             \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampling_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msampling_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0mencodings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/whisper/feature_extraction_whisper.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, raw_speech, truncation, pad_to_multiple_of, return_tensors, return_attention_mask, padding, max_length, sampling_rate, do_normalize, device, return_token_timestamps, **kwargs)\u001b[0m\n\u001b[1;32m    280\u001b[0m         \u001b[0;31m# convert into correct format for padding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m         padded_inputs = self.pad(\n\u001b[0m\u001b[1;32m    283\u001b[0m             \u001b[0mbatched_speech\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m             \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/feature_extraction_sequence_utils.py\u001b[0m in \u001b[0;36mpad\u001b[0;34m(self, processed_features, padding, max_length, truncation, pad_to_multiple_of, return_attention_mask, return_tensors)\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m             \u001b[0;31m# padding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m             outputs = self._pad(\n\u001b[0m\u001b[1;32m    211\u001b[0m                 \u001b[0mtruncated_inputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m                 \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/feature_extraction_sequence_utils.py\u001b[0m in \u001b[0;36m_pad\u001b[0;34m(self, processed_features, max_length, padding_strategy, pad_to_multiple_of, return_attention_mask)\u001b[0m\n\u001b[1;32m    280\u001b[0m                     )\n\u001b[1;32m    281\u001b[0m                 \u001b[0mpadding_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdifference\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_size\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdifference\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m                 processed_features[self.model_input_names[0]] = np.pad(\n\u001b[0m\u001b[1;32m    283\u001b[0m                     \u001b[0mrequired_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"constant\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconstant_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/lib/arraypad.py\u001b[0m in \u001b[0;36mpad\u001b[0;34m(array, pad_width, mode, **kwargs)\u001b[0m\n\u001b[1;32m    746\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    747\u001b[0m     \u001b[0;31m# Broadcast to shape (array.ndim, 2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 748\u001b[0;31m     \u001b[0mpad_width\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_as_pairs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpad_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    749\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/lib/arraypad.py\u001b[0m in \u001b[0;36m_as_pairs\u001b[0;34m(x, ndim, as_index)\u001b[0m\n\u001b[1;32m    520\u001b[0m     \u001b[0;31m# Converting the array with `tolist` seems to improve performance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m     \u001b[0;31m# when iterating and indexing the result (see usage in `pad`)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 522\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    523\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/lib/stride_tricks.py\u001b[0m in \u001b[0;36mbroadcast_to\u001b[0;34m(array, shape, subok)\u001b[0m\n\u001b[1;32m    411\u001b[0m            [1, 2, 3]])\n\u001b[1;32m    412\u001b[0m     \"\"\"\n\u001b[0;32m--> 413\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_broadcast_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubok\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    414\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/lib/stride_tricks.py\u001b[0m in \u001b[0;36m_broadcast_to\u001b[0;34m(array, shape, subok, readonly)\u001b[0m\n\u001b[1;32m    347\u001b[0m                          'negative')\n\u001b[1;32m    348\u001b[0m     \u001b[0mextras\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m     it = np.nditer(\n\u001b[0m\u001b[1;32m    350\u001b[0m         \u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'multi_index'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'refs_ok'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'zerosize_ok'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mextras\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m         op_flags=['readonly'], itershape=shape, order='C')\n",
            "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with remapped shapes [original->remapped]: (2,2)  and requested shape (3,2)"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import wandb\n",
        "from datasets import Dataset, Audio\n",
        "from transformers import (\n",
        "    WhisperProcessor,\n",
        "    WhisperForConditionalGeneration,\n",
        "    WhisperFeatureExtractor,\n",
        "    WhisperTokenizer,\n",
        "    Seq2SeqTrainingArguments,\n",
        "    Seq2SeqTrainer\n",
        ")\n",
        "import librosa\n",
        "import evaluate\n",
        "from dataclasses import dataclass\n",
        "from typing import Any, Dict, List, Optional, Union\n",
        "from google.colab import drive\n",
        "import logging\n",
        "import sys\n",
        "from torch.utils.data import DataLoader\n",
        "import json\n",
        "from datetime import datetime\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from itertools import zip_longest\n",
        "\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.FileHandler('training.log'),\n",
        "        logging.StreamHandler(sys.stdout)\n",
        "    ]\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "def load_data(tsv_file, audio_dir, max_samples=None):\n",
        "    \"\"\"\n",
        "    Load data from TSV file with timestamp handling, compatible with both \"sec\" and \"min:sec\" formats.\n",
        "    \"\"\"\n",
        "    audio_files, transcripts, languages, timestamps = [], [], [], []\n",
        "    # Read TSV file\n",
        "    df = pd.read_csv(tsv_file, sep='\\t')\n",
        "    required_columns = ['path', 'start_time', 'end_time', 'language', 'sentence']\n",
        "    # Verify all required columns are present\n",
        "    if not all(col in df.columns for col in required_columns):\n",
        "        raise ValueError(f\"TSV file must contain columns: {required_columns}\")\n",
        "    # Shuffle and limit samples if specified\n",
        "    df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "    if max_samples:\n",
        "        df = df.head(max_samples)\n",
        "    for _, row in df.iterrows():\n",
        "        audio_file = row['path']\n",
        "        if not audio_file.endswith((\".mp3\", \".wav\", \".flac\")):\n",
        "            print(f\"Skipping unsupported file type: {audio_file}\")\n",
        "            continue\n",
        "        full_audio_path = os.path.join(audio_dir, audio_file)\n",
        "        if not os.path.exists(full_audio_path):\n",
        "            print(f\"Warning: Audio file not found: {full_audio_path}\")\n",
        "            continue\n",
        "        # Parse timestamps\n",
        "        def parse_time(time_str):\n",
        "            try:\n",
        "                # Check if time is already in seconds\n",
        "                return float(time_str)\n",
        "            except ValueError:\n",
        "                # Convert from \"min:sec\" format to seconds\n",
        "                minutes, seconds = map(float, time_str.split(\":\"))\n",
        "                return minutes * 60 + seconds\n",
        "        try:\n",
        "            start_time = parse_time(row['start_time'])\n",
        "            end_time = parse_time(row['end_time'])\n",
        "        except Exception as e:\n",
        "            print(f\"Error parsing timestamps for {audio_file}: {str(e)}\")\n",
        "            continue\n",
        "        audio_files.append(full_audio_path)\n",
        "        transcripts.append(row['sentence'])\n",
        "        timestamps.append((start_time, end_time))\n",
        "        languages.append(row['language'])\n",
        "    return audio_files, transcripts, languages, timestamps\n",
        "\n",
        "class WhisperTrainer:\n",
        "    def __init__(self, config):\n",
        "        \"\"\"\n",
        "        Initialize the WhisperTrainer with configuration.\n",
        "\n",
        "        Args:\n",
        "            config (dict): Configuration dictionary containing training parameters\n",
        "        \"\"\"\n",
        "        self.config = config\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        logger.info(f\"Using device: {self.device}\")\n",
        "\n",
        "        # Initialize WandB\n",
        "        if self.config['use_wandb']:\n",
        "            wandb.init(\n",
        "                project=self.config['wandb_project_name'],\n",
        "                name=self.config['wandb_run_name'],\n",
        "                config=self.config\n",
        "            )\n",
        "\n",
        "        self.setup_model_and_processor()\n",
        "        self.metrics = {\n",
        "            'wer': evaluate.load('wer'),\n",
        "            'cer': evaluate.load('cer')\n",
        "        }\n",
        "\n",
        "    def setup_model_and_processor(self):\n",
        "        \"\"\"Setup the Whisper model and processor.\"\"\"\n",
        "        try:\n",
        "            self.processor = WhisperProcessor.from_pretrained(self.config['model_name'])\n",
        "            self.model = WhisperForConditionalGeneration.from_pretrained(self.config['model_name'])\n",
        "            self.feature_extractor = WhisperFeatureExtractor.from_pretrained(self.config['model_name'])\n",
        "            self.tokenizer = WhisperTokenizer.from_pretrained(self.config['model_name'])\n",
        "\n",
        "            # Move model to appropriate device\n",
        "            self.model = self.model.to(self.device)\n",
        "            logger.info(\"Model and processor setup completed successfully\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error in setting up model and processor: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def preprocess_audio(self, audio_path, start_time=None, end_time=None):\n",
        "        \"\"\"\n",
        "        Preprocess audio file with resampling and segmentation.\n",
        "\n",
        "        Args:\n",
        "            audio_path (str): Path to audio file\n",
        "            start_time (float, optional): Start time in seconds\n",
        "            end_time (float, optional): End time in seconds\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Load audio with librosa\n",
        "            audio, sr = librosa.load(audio_path, sr=None)\n",
        "\n",
        "            # Resample if necessary\n",
        "            if sr != 16000:\n",
        "                audio = librosa.resample(audio, orig_sr=sr, target_sr=16000)\n",
        "                sr = 16000\n",
        "\n",
        "            # Extract segment if timestamps are provided\n",
        "            if start_time is not None and end_time is not None:\n",
        "                start_idx = int(start_time * sr)\n",
        "                end_idx = int(end_time * sr)\n",
        "                audio = audio[start_idx:end_idx]\n",
        "\n",
        "            return audio, sr\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error preprocessing audio {audio_path}: {str(e)}\")\n",
        "            return None, None\n",
        "\n",
        "    def prepare_dataset(self, audio_files, transcripts, languages, timestamps):\n",
        "        \"\"\"\n",
        "        Prepare dataset for training.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            dataset_dict = {\n",
        "                \"audio\": [],\n",
        "                \"text\": [],\n",
        "                \"language\": [],\n",
        "                \"path\": []\n",
        "            }\n",
        "\n",
        "            max_length = 0\n",
        "            for audio_path, transcript, lang, (start, end) in zip(\n",
        "                audio_files, transcripts, languages, timestamps\n",
        "            ):\n",
        "                # Preprocess audio\n",
        "                audio, sr = self.preprocess_audio(audio_path, start, end)\n",
        "                if audio is None:\n",
        "                    continue\n",
        "\n",
        "                audio_tensor = torch.from_numpy(audio)\n",
        "                max_length = max(max_length, audio_tensor.shape[0])\n",
        "                dataset_dict[\"audio\"].append(audio_tensor)\n",
        "                dataset_dict[\"text\"].append(transcript)\n",
        "                dataset_dict[\"language\"].append(lang)\n",
        "                dataset_dict[\"path\"].append(audio_path)\n",
        "\n",
        "            # Pad the audio tensors\n",
        "            for i, tensor in enumerate(dataset_dict[\"audio\"]):\n",
        "                dataset_dict[\"audio\"][i] = torch.nn.functional.pad(tensor, (0, max_length - tensor.shape[0]), mode='constant', value=0)\n",
        "            return Dataset.from_dict(dataset_dict)\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error preparing dataset: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def compute_metrics(self, pred):\n",
        "        pred_ids = pred.predictions\n",
        "        label_ids = pred.label_ids\n",
        "\n",
        "        # Replace -100 with pad token id\n",
        "        label_ids[label_ids == -100] = self.tokenizer.pad_token_id\n",
        "\n",
        "        # Decode predictions and references\n",
        "        predictions = self.tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
        "        references = self.tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
        "\n",
        "        # Compute metrics\n",
        "        wer_scores = []\n",
        "        cer_scores = []\n",
        "\n",
        "        for i in range(len(predictions)):\n",
        "            wer_scores.append(self.metrics['wer'].compute(predictions=[predictions[i]], references=[references[i]]))\n",
        "            cer_scores.append(self.metrics['cer'].compute(predictions=[predictions[i]], references=[references[i]]))\n",
        "\n",
        "        wer = sum(wer_scores) / len(wer_scores)\n",
        "        cer = sum(cer_scores) / len(cer_scores)\n",
        "\n",
        "        # Log detailed predictions vs references\n",
        "        for pred, ref, path in zip(predictions, references, pred.input_ids):\n",
        "            logger.info(f\"\\nAudio: {path}\")\n",
        "            logger.info(f\"Reference: {ref}\")\n",
        "            logger.info(f\"Prediction: {pred}\")\n",
        "\n",
        "        return {\n",
        "            \"wer\": wer,\n",
        "            \"cer\": cer\n",
        "        }\n",
        "\n",
        "    def train(self, train_dataset, eval_dataset):\n",
        "        \"\"\"\n",
        "        Train the model with the prepared datasets.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            training_args = Seq2SeqTrainingArguments(\n",
        "                output_dir=self.config['output_dir'],\n",
        "                per_device_train_batch_size=self.config['batch_size'],\n",
        "                gradient_accumulation_steps=self.config['gradient_accumulation_steps'],\n",
        "                learning_rate=self.config['learning_rate'],\n",
        "                warmup_steps=self.config['warmup_steps'],\n",
        "                max_steps=self.config['max_steps'],\n",
        "                fp16=torch.cuda.is_available(),\n",
        "                evaluation_strategy=\"steps\",\n",
        "                eval_steps=self.config['eval_steps'],\n",
        "                save_steps=self.config['save_steps'],\n",
        "                logging_steps=self.config['logging_steps'],\n",
        "                report_to=\"wandb\" if self.config['use_wandb'] else None,\n",
        "                load_best_model_at_end=True,\n",
        "                metric_for_best_model=\"wer\",\n",
        "                greater_is_better=False\n",
        "            )\n",
        "\n",
        "            trainer = Seq2SeqTrainer(\n",
        "                model=self.model,\n",
        "                args=training_args,\n",
        "                train_dataset=train_dataset,\n",
        "                eval_dataset=eval_dataset,\n",
        "                compute_metrics=self.compute_metrics,\n",
        "            )\n",
        "\n",
        "            # Load checkpoint if it exists\n",
        "            if os.path.exists(self.config['checkpoint_dir']):\n",
        "                logger.info(f\"Loading checkpoint from {self.config['checkpoint_dir']}\")\n",
        "                trainer.train(resume_from_checkpoint=self.config['checkpoint_dir'])\n",
        "            else:\n",
        "                trainer.train()\n",
        "\n",
        "            # Save final model\n",
        "            trainer.save_model(self.config['output_dir'])\n",
        "            logger.info(\"Training completed successfully\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error during training: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def evaluate_model(self, eval_dataset, split_name=\"eval\"):\n",
        "        \"\"\"\n",
        "        Evaluate the model on a dataset.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            logger.info(f\"Starting evaluation on {split_name} split\")\n",
        "\n",
        "            eval_dataloader = DataLoader(\n",
        "                eval_dataset,\n",
        "                batch_size=self.config['eval_batch_size'],\n",
        "                shuffle=False\n",
        "            )\n",
        "\n",
        "            self.model.eval()\n",
        "            all_metrics = {\n",
        "                'wer': [], 'cer': [],\n",
        "                'predictions': [], 'references': [],\n",
        "                'audio_paths': [], 'timestamps': []\n",
        "            }\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for batch in eval_dataloader:\n",
        "                    # Generate predictions\n",
        "                    inputs = self.processor(\n",
        "                        batch['audio'],\n",
        "                        sampling_rate=16000,\n",
        "                        return_tensors=\"pt\"\n",
        "                    ).to(self.device)\n",
        "\n",
        "                    generated_ids = self.model.generate(\n",
        "                        inputs.input_features,\n",
        "                        max_length=self.config['max_length']\n",
        "                    )\n",
        "\n",
        "                    # Decode predictions\n",
        "                    transcriptions = self.tokenizer.batch_decode(\n",
        "                        generated_ids,\n",
        "                        skip_special_tokens=True\n",
        "                    )\n",
        "\n",
        "                    # Compute metrics\n",
        "                    wer = self.metrics['wer'].compute(\n",
        "                        predictions=transcriptions,\n",
        "                        references=batch['text']\n",
        "                    )\n",
        "                    cer = self.metrics['cer'].compute(\n",
        "                        predictions=transcriptions,\n",
        "                        references=batch['text']\n",
        "                    )\n",
        "\n",
        "                    # Store results\n",
        "                    all_metrics['wer'].append(wer)\n",
        "                    all_metrics['cer'].append(cer)\n",
        "                    all_metrics['predictions'].extend(transcriptions)\n",
        "                    all_metrics['references'].extend(batch['text'])\n",
        "                    all_metrics['audio_paths'].extend(batch['path'])\n",
        "\n",
        "                    # Log detailed results\n",
        "                    for pred, ref, path in zip(\n",
        "                        transcriptions, batch['text'], batch['path']\n",
        "                    ):\n",
        "                        logger.info(f\"\\nAudio: {path}\")\n",
        "                        logger.info(f\"Reference: {ref}\")\n",
        "                        logger.info(f\"Prediction: {pred}\")\n",
        "\n",
        "            # Compute average metrics\n",
        "            avg_metrics = {\n",
        "                'wer': np.mean(all_metrics['wer']),\n",
        "                'cer': np.mean(all_metrics['cer'])\n",
        "            }\n",
        "\n",
        "            # Log to WandB\n",
        "            if self.config['use_wandb']:\n",
        "                wandb.log({f\"{split_name}_{k}\": v for k, v in avg_metrics.items()})\n",
        "\n",
        "            # Save detailed results\n",
        "            results_file = os.path.join(\n",
        "                self.config['output_dir'],\n",
        "                f\"{split_name}_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
        "            )\n",
        "            with open(results_file, 'w') as f:\n",
        "                json.dump(all_metrics, f, indent=2)\n",
        "\n",
        "            logger.info(f\"Evaluation results saved to {results_file}\")\n",
        "            return avg_metrics\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error during evaluation: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "def main():\n",
        "    # Configuration\n",
        "    config = {\n",
        "        'model_name': 'openai/whisper-small',  # The name of the pre-trained Whisper model to use for fine-tuning\n",
        "        'output_dir': '/content/drive/Shareddrives/CS307-Thesis/Dataset/whisper-taglish/single-speaker/output',  # The directory where the trained model and other artifacts will be saved\n",
        "        'checkpoint_dir': '/content/drive/Shareddrives/CS307-Thesis/Dataset/whisper-taglish/single-speaker/checkpoints',  # The directory where checkpoints will be saved and loaded from\n",
        "        'batch_size': 8,  # The batch size for training\n",
        "        'eval_batch_size': 4,  # The batch size for evaluation\n",
        "        'gradient_accumulation_steps': 2,  # The number of gradient accumulation steps\n",
        "        'learning_rate': 1e-5,  # The learning rate for training\n",
        "        'warmup_steps': 500,  # The number of warmup steps for the learning rate scheduler\n",
        "        'max_steps': 5000,  # The maximum number of training steps\n",
        "        'eval_steps': 1000,  # The number of steps between each evaluation\n",
        "        'save_steps': 1000,  # The number of steps between each checkpoint save\n",
        "        'logging_steps': 100,  # The number of steps between each logging step\n",
        "        'max_length': 1000,  # The maximum length of the output transcription\n",
        "        'use_wandb': True,  # Whether to use Weights & Biases (WandB) for experiment tracking\n",
        "        'wandb_project_name': 'whisper-taglish',  # The name of the WandB project\n",
        "        'wandb_run_name': f'whisper-small-taglish-{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}'  # The name of the WandB run, generated with the current timestamp\n",
        "    }\n",
        "\n",
        "    # Load data using the provided function\n",
        "    audio_files, transcripts, languages, timestamps = load_data(\n",
        "        tsv_file='/content/drive/Shareddrives/CS307-Thesis/Dataset/single-speaker/validated.tsv',\n",
        "        audio_dir='/content/drive/Shareddrives/CS307-Thesis/Dataset/single-speaker/',\n",
        "        max_samples=None\n",
        "    )\n",
        "\n",
        "    # Initialize trainer\n",
        "    trainer = WhisperTrainer(config)\n",
        "\n",
        "    # Prepare datasets\n",
        "    full_dataset = trainer.prepare_dataset(audio_files, transcripts, languages, timestamps)\n",
        "\n",
        "    # Split dataset\n",
        "    train_test_split = full_dataset.train_test_split(test_size=0.2, seed=42)\n",
        "    train_dataset = train_test_split['train']\n",
        "    eval_dataset = train_test_split['test']\n",
        "\n",
        "    # Evaluate pre-training\n",
        "    logger.info(\"Evaluating model before training...\")\n",
        "    pre_train_metrics = trainer.evaluate_model(eval_dataset, split_name=\"pre_training\")\n",
        "    logger.info(f\"Pre-training metrics: {pre_train_metrics}\")\n",
        "\n",
        "    # Train model\n",
        "    logger.info(\"Starting training...\")\n",
        "    trainer.train(train_dataset, eval_dataset)\n",
        "\n",
        "    # Evaluate post-training\n",
        "    logger.info(\"Evaluating model after training...\")\n",
        "    post_train_metrics = trainer.evaluate_model(eval_dataset, split_name=\"post_training\")\n",
        "    logger.info(f\"Post-training metrics: {post_train_metrics}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}