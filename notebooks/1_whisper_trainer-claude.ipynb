{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch pandas numpy datasets transformers librosa evaluate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import wandb\n",
    "from datasets import Dataset, Audio\n",
    "from transformers import (\n",
    "    WhisperProcessor,\n",
    "    WhisperForConditionalGeneration,\n",
    "    WhisperFeatureExtractor,\n",
    "    WhisperTokenizer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer\n",
    ")\n",
    "import librosa\n",
    "import evaluate\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "from google.colab import drive\n",
    "import logging\n",
    "import sys\n",
    "from torch.utils.data import DataLoader\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('training.log'),\n",
    "        logging.StreamHandler(sys.stdout)\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "class WhisperTrainer:\n",
    "    def __init__(self, config):\n",
    "        \"\"\"\n",
    "        Initialize the WhisperTrainer with configuration.\n",
    "        \n",
    "        Args:\n",
    "            config (dict): Configuration dictionary containing training parameters\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        logger.info(f\"Using device: {self.device}\")\n",
    "        \n",
    "        # Initialize WandB\n",
    "        if self.config['use_wandb']:\n",
    "            wandb.init(\n",
    "                project=self.config['wandb_project_name'],\n",
    "                name=self.config['wandb_run_name'],\n",
    "                config=self.config\n",
    "            )\n",
    "        \n",
    "        self.setup_model_and_processor()\n",
    "        self.metrics = {\n",
    "            'wer': evaluate.load('wer'),\n",
    "            'cer': evaluate.load('cer')\n",
    "        }\n",
    "\n",
    "    def setup_model_and_processor(self):\n",
    "        \"\"\"Setup the Whisper model and processor.\"\"\"\n",
    "        try:\n",
    "            self.processor = WhisperProcessor.from_pretrained(self.config['model_name'])\n",
    "            self.model = WhisperForConditionalGeneration.from_pretrained(self.config['model_name'])\n",
    "            self.feature_extractor = WhisperFeatureExtractor.from_pretrained(self.config['model_name'])\n",
    "            self.tokenizer = WhisperTokenizer.from_pretrained(self.config['model_name'])\n",
    "            \n",
    "            # Move model to appropriate device\n",
    "            self.model = self.model.to(self.device)\n",
    "            logger.info(\"Model and processor setup completed successfully\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in setting up model and processor: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def preprocess_audio(self, audio_path, start_time=None, end_time=None):\n",
    "        \"\"\"\n",
    "        Preprocess audio file with resampling and segmentation.\n",
    "        \n",
    "        Args:\n",
    "            audio_path (str): Path to audio file\n",
    "            start_time (float, optional): Start time in seconds\n",
    "            end_time (float, optional): End time in seconds\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Load audio with librosa\n",
    "            audio, sr = librosa.load(audio_path, sr=None)\n",
    "            \n",
    "            # Resample if necessary\n",
    "            if sr != 16000:\n",
    "                audio = librosa.resample(audio, orig_sr=sr, target_sr=16000)\n",
    "                sr = 16000\n",
    "            \n",
    "            # Extract segment if timestamps are provided\n",
    "            if start_time is not None and end_time is not None:\n",
    "                start_idx = int(start_time * sr)\n",
    "                end_idx = int(end_time * sr)\n",
    "                audio = audio[start_idx:end_idx]\n",
    "            \n",
    "            return audio, sr\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error preprocessing audio {audio_path}: {str(e)}\")\n",
    "            return None, None\n",
    "\n",
    "    def prepare_dataset(self, audio_files, transcripts, languages, timestamps):\n",
    "        \"\"\"\n",
    "        Prepare dataset for training.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            dataset_dict = {\n",
    "                \"audio\": [],\n",
    "                \"text\": [],\n",
    "                \"language\": [],\n",
    "                \"path\": []\n",
    "            }\n",
    "\n",
    "            for audio_path, transcript, lang, (start, end) in zip(\n",
    "                audio_files, transcripts, languages, timestamps\n",
    "            ):\n",
    "                # Preprocess audio\n",
    "                audio, sr = self.preprocess_audio(audio_path, start, end)\n",
    "                if audio is None:\n",
    "                    continue\n",
    "\n",
    "                dataset_dict[\"audio\"].append(audio)\n",
    "                dataset_dict[\"text\"].append(transcript)\n",
    "                dataset_dict[\"language\"].append(lang)\n",
    "                dataset_dict[\"path\"].append(audio_path)\n",
    "\n",
    "            return Dataset.from_dict(dataset_dict)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error preparing dataset: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def compute_metrics(self, pred):\n",
    "        \"\"\"\n",
    "        Compute various metrics including WER, CER, and other metrics.\n",
    "        \"\"\"\n",
    "        pred_ids = pred.predictions\n",
    "        label_ids = pred.label_ids\n",
    "\n",
    "        # Replace -100 with pad token id\n",
    "        label_ids[label_ids == -100] = self.tokenizer.pad_token_id\n",
    "\n",
    "        # Decode predictions and references\n",
    "        predictions = self.tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "        references = self.tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "        # Compute metrics\n",
    "        wer = self.metrics['wer'].compute(predictions=predictions, references=references)\n",
    "        cer = self.metrics['cer'].compute(predictions=predictions, references=references)\n",
    "\n",
    "        # Log detailed predictions vs references\n",
    "        for pred, ref in zip(predictions[:5], references[:5]):  # Log first 5 examples\n",
    "            logger.info(f\"\\nReference: {ref}\\nPrediction: {pred}\\n\")\n",
    "\n",
    "        return {\n",
    "            \"wer\": wer,\n",
    "            \"cer\": cer\n",
    "        }\n",
    "\n",
    "    def train(self, train_dataset, eval_dataset):\n",
    "        \"\"\"\n",
    "        Train the model with the prepared datasets.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            training_args = Seq2SeqTrainingArguments(\n",
    "                output_dir=self.config['output_dir'],\n",
    "                per_device_train_batch_size=self.config['batch_size'],\n",
    "                gradient_accumulation_steps=self.config['gradient_accumulation_steps'],\n",
    "                learning_rate=self.config['learning_rate'],\n",
    "                warmup_steps=self.config['warmup_steps'],\n",
    "                max_steps=self.config['max_steps'],\n",
    "                fp16=torch.cuda.is_available(),\n",
    "                evaluation_strategy=\"steps\",\n",
    "                eval_steps=self.config['eval_steps'],\n",
    "                save_steps=self.config['save_steps'],\n",
    "                logging_steps=self.config['logging_steps'],\n",
    "                report_to=\"wandb\" if self.config['use_wandb'] else None,\n",
    "                load_best_model_at_end=True,\n",
    "                metric_for_best_model=\"wer\",\n",
    "                greater_is_better=False\n",
    "            )\n",
    "\n",
    "            trainer = Seq2SeqTrainer(\n",
    "                model=self.model,\n",
    "                args=training_args,\n",
    "                train_dataset=train_dataset,\n",
    "                eval_dataset=eval_dataset,\n",
    "                compute_metrics=self.compute_metrics,\n",
    "            )\n",
    "\n",
    "            # Load checkpoint if it exists\n",
    "            if os.path.exists(self.config['checkpoint_dir']):\n",
    "                logger.info(f\"Loading checkpoint from {self.config['checkpoint_dir']}\")\n",
    "                trainer.train(resume_from_checkpoint=self.config['checkpoint_dir'])\n",
    "            else:\n",
    "                trainer.train()\n",
    "\n",
    "            # Save final model\n",
    "            trainer.save_model(self.config['output_dir'])\n",
    "            logger.info(\"Training completed successfully\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during training: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def evaluate_model(self, eval_dataset, split_name=\"eval\"):\n",
    "        \"\"\"\n",
    "        Evaluate the model on a dataset.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logger.info(f\"Starting evaluation on {split_name} split\")\n",
    "            \n",
    "            eval_dataloader = DataLoader(\n",
    "                eval_dataset,\n",
    "                batch_size=self.config['eval_batch_size'],\n",
    "                shuffle=False\n",
    "            )\n",
    "\n",
    "            self.model.eval()\n",
    "            all_metrics = {\n",
    "                'wer': [], 'cer': [],\n",
    "                'predictions': [], 'references': [],\n",
    "                'audio_paths': [], 'timestamps': []\n",
    "            }\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for batch in eval_dataloader:\n",
    "                    # Generate predictions\n",
    "                    inputs = self.processor(\n",
    "                        batch['audio'],\n",
    "                        sampling_rate=16000,\n",
    "                        return_tensors=\"pt\"\n",
    "                    ).to(self.device)\n",
    "                    \n",
    "                    generated_ids = self.model.generate(\n",
    "                        inputs.input_features,\n",
    "                        max_length=self.config['max_length']\n",
    "                    )\n",
    "\n",
    "                    # Decode predictions\n",
    "                    transcriptions = self.tokenizer.batch_decode(\n",
    "                        generated_ids,\n",
    "                        skip_special_tokens=True\n",
    "                    )\n",
    "\n",
    "                    # Compute metrics\n",
    "                    wer = self.metrics['wer'].compute(\n",
    "                        predictions=transcriptions,\n",
    "                        references=batch['text']\n",
    "                    )\n",
    "                    cer = self.metrics['cer'].compute(\n",
    "                        predictions=transcriptions,\n",
    "                        references=batch['text']\n",
    "                    )\n",
    "\n",
    "                    # Store results\n",
    "                    all_metrics['wer'].append(wer)\n",
    "                    all_metrics['cer'].append(cer)\n",
    "                    all_metrics['predictions'].extend(transcriptions)\n",
    "                    all_metrics['references'].extend(batch['text'])\n",
    "                    all_metrics['audio_paths'].extend(batch['path'])\n",
    "\n",
    "                    # Log detailed results\n",
    "                    for pred, ref, path in zip(\n",
    "                        transcriptions, batch['text'], batch['path']\n",
    "                    ):\n",
    "                        logger.info(f\"\\nAudio: {path}\")\n",
    "                        logger.info(f\"Reference: {ref}\")\n",
    "                        logger.info(f\"Prediction: {pred}\")\n",
    "\n",
    "            # Compute average metrics\n",
    "            avg_metrics = {\n",
    "                'wer': np.mean(all_metrics['wer']),\n",
    "                'cer': np.mean(all_metrics['cer'])\n",
    "            }\n",
    "\n",
    "            # Log to WandB\n",
    "            if self.config['use_wandb']:\n",
    "                wandb.log({f\"{split_name}_{k}\": v for k, v in avg_metrics.items()})\n",
    "\n",
    "            # Save detailed results\n",
    "            results_file = os.path.join(\n",
    "                self.config['output_dir'],\n",
    "                f\"{split_name}_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "            )\n",
    "            with open(results_file, 'w') as f:\n",
    "                json.dump(all_metrics, f, indent=2)\n",
    "\n",
    "            logger.info(f\"Evaluation results saved to {results_file}\")\n",
    "            return avg_metrics\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during evaluation: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "def main():\n",
    "    # Configuration\n",
    "    config = {\n",
    "        'model_name': 'openai/whisper-small',\n",
    "        'output_dir': '/content/drive/MyDrive/whisper_tagalog_english',\n",
    "        'checkpoint_dir': '/content/drive/MyDrive/whisper_tagalog_english/checkpoint',\n",
    "        'batch_size': 8,\n",
    "        'eval_batch_size': 4,\n",
    "        'gradient_accumulation_steps': 2,\n",
    "        'learning_rate': 1e-5,\n",
    "        'warmup_steps': 500,\n",
    "        'max_steps': 5000,\n",
    "        'eval_steps': 1000,\n",
    "        'save_steps': 1000,\n",
    "        'logging_steps': 100,\n",
    "        'max_length': 128,\n",
    "        'use_wandb': True,\n",
    "        'wandb_project_name': 'whisper-tagalog-english',\n",
    "        'wandb_run_name': f'whisper-small-{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}'\n",
    "    }\n",
    "\n",
    "    # Load data using the provided function\n",
    "    audio_files, transcripts, languages, timestamps = load_data(\n",
    "        tsv_file='/content/drive/MyDrive/data/transcriptions.tsv',\n",
    "        audio_dir='/content/drive/MyDrive/data/audio',\n",
    "        max_samples=None\n",
    "    )\n",
    "\n",
    "    # Initialize trainer\n",
    "    trainer = WhisperTrainer(config)\n",
    "\n",
    "    # Prepare datasets\n",
    "    full_dataset = trainer.prepare_dataset(audio_files, transcripts, languages, timestamps)\n",
    "    \n",
    "    # Split dataset\n",
    "    train_test_split = full_dataset.train_test_split(test_size=0.2, seed=42)\n",
    "    train_dataset = train_test_split['train']\n",
    "    eval_dataset = train_test_split['test']\n",
    "\n",
    "    # Evaluate pre-training\n",
    "    logger.info(\"Evaluating model before training...\")\n",
    "    pre_train_metrics = trainer.evaluate_model(eval_dataset, split_name=\"pre_training\")\n",
    "    logger.info(f\"Pre-training metrics: {pre_train_metrics}\")\n",
    "\n",
    "    # Train model\n",
    "    logger.info(\"Starting training...\")\n",
    "    trainer.train(train_dataset, eval_dataset)\n",
    "\n",
    "    # Evaluate post-training\n",
    "    logger.info(\"Evaluating model after training...\")\n",
    "    post_train_metrics = trainer.evaluate_model(eval_dataset, split_name=\"post_training\")\n",
    "    logger.info(f\"Post-training metrics: {post_train_metrics}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "proj-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
