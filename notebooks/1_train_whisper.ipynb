{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers torchaudio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data function provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(tsv_file, audio_dir, max_samples=100):\n",
    "    audio_files = []\n",
    "    transcripts = []\n",
    "    count = 0\n",
    "\n",
    "    try:\n",
    "        print(\"Loading dataset...\\n\\n\" + \"=\" * 50 + \"\\n\")\n",
    "        df = pd.read_csv(tsv_file, sep='\\t')\n",
    "        df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "        for index, row in df.iterrows():\n",
    "            audio_file = row['path']\n",
    "            if not audio_file.endswith(\".mp3\"):\n",
    "                audio_file += \".mp3\"\n",
    "            transcript = row['sentence']\n",
    "\n",
    "            audio_files.append(os.path.join(audio_dir, audio_file))\n",
    "            transcripts.append(transcript)\n",
    "            count += 1\n",
    "\n",
    "            if count >= max_samples:\n",
    "                print(f\"Finished loading {count} audio files and transcripts.\\n\\n\" + \"=\" * 50 + \"\\n\")\n",
    "                break\n",
    "\n",
    "        return audio_files, transcripts\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading Common Voice data: {e}\\n\")\n",
    "        return [], []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pre-trained Whisper model and special tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained Whisper tokenizer and processor\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\")\n",
    "\n",
    "# Custom tokens can be added based on frequent Tagalog phrases\n",
    "special_tokens = [\"[tagalog_token]\", \"[english_token]\"]  # Example tokens\n",
    "processor.tokenizer.add_tokens(special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained Whisper model and resize the token embeddings\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\")\n",
    "model.resize_token_embeddings(len(processor.tokenizer))  # Resize embeddings to include new tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processes the loaded data and trains the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class process_data(Dataset):\n",
    "    def __init__(self, audio_files, transcripts, processor):\n",
    "        self.audio_files = audio_files\n",
    "        self.transcripts = transcripts\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        audio_path = self.audio_files[idx]\n",
    "        transcript = self.transcripts[idx]\n",
    "\n",
    "        # Load and process audio\n",
    "        audio = torchaudio.load(audio_path)\n",
    "        input_features = self.processor(audio, sampling_rate=16000, return_tensors=\"pt\").input_features\n",
    "\n",
    "        # Encode the transcription\n",
    "        labels = self.processor.tokenizer(transcript, return_tensors=\"pt\").input_ids\n",
    "        return {\"input_features\": input_features.squeeze(), \"labels\": labels.squeeze()}\n",
    "\n",
    "# Load the data\n",
    "tsv_file = \"/path/to/train.tsv\"\n",
    "audio_dir = \"/path/to/audio_files\"\n",
    "audio_files, transcripts = load_data(tsv_file, audio_dir)\n",
    "\n",
    "# Prepare dataset and dataloader\n",
    "train_dataset = process_data(audio_files, transcripts, processor)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set training parameters\n",
    "num_epochs = 10\n",
    "output_dir = \"/content/checkpoints\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    for batch in train_dataloader:\n",
    "        input_features = batch[\"input_features\"].to(\"cuda\")\n",
    "        labels = batch[\"labels\"].to(\"cuda\")\n",
    "\n",
    "        outputs = model(input_features, labels=labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Save checkpoint every few epochs\n",
    "    if (epoch + 1) % 2 == 0:  # Save every 2 epochs\n",
    "        checkpoint_path = os.path.join(output_dir, f\"whisper_checkpoint_epoch_{epoch + 1}.pt\")\n",
    "        torch.save(model.state_dict(), checkpoint_path)\n",
    "        print(f\"Checkpoint saved at {checkpoint_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saves the model after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model_path = \"/content/fine_tuned_whisper\"\n",
    "model.save_pretrained(final_model_path)\n",
    "processor.save_pretrained(final_model_path)\n",
    "print(\"Fine-tuned model and processor saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "proj-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
