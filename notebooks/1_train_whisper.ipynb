{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers torchaudio jiwer sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "from jiwer import wer, cer\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim import AdamW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data function provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(tsv_file, audio_dir, max_samples=100):\n",
    "    audio_files = []\n",
    "    transcripts = []\n",
    "    count = 0\n",
    "\n",
    "    try:\n",
    "        print(\"Loading dataset...\\n\\n\" + \"=\" * 50 + \"\\n\")\n",
    "        df = pd.read_csv(tsv_file, sep='\\t')\n",
    "        df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "        for index, row in df.iterrows():\n",
    "            audio_file = row['path']\n",
    "            if not audio_file.endswith(\".mp3\"):\n",
    "                audio_file += \".mp3\"\n",
    "            transcript = row['sentence']\n",
    "\n",
    "            audio_files.append(os.path.join(audio_dir, audio_file))\n",
    "            transcripts.append(transcript)\n",
    "            count += 1\n",
    "\n",
    "            if count >= max_samples:\n",
    "                print(f\"Finished loading {count} audio files and transcripts.\\n\\n\" + \"=\" * 50 + \"\\n\")\n",
    "                break\n",
    "\n",
    "        return audio_files, transcripts\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading Common Voice data: {e}\\n\")\n",
    "        return [], []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProcessData(Dataset):\n",
    "    def __init__(self, audio_files, transcripts, processor):\n",
    "        self.audio_files = audio_files\n",
    "        self.transcripts = transcripts\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        audio_path = self.audio_files[idx]\n",
    "        transcript = self.transcripts[idx]\n",
    "\n",
    "        audio = torchaudio.load(audio_path)\n",
    "        input_features = self.processor(audio, sampling_rate=16000, return_tensors=\"pt\").input_features\n",
    "        labels = self.processor.tokenizer(transcript, return_tensors=\"pt\").input_ids\n",
    "\n",
    "        return {\"input_features\": input_features.squeeze(), \"labels\": labels.squeeze()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data and Prepare Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsv_file = \"/path/to/train.tsv\"\n",
    "audio_dir = \"/path/to/audio_files\"\n",
    "audio_files, transcripts = load_data(tsv_file, audio_dir)\n",
    "\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-base\")\n",
    "\n",
    "train_dataset = ProcessData(audio_files, transcripts, processor)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model and Optimizer Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-base\")\n",
    "model.resize_token_embeddings(len(processor.tokenizer))\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, processor, eval_dataloader):\n",
    "    model.eval()\n",
    "    total_preds = []\n",
    "    total_labels = []\n",
    "\n",
    "    total_wer = 0\n",
    "    total_cer = 0\n",
    "    num_samples = 0\n",
    "\n",
    "    for batch in eval_dataloader:\n",
    "        input_features = batch[\"input_features\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            generated_ids = model.generate(input_features)\n",
    "            preds = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "            refs = processor.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "            total_wer += sum([wer(r, p) for r, p in zip(refs, preds)]) / len(refs)\n",
    "            total_cer += sum([cer(r, p) for r, p in zip(refs, preds)]) / len(refs)\n",
    "            num_samples += len(refs)\n",
    "\n",
    "            total_preds.extend(preds)\n",
    "            total_labels.extend(refs)\n",
    "\n",
    "    avg_wer = total_wer / num_samples\n",
    "    avg_cer = total_cer / num_samples\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(total_labels, total_preds, average=\"weighted\")\n",
    "    accuracy = accuracy_score(total_labels, total_preds)\n",
    "\n",
    "    return {\n",
    "        \"WER\": avg_wer,\n",
    "        \"CER\": avg_cer,\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1-Score\": f1,\n",
    "        \"Accuracy\": accuracy\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to plot metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(metrics_before, metrics_after):\n",
    "    metrics_names = list(metrics_before.keys())\n",
    "    before_values = list(metrics_before.values())\n",
    "    after_values = list(metrics_after.values())\n",
    "\n",
    "    x = range(len(metrics_names))\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(x, before_values, width=0.4, label=\"Before Training\", color=\"skyblue\", align=\"center\")\n",
    "    plt.bar(x, after_values, width=0.4, label=\"After Training\", color=\"salmon\", align=\"edge\")\n",
    "\n",
    "    plt.xlabel(\"Metrics\")\n",
    "    plt.ylabel(\"Scores\")\n",
    "    plt.title(\"Model Performance Before and After Fine-Tuning\")\n",
    "    plt.xticks(x, metrics_names)\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Model Before Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataloader = DataLoader(ProcessData(audio_files, transcripts, processor), batch_size=4)\n",
    "\n",
    "metrics_before = evaluate_model(model, processor, eval_dataloader)\n",
    "print(\"Metrics Before Training:\", metrics_before)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop with Checkpoint Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "checkpoint_interval = 2\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for batch in train_dataloader:\n",
    "        input_features = batch[\"input_features\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        outputs = model(input_features, labels=labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    # Save checkpoint\n",
    "    if (epoch + 1) % checkpoint_interval == 0:\n",
    "        model.save_pretrained(f\"./checkpoint_epoch_{epoch + 1}\")\n",
    "        processor.save_pretrained(f\"./processor_epoch_{epoch + 1}\")\n",
    "\n",
    "    print(f\"Epoch {epoch + 1} completed. Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Model After Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_after = evaluate_model(model, processor, eval_dataloader)\n",
    "print(\"Metrics After Training:\", metrics_after)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Performance Before and After Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(metrics_before, metrics_after)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "proj-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
