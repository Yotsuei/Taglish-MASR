{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wandb transformers torchaudio jiwer sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import wandb\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration, get_scheduler\n",
    "from jiwer import wer, cer\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim import AdamW\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from google.colab import drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Initialize WandB\n",
    "wandb.init(project=\"taglish-whisper-fine-tuning\")\n",
    "\n",
    "# Configurations for flexible parameter adjustment\n",
    "config = {\n",
    "    \"tsv_file\": \"/content/drive/MyDrive/path/to/train.tsv\",\n",
    "    \"audio_dir\": \"/content/drive/MyDrive/path/to/audio_files\",\n",
    "    \"batch_size\": 4,\n",
    "    \"learning_rate\": 5e-5,\n",
    "    \"weight_decay\": 0.01,             # Added weight decay\n",
    "    \"warmup_steps\": 500,               # Added warm-up steps\n",
    "    \"epochs\": 3,\n",
    "    \"max_samples\": 100,\n",
    "    \"checkpoint_interval\": 2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the processor and model for Whisper\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-base\")\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-base\")\n",
    "model.resize_token_embeddings(len(processor.tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer setup\n",
    "optimizer = AdamW(model.parameters(), lr=config[\"learning_rate\"], weight_decay=config[\"weight_decay\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset with timestamp support\n",
    "def load_data(tsv_file, audio_dir, max_samples):\n",
    "    audio_files, transcripts, timestamps = [], [], []\n",
    "    df = pd.read_csv(tsv_file, sep='\\t').sample(frac=1).reset_index(drop=True)\n",
    "    count = 0\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        audio_file = row['path']\n",
    "        if not audio_file.endswith(\".mp3\"):\n",
    "            audio_file += \".mp3\"\n",
    "        transcript = row['sentence']\n",
    "        start_time, end_time = row.get(\"start\", 0), row.get(\"end\", None)\n",
    "\n",
    "        audio_files.append(os.path.join(audio_dir, audio_file))\n",
    "        transcripts.append(transcript)\n",
    "        timestamps.append((start_time, end_time))\n",
    "        count += 1\n",
    "\n",
    "        if count >= max_samples:\n",
    "            break\n",
    "    return audio_files, transcripts, timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset with timestamp support\n",
    "class ProcessData(Dataset):\n",
    "    def __init__(self, audio_files, transcripts, timestamps, processor):\n",
    "        self.audio_files = audio_files\n",
    "        self.transcripts = transcripts\n",
    "        self.timestamps = timestamps\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        audio_path = self.audio_files[idx]\n",
    "        transcript = self.transcripts[idx]\n",
    "        start_time, end_time = self.timestamps[idx]\n",
    "\n",
    "        audio, sample_rate = torchaudio.load(audio_path)\n",
    "        if sample_rate != 16000:\n",
    "            resampler = torchaudio.transforms.Resample(sample_rate, 16000)\n",
    "            audio = resampler(audio)\n",
    "\n",
    "        # Use timestamps to trim audio\n",
    "        if start_time or end_time:\n",
    "            start_frame, end_frame = int(start_time * 16000), int(end_time * 16000) if end_time else -1\n",
    "            audio = audio[:, start_frame:end_frame]\n",
    "\n",
    "        input_features = self.processor(audio.squeeze().numpy(), sampling_rate=16000, return_tensors=\"pt\").input_features\n",
    "        labels = self.processor(transcript, return_tensors=\"pt\").input_ids\n",
    "\n",
    "        return {\"input_features\": input_features.squeeze(), \"labels\": labels.squeeze()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "audio_files, transcripts, timestamps = load_data(config[\"tsv_file\"], config[\"audio_dir\"], config[\"max_samples\"])\n",
    "train_dataset = ProcessData(audio_files, transcripts, timestamps, processor)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=config[\"batch_size\"], shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scheduler setup with warmup steps\n",
    "num_training_steps = config[\"epochs\"] * len(train_dataloader)\n",
    "scheduler = get_scheduler(\n",
    "    \"cosine\",                 # You could also try \"linear\" or \"reduce_on_plateau\"\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=config[\"warmup_steps\"],\n",
    "    num_training_steps=num_training_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function with metrics\n",
    "def evaluate_model(model, processor, eval_dataloader):\n",
    "    model.eval()\n",
    "    total_preds, total_labels, total_wer, total_cer, num_samples = [], [], 0, 0, 0\n",
    "\n",
    "    for batch in eval_dataloader:\n",
    "        input_features = batch[\"input_features\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            generated_ids = model.generate(input_features)\n",
    "            preds = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "            refs = processor.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "            total_wer += sum([wer(r, p) for r, p in zip(refs, preds)]) / len(refs)\n",
    "            total_cer += sum([cer(r, p) for r, p in zip(refs, preds)]) / len(refs)\n",
    "            num_samples += len(refs)\n",
    "\n",
    "            total_preds.extend(preds)\n",
    "            total_labels.extend(refs)\n",
    "\n",
    "    avg_wer = total_wer / num_samples\n",
    "    avg_cer = total_cer / num_samples\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(total_labels, total_preds, average=\"weighted\")\n",
    "    accuracy = accuracy_score(total_labels, total_preds)\n",
    "\n",
    "    return {\n",
    "        \"WER\": avg_wer,\n",
    "        \"CER\": avg_cer,\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1-Score\": f1,\n",
    "        \"Accuracy\": accuracy\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop with WandB logging, scheduler step, and checkpoint saving\n",
    "eval_dataloader = DataLoader(ProcessData(audio_files, transcripts, timestamps, processor), batch_size=config[\"batch_size\"])\n",
    "metrics_before = evaluate_model(model, processor, eval_dataloader)\n",
    "print(\"Metrics Before Training:\", metrics_before)\n",
    "wandb.log(metrics_before)\n",
    "\n",
    "for epoch in range(config[\"epochs\"]):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_dataloader:\n",
    "        input_features = batch[\"input_features\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        outputs = model(input_features, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()  # Adjust learning rate\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    # Log epoch metrics and loss to WandB\n",
    "    wandb.log({\"loss\": total_loss / len(train_dataloader), \"epoch\": epoch + 1})\n",
    "\n",
    "    # Save checkpoint\n",
    "    if (epoch + 1) % config[\"checkpoint_interval\"] == 0:\n",
    "        checkpoint_dir = f\"/content/drive/MyDrive/whisper_checkpoints/checkpoint_epoch_{epoch + 1}\"\n",
    "        model.save_pretrained(checkpoint_dir)\n",
    "        processor.save_pretrained(checkpoint_dir)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1} completed. Loss: {total_loss / len(train_dataloader)}\")\n",
    "\n",
    "# Evaluation after training\n",
    "metrics_after = evaluate_model(model, processor, eval_dataloader)\n",
    "print(\"Metrics After Training:\", metrics_after)\n",
    "wandb.log(metrics_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting metrics comparison\n",
    "def plot_metrics(metrics_before, metrics_after):\n",
    "    metrics_names = list(metrics_before.keys())\n",
    "    before_values = list(metrics_before.values())\n",
    "    after_values = list(metrics_after.values())\n",
    "    x = range(len(metrics_names))\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(x, before_values, width=0.4, label=\"Before Training\", color=\"skyblue\", align=\"center\")\n",
    "    plt.bar(x, after_values, width=0.4, label=\"After Training\", color=\"salmon\", align=\"edge\")\n",
    "    plt.xlabel(\"Metrics\")\n",
    "    plt.ylabel(\"Scores\")\n",
    "    plt.title(\"Model Performance Before and After Fine-Tuning\")\n",
    "    plt.xticks(x, metrics_names)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_metrics(metrics_before, metrics_after)\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "proj-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
