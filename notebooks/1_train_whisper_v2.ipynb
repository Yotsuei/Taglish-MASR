{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X8S0cMJ9-SuW",
        "outputId": "54275cb0-3ab9-4fd9-d580-9938e081c02a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.18.5)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.0+cu121)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.5.0+cu121)\n",
            "Requirement already satisfied: jiwer in /usr/local/lib/python3.10/dist-packages (3.0.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.43)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.17.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (75.1.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.10/dist-packages (from wandb) (4.12.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: rapidfuzz<4,>=3 in /usr/local/lib/python3.10/dist-packages (from jiwer) (3.10.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install wandb transformers torch torchaudio jiwer scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rxIwC0z4-SuX"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import wandb\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torchaudio\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from transformers import WhisperProcessor, WhisperForConditionalGeneration, get_scheduler\n",
        "from jiwer import wer, cer\n",
        "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.optim import AdamW\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "import torch.nn.functional as F\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WoqRKUR_-SuX"
      },
      "outputs": [],
      "source": [
        "# Enhanced configurations\n",
        "config = {\n",
        "    \"tsv_file\": \"/content/drive/Shareddrives/CS307-Thesis/Dataset/single-speaker/validated.tsv\",\n",
        "    \"audio_dir\": \"/content/drive/Shareddrives/CS307-Thesis/Dataset/single-speaker/\",\n",
        "    \"batch_size\": 4,\n",
        "    \"learning_rate\": 5e-5,\n",
        "    \"weight_decay\": 0.01,\n",
        "    \"warmup_steps\": 500,\n",
        "    \"epochs\": 3,\n",
        "    \"max_samples\": 9,\n",
        "    \"checkpoint_interval\": 2,\n",
        "    \"validation_split\": 0.1,\n",
        "    \"max_grad_norm\": 1.0,\n",
        "    \"early_stopping_patience\": 3,\n",
        "    \"mixed_precision\": True,\n",
        "    \"gradient_accumulation_steps\": 4\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HeFoIWaN-SuY"
      },
      "outputs": [],
      "source": [
        "def load_data(tsv_file, audio_dir, max_samples=None):\n",
        "    \"\"\"\n",
        "    Load data from TSV file with timestamp handling, compatible with both \"sec\" and \"min:sec\" formats.\n",
        "    \"\"\"\n",
        "    audio_files, transcripts, timestamps = [], [], []\n",
        "\n",
        "    # Read TSV file\n",
        "    df = pd.read_csv(tsv_file, sep='\\t')\n",
        "    required_columns = ['path', 'start_time', 'end_time', 'sentence']\n",
        "\n",
        "    # Verify all required columns are present\n",
        "    if not all(col in df.columns for col in required_columns):\n",
        "        raise ValueError(f\"TSV file must contain columns: {required_columns}\")\n",
        "\n",
        "    # Shuffle and limit samples if specified\n",
        "    df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "    if max_samples:\n",
        "        df = df.head(max_samples)\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        audio_file = row['path']\n",
        "        if not audio_file.endswith((\".mp3\", \".wav\", \".flac\")):\n",
        "            print(f\"Skipping unsupported file type: {audio_file}\")\n",
        "            continue\n",
        "\n",
        "        full_audio_path = os.path.join(audio_dir, audio_file)\n",
        "        if not os.path.exists(full_audio_path):\n",
        "            print(f\"Warning: Audio file not found: {full_audio_path}\")\n",
        "            continue\n",
        "\n",
        "        # Parse timestamps\n",
        "        def parse_time(time_str):\n",
        "            try:\n",
        "                # Check if time is already in seconds\n",
        "                return float(time_str)\n",
        "            except ValueError:\n",
        "                # Convert from \"min:sec\" format to seconds\n",
        "                minutes, seconds = map(float, time_str.split(\":\"))\n",
        "                return minutes * 60 + seconds\n",
        "\n",
        "        try:\n",
        "            start_time = parse_time(row['start_time'])\n",
        "            end_time = parse_time(row['end_time'])\n",
        "        except Exception as e:\n",
        "            print(f\"Error parsing timestamps for {audio_file}: {str(e)}\")\n",
        "            continue\n",
        "\n",
        "        audio_files.append(full_audio_path)\n",
        "        transcripts.append(row['sentence'])\n",
        "        timestamps.append((start_time, end_time))\n",
        "\n",
        "    return audio_files, transcripts, timestamps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OvHBaGuX-SuY"
      },
      "outputs": [],
      "source": [
        "class EarlyStopping:\n",
        "    def __init__(self, patience=3, min_delta=0):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.counter = 0\n",
        "        self.best_loss = None\n",
        "        self.should_stop = False\n",
        "\n",
        "    def __call__(self, val_loss):\n",
        "        if self.best_loss is None:\n",
        "            self.best_loss = val_loss\n",
        "        elif val_loss > self.best_loss - self.min_delta:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.should_stop = True\n",
        "        else:\n",
        "            self.best_loss = val_loss\n",
        "            self.counter = 0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch):\n",
        "    \"\"\"\n",
        "    Custom collate function to handle variable length sequences\n",
        "    \"\"\"\n",
        "    # Filter out any None or empty items\n",
        "    batch = [item for item in batch if item is not None]\n",
        "\n",
        "    if len(batch) == 0:\n",
        "        return {}\n",
        "\n",
        "    # Get maximum lengths\n",
        "    max_input_length = max(item['input_features'].size(1) for item in batch)\n",
        "    max_label_length = max(item['labels'].size(0) for item in batch)\n",
        "\n",
        "    # Initialize padded tensors\n",
        "    batch_size = len(batch)\n",
        "    padded_input_features = torch.zeros(batch_size, 80, max_input_length)\n",
        "    padded_labels = torch.full((batch_size, max_label_length), -100, dtype=torch.long)  # -100 is often used for padding in transformers\n",
        "\n",
        "    # Fill padded tensors\n",
        "    for i, item in enumerate(batch):\n",
        "        # Pad input features\n",
        "        input_features = item['input_features']\n",
        "        length = input_features.size(1)\n",
        "        padded_input_features[i, :, :length] = input_features\n",
        "\n",
        "        # Pad labels\n",
        "        labels = item['labels']\n",
        "        length = labels.size(0)\n",
        "        padded_labels[i, :length] = labels\n",
        "\n",
        "    return {\n",
        "        'input_features': padded_input_features,\n",
        "        'labels': padded_labels\n",
        "    }"
      ],
      "metadata": {
        "id": "Dpw6Yh0KESQG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Rd4eaXD-SuZ"
      },
      "outputs": [],
      "source": [
        "class ProcessData(Dataset):\n",
        "    def __init__(self, audio_files, transcripts, timestamps, processor, training=True):\n",
        "        self.audio_files = audio_files\n",
        "        self.transcripts = transcripts\n",
        "        self.timestamps = timestamps\n",
        "        self.processor = processor\n",
        "        self.training = training\n",
        "\n",
        "        # Initialize transforms for training\n",
        "        if training:\n",
        "            self.time_stretch = torchaudio.transforms.TimeStretch(fixed_rate=0.98)\n",
        "            self.freq_mask = torchaudio.transforms.FrequencyMasking(freq_mask_param=30)\n",
        "            self.time_mask = torchaudio.transforms.TimeMasking(time_mask_param=100)\n",
        "\n",
        "    def apply_audio_transforms(self, audio):\n",
        "        \"\"\"Apply audio augmentation transforms with proper complex conversion\"\"\"\n",
        "        if self.training:\n",
        "            # Convert to complex representation for TimeStretch\n",
        "            spec = torch.stft(\n",
        "                audio,\n",
        "                n_fft=400,\n",
        "                hop_length=100,\n",
        "                win_length=400,\n",
        "                window=torch.hann_window(400),\n",
        "                return_complex=True\n",
        "            )\n",
        "\n",
        "            # Apply TimeStretch\n",
        "            spec = self.time_stretch(spec)\n",
        "\n",
        "            # Convert back to time domain\n",
        "            audio = torch.istft(\n",
        "                spec,\n",
        "                n_fft=400,\n",
        "                hop_length=100,\n",
        "                win_length=400,\n",
        "                window=torch.hann_window(400)\n",
        "            )\n",
        "\n",
        "            # Apply other transforms\n",
        "            audio = self.freq_mask(audio.unsqueeze(0)).squeeze(0)\n",
        "            audio = self.time_mask(audio.unsqueeze(0)).squeeze(0)\n",
        "\n",
        "        return audio\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.audio_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        audio_path = self.audio_files[idx]\n",
        "        transcript = self.transcripts[idx]\n",
        "        start_time, end_time = self.timestamps[idx]\n",
        "\n",
        "        try:\n",
        "            # Load audio\n",
        "            audio, sample_rate = torchaudio.load(audio_path)\n",
        "\n",
        "            # Convert to mono if stereo\n",
        "            if audio.shape[0] > 1:\n",
        "                audio = torch.mean(audio, dim=0, keepdim=True)\n",
        "\n",
        "            # Resample if necessary\n",
        "            if sample_rate != 16000:\n",
        "                resampler = torchaudio.transforms.Resample(sample_rate, 16000)\n",
        "                audio = resampler(audio)\n",
        "\n",
        "            # Apply timestamps to trim audio\n",
        "            start_frame = int(start_time * 16000)\n",
        "            end_frame = int(end_time * 16000)\n",
        "            audio = audio[:, start_frame:end_frame]\n",
        "\n",
        "            # Apply audio transforms if in training mode\n",
        "            if self.training:\n",
        "                audio = self.apply_audio_transforms(audio.squeeze())\n",
        "                audio = audio.unsqueeze(0)\n",
        "\n",
        "            # Process audio features\n",
        "            input_features = self.processor(\n",
        "                audio.squeeze().numpy(),\n",
        "                sampling_rate=16000,\n",
        "                return_tensors=\"pt\"\n",
        "            ).input_features\n",
        "\n",
        "            # Process labels and ensure correct type\n",
        "            labels = self.processor(\n",
        "                text=transcript,\n",
        "                return_tensors=\"pt\"\n",
        "            ).input_ids.squeeze()\n",
        "\n",
        "            # Ensure labels are of type Long\n",
        "            labels = labels.long()\n",
        "\n",
        "            return {\n",
        "                \"input_features\": input_features.squeeze(),\n",
        "                \"labels\": labels\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {audio_path}: {str(e)}\")\n",
        "            return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g4tHWUHS-SuZ"
      },
      "outputs": [],
      "source": [
        "def create_dataloaders(audio_files, transcripts, timestamps, processor, config):\n",
        "    # Create full dataset\n",
        "    full_dataset = ProcessData(audio_files, transcripts, timestamps, processor)\n",
        "\n",
        "    # Calculate split sizes\n",
        "    val_size = int(len(full_dataset) * config[\"validation_split\"])\n",
        "    train_size = len(full_dataset) - val_size\n",
        "\n",
        "    # Split dataset\n",
        "    train_dataset, val_dataset = random_split(\n",
        "        full_dataset,\n",
        "        [train_size, val_size],\n",
        "        generator=torch.Generator().manual_seed(42)\n",
        "    )\n",
        "\n",
        "    # Create training dataloader with shuffling and custom collate_fn\n",
        "    train_dataloader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=config[\"batch_size\"],\n",
        "        shuffle=True,\n",
        "        num_workers=2,\n",
        "        pin_memory=True,\n",
        "        collate_fn=collate_fn\n",
        "    )\n",
        "\n",
        "    # Create validation dataloader without shuffling\n",
        "    val_dataloader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=config[\"batch_size\"],\n",
        "        shuffle=False,\n",
        "        num_workers=2,\n",
        "        pin_memory=True,\n",
        "        collate_fn=collate_fn\n",
        "    )\n",
        "\n",
        "    return train_dataloader, val_dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YhrMhW5p-SuZ"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, processor, dataloader, device):\n",
        "    \"\"\"\n",
        "    Evaluate the model using multiple metrics\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            # Skip empty batches\n",
        "            if not batch:\n",
        "                continue\n",
        "\n",
        "            input_features = batch[\"input_features\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "\n",
        "            # Forward pass without forced_decoder_ids\n",
        "            outputs = model(\n",
        "                input_features,\n",
        "                labels=labels\n",
        "            )\n",
        "\n",
        "            total_loss += outputs.loss.item()\n",
        "\n",
        "            # Generate predictions with forced language and task\n",
        "            generated_ids = model.generate(\n",
        "                input_features,\n",
        "                forced_decoder_ids=processor.get_decoder_prompt_ids(language=\"en\", task=\"transcribe\"),\n",
        "                max_length=256  # Adjust as needed\n",
        "            )\n",
        "\n",
        "            # Decode predictions and labels\n",
        "            decoded_preds = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
        "            # Mask out padding tokens (-100) in labels before decoding\n",
        "            labels_clean = torch.where(labels != -100, labels, processor.tokenizer.pad_token_id)\n",
        "            decoded_labels = processor.batch_decode(labels_clean, skip_special_tokens=True)\n",
        "\n",
        "            # Store non-empty predictions and labels\n",
        "            for pred, label in zip(decoded_preds, decoded_labels):\n",
        "                if pred.strip() and label.strip():  # Only store non-empty strings\n",
        "                    all_preds.append(pred)\n",
        "                    all_labels.append(label)\n",
        "\n",
        "    # Calculate average loss\n",
        "    avg_loss = total_loss / len(dataloader) if len(dataloader) > 0 else float('inf')\n",
        "\n",
        "    # Only calculate other metrics if we have predictions\n",
        "    if len(all_preds) > 0 and len(all_labels) > 0:\n",
        "        # Calculate WER and CER\n",
        "        wer_score = wer(all_labels, all_preds)\n",
        "        cer_score = cer(all_labels, all_preds)\n",
        "\n",
        "        # Tokenize for word-level metrics\n",
        "        tokenized_labels = [text.split() for text in all_labels]\n",
        "        tokenized_preds = [text.split() for text in all_preds]\n",
        "\n",
        "        # Ensure equal length for calculating precision/recall\n",
        "        min_len = min(len(tokenized_labels), len(tokenized_preds))\n",
        "        tokenized_labels = tokenized_labels[:min_len]\n",
        "        tokenized_preds = tokenized_preds[:min_len]\n",
        "\n",
        "        # Flatten the lists\n",
        "        flat_labels = [word for sentence in tokenized_labels for word in sentence]\n",
        "        flat_preds = [word for sentence in tokenized_preds for word in sentence]\n",
        "\n",
        "        # Calculate precision, recall, F1 only if we have matching samples\n",
        "        if len(flat_labels) == len(flat_preds) and len(flat_labels) > 0:\n",
        "            precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "                flat_labels,\n",
        "                flat_preds,\n",
        "                average='weighted',\n",
        "                zero_division=0\n",
        "            )\n",
        "            accuracy = accuracy_score(flat_labels, flat_preds)\n",
        "        else:\n",
        "            precision = recall = f1 = accuracy = 0.0\n",
        "    else:\n",
        "        wer_score = cer_score = precision = recall = f1 = accuracy = 0.0\n",
        "\n",
        "    metrics = {\n",
        "        \"loss\": avg_loss,\n",
        "        \"WER\": wer_score,\n",
        "        \"CER\": cer_score,\n",
        "        \"Precision\": precision,\n",
        "        \"Recall\": recall,\n",
        "        \"F1-Score\": f1,\n",
        "        \"Accuracy\": accuracy\n",
        "    }\n",
        "\n",
        "    return metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "78_1ayt4-Sua"
      },
      "outputs": [],
      "source": [
        "def train_model(config, checkpoint_path=None):\n",
        "    # Initialize wandb\n",
        "    wandb.init(project=\"taglish-whisper-fine-tuning\", config=config)\n",
        "\n",
        "    # Set device\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Load model and processor\n",
        "    if checkpoint_path and os.path.exists(checkpoint_path):\n",
        "        print(f\"Loading checkpoint from {checkpoint_path}\")\n",
        "        processor = WhisperProcessor.from_pretrained(checkpoint_path)\n",
        "        model = WhisperForConditionalGeneration.from_pretrained(checkpoint_path)\n",
        "\n",
        "        # Load training state\n",
        "        training_state = torch.load(os.path.join(checkpoint_path, 'training_state.pt'))\n",
        "        start_epoch = training_state['epoch'] + 1\n",
        "        print(f\"Resuming from epoch {start_epoch}\")\n",
        "    else:\n",
        "        print(\"Starting fresh training with base model\")\n",
        "        processor = WhisperProcessor.from_pretrained(\"openai/whisper-base\")\n",
        "        model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-base\")\n",
        "        start_epoch = 0\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    # Load data\n",
        "    audio_files, transcripts, timestamps = load_data(\n",
        "        config[\"tsv_file\"],\n",
        "        config[\"audio_dir\"],\n",
        "        config[\"max_samples\"]\n",
        "    )\n",
        "\n",
        "    # Create dataloaders\n",
        "    train_dataloader, val_dataloader = create_dataloaders(\n",
        "        audio_files,\n",
        "        transcripts,\n",
        "        timestamps,\n",
        "        processor,\n",
        "        config\n",
        "    )\n",
        "\n",
        "    # Setup optimizer and scheduler\n",
        "    optimizer = AdamW(\n",
        "        model.parameters(),\n",
        "        lr=config[\"learning_rate\"],\n",
        "        weight_decay=config[\"weight_decay\"]\n",
        "    )\n",
        "\n",
        "    num_training_steps = config[\"epochs\"] * len(train_dataloader)\n",
        "    scheduler = get_scheduler(\n",
        "        \"cosine\",\n",
        "        optimizer=optimizer,\n",
        "        num_warmup_steps=config[\"warmup_steps\"],\n",
        "        num_training_steps=num_training_steps\n",
        "    )\n",
        "\n",
        "    # Load optimizer and scheduler states if resuming\n",
        "    if checkpoint_path and os.path.exists(checkpoint_path):\n",
        "        optimizer.load_state_dict(training_state['optimizer_state_dict'])\n",
        "        scheduler.load_state_dict(training_state['scheduler_state_dict'])\n",
        "\n",
        "    # Setup mixed precision training\n",
        "    scaler = torch.amp.GradScaler() if config[\"mixed_precision\"] else None\n",
        "\n",
        "    # Setup early stopping\n",
        "    early_stopping = EarlyStopping(patience=config[\"early_stopping_patience\"])\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(start_epoch, config[\"epochs\"]):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        for i, batch in enumerate(train_dataloader):\n",
        "            # Skip empty batches\n",
        "            if not batch:\n",
        "                continue\n",
        "\n",
        "            input_features = batch[\"input_features\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "\n",
        "            # Mixed precision training\n",
        "            if config[\"mixed_precision\"]:\n",
        "                with torch.amp.autocast(device_type='cuda', dtype=torch.float16):\n",
        "                    outputs = model(\n",
        "                        input_features,\n",
        "                        labels=labels,\n",
        "                        forced_decoder_ids=processor.get_decoder_prompt_ids(language=\"en\", task=\"transcribe\")\n",
        "                    )\n",
        "                    loss = outputs.loss / config[\"gradient_accumulation_steps\"]\n",
        "\n",
        "                scaler.scale(loss).backward()\n",
        "\n",
        "                if (i + 1) % config[\"gradient_accumulation_steps\"] == 0:\n",
        "                    scaler.unscale_(optimizer)\n",
        "                    clip_grad_norm_(model.parameters(), config[\"max_grad_norm\"])\n",
        "                    scaler.step(optimizer)\n",
        "                    scaler.update()\n",
        "                    scheduler.step()\n",
        "                    optimizer.zero_grad()\n",
        "            else:\n",
        "                outputs = model(\n",
        "                    input_features,\n",
        "                    labels=labels,\n",
        "                    forced_decoder_ids=processor.get_decoder_prompt_ids(language=\"en\", task=\"transcribe\")\n",
        "                )\n",
        "                loss = outputs.loss / config[\"gradient_accumulation_steps\"]\n",
        "                loss.backward()\n",
        "\n",
        "                if (i + 1) % config[\"gradient_accumulation_steps\"] == 0:\n",
        "                    clip_grad_norm_(model.parameters(), config[\"max_grad_norm\"])\n",
        "                    optimizer.step()\n",
        "                    scheduler.step()\n",
        "                    optimizer.zero_grad()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Log batch progress\n",
        "            if i % 10 == 0:\n",
        "                wandb.log({\n",
        "                    \"batch_loss\": loss.item(),\n",
        "                    \"learning_rate\": scheduler.get_last_lr()[0]\n",
        "                })\n",
        "\n",
        "        # Validation phase\n",
        "        val_metrics = evaluate_model(model, processor, val_dataloader, device)\n",
        "\n",
        "        # Log metrics\n",
        "        wandb.log({\n",
        "            \"train_loss\": total_loss / len(train_dataloader),\n",
        "            **val_metrics,\n",
        "            \"epoch\": epoch + 1\n",
        "        })\n",
        "\n",
        "        # Early stopping check\n",
        "        early_stopping(val_metrics[\"loss\"])\n",
        "        if early_stopping.should_stop:\n",
        "            print(f\"Early stopping triggered at epoch {epoch + 1}\")\n",
        "            break\n",
        "\n",
        "        # Save checkpoint\n",
        "        if (epoch + 1) % config[\"checkpoint_interval\"] == 0:\n",
        "            checkpoint_dir = f\"/content/drive/Shareddrives/CS307-Thesis/Dataset/whisper_checkpoints/checkpoint_epoch_{epoch + 1}\"\n",
        "            os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "            model.save_pretrained(checkpoint_dir)\n",
        "            processor.save_pretrained(checkpoint_dir)\n",
        "\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'scheduler_state_dict': scheduler.state_dict(),\n",
        "                'metrics': val_metrics\n",
        "            }, os.path.join(checkpoint_dir, 'training_state.pt'))\n",
        "\n",
        "    wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 587
        },
        "id": "jPTdgrf2-Sua",
        "outputId": "0c9cf74a-0576-4a52-b77e-e457fde2fd2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33midcsalvame\u001b[0m (\u001b[33midcsalvame-n-a\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.18.5"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20241103_082153-0f8l6b7t</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/idcsalvame-n-a/taglish-whisper-fine-tuning/runs/0f8l6b7t' target=\"_blank\">proud-lake-10</a></strong> to <a href='https://wandb.ai/idcsalvame-n-a/taglish-whisper-fine-tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/idcsalvame-n-a/taglish-whisper-fine-tuning' target=\"_blank\">https://wandb.ai/idcsalvame-n-a/taglish-whisper-fine-tuning</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/idcsalvame-n-a/taglish-whisper-fine-tuning/runs/0f8l6b7t' target=\"_blank\">https://wandb.ai/idcsalvame-n-a/taglish-whisper-fine-tuning/runs/0f8l6b7t</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50358, 50359, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Accuracy</td><td>▁▁▁</td></tr><tr><td>CER</td><td>▁▁▁</td></tr><tr><td>F1-Score</td><td>▁▁▁</td></tr><tr><td>Precision</td><td>▁▁▁</td></tr><tr><td>Recall</td><td>▁▁▁</td></tr><tr><td>WER</td><td>▁▁▁</td></tr><tr><td>batch_loss</td><td>▁▄█</td></tr><tr><td>epoch</td><td>▁▅█</td></tr><tr><td>learning_rate</td><td>▁▁▁</td></tr><tr><td>train_loss</td><td>▃▁█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Accuracy</td><td>0</td></tr><tr><td>CER</td><td>0</td></tr><tr><td>F1-Score</td><td>0</td></tr><tr><td>Precision</td><td>0</td></tr><tr><td>Recall</td><td>0</td></tr><tr><td>WER</td><td>0</td></tr><tr><td>batch_loss</td><td>2.16708</td></tr><tr><td>epoch</td><td>3</td></tr><tr><td>learning_rate</td><td>0</td></tr><tr><td>loss</td><td>inf</td></tr><tr><td>train_loss</td><td>1.86874</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">proud-lake-10</strong> at: <a href='https://wandb.ai/idcsalvame-n-a/taglish-whisper-fine-tuning/runs/0f8l6b7t' target=\"_blank\">https://wandb.ai/idcsalvame-n-a/taglish-whisper-fine-tuning/runs/0f8l6b7t</a><br/> View project at: <a href='https://wandb.ai/idcsalvame-n-a/taglish-whisper-fine-tuning' target=\"_blank\">https://wandb.ai/idcsalvame-n-a/taglish-whisper-fine-tuning</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20241103_082153-0f8l6b7t/logs</code>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    # To resume from a checkpoint, specify the checkpoint path\n",
        "    checkpoint_path = \"/content/drive/Shareddrives/CS307-Thesis/Dataset/whisper_checkpoints/checkpoint_epoch_2\"  # Replace X with the epoch number\n",
        "    train_model(config, checkpoint_path=checkpoint_path)\n",
        "\n",
        "    # Or to start fresh training:\n",
        "    # train_model(config)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.6"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}