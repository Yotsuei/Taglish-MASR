{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X8S0cMJ9-SuW",
        "outputId": "d4fb7a26-f282-4a98-c097-64f8e9c43f2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.18.5)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.0+cu121)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.5.0+cu121)\n",
            "Requirement already satisfied: jiwer in /usr/local/lib/python3.10/dist-packages (3.0.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.43)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.17.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (75.1.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.10/dist-packages (from wandb) (4.12.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: rapidfuzz<4,>=3 in /usr/local/lib/python3.10/dist-packages (from jiwer) (3.10.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install wandb transformers torch torchaudio jiwer scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "rxIwC0z4-SuX"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import wandb\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torchaudio\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from transformers import WhisperProcessor, WhisperForConditionalGeneration, get_scheduler\n",
        "from jiwer import wer, cer\n",
        "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.optim import AdamW\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "WoqRKUR_-SuX"
      },
      "outputs": [],
      "source": [
        "# Enhanced configurations\n",
        "config = {\n",
        "    \"tsv_file\": \"/content/drive/Shareddrives/CS307-Thesis/Dataset/single-speaker/validated.tsv\",\n",
        "    \"audio_dir\": \"/content/drive/Shareddrives/CS307-Thesis/Dataset/single-speaker/\",\n",
        "    \"batch_size\": 4,\n",
        "    \"learning_rate\": 5e-5,\n",
        "    \"weight_decay\": 0.01,\n",
        "    \"warmup_steps\": 500,\n",
        "    \"epochs\": 3,\n",
        "    \"max_samples\": 100,\n",
        "    \"checkpoint_interval\": 2,\n",
        "    \"validation_split\": 0.1,\n",
        "    \"max_grad_norm\": 1.0,\n",
        "    \"early_stopping_patience\": 3,\n",
        "    \"mixed_precision\": True,\n",
        "    \"gradient_accumulation_steps\": 4\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "HeFoIWaN-SuY"
      },
      "outputs": [],
      "source": [
        "def load_data(tsv_file, audio_dir, max_samples=None):\n",
        "    \"\"\"\n",
        "    Load data from TSV file with support for WAV, MP3, and FLAC formats.\n",
        "    \"\"\"\n",
        "    audio_files, transcripts, timestamps = [], [], []\n",
        "\n",
        "    # Read TSV file with explicit column names\n",
        "    df = pd.read_csv(tsv_file, sep='\\t')\n",
        "    required_columns = ['path', 'start_time', 'end_time', 'sentence']\n",
        "\n",
        "    # Verify all required columns are present\n",
        "    if not all(col in df.columns for col in required_columns):\n",
        "        raise ValueError(f\"TSV file must contain columns: {required_columns}\")\n",
        "\n",
        "    # Shuffle the dataframe\n",
        "    df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "    # Limit samples if specified\n",
        "    if max_samples:\n",
        "        df = df.head(max_samples)\n",
        "\n",
        "    # Define supported audio formats\n",
        "    supported_formats = (\".wav\", \".mp3\", \".flac\")\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        base_audio_file = row['path']\n",
        "\n",
        "        # Look for file in supported formats\n",
        "        for ext in supported_formats:\n",
        "            audio_file = base_audio_file if base_audio_file.endswith(ext) else f\"{base_audio_file}{ext}\"\n",
        "            full_audio_path = os.path.join(audio_dir, audio_file)\n",
        "\n",
        "            if os.path.exists(full_audio_path):\n",
        "                audio_files.append(full_audio_path)\n",
        "                transcripts.append(row['sentence'])\n",
        "                timestamps.append((float(row['start_time']), float(row['end_time'])))\n",
        "                break\n",
        "        else:\n",
        "            print(f\"Warning: Audio file not found for base name '{base_audio_file}' with supported formats.\")\n",
        "\n",
        "    return audio_files, transcripts, timestamps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "OvHBaGuX-SuY"
      },
      "outputs": [],
      "source": [
        "class EarlyStopping:\n",
        "    def __init__(self, patience=3, min_delta=0):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.counter = 0\n",
        "        self.best_loss = None\n",
        "        self.should_stop = False\n",
        "\n",
        "    def __call__(self, val_loss):\n",
        "        if self.best_loss is None:\n",
        "            self.best_loss = val_loss\n",
        "        elif val_loss > self.best_loss - self.min_delta:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.should_stop = True\n",
        "        else:\n",
        "            self.best_loss = val_loss\n",
        "            self.counter = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "9Rd4eaXD-SuZ"
      },
      "outputs": [],
      "source": [
        "class ProcessData(Dataset):\n",
        "    def __init__(self, audio_files, transcripts, timestamps, processor, training=True):\n",
        "        self.audio_files = audio_files\n",
        "        self.transcripts = transcripts\n",
        "        self.timestamps = timestamps\n",
        "        self.processor = processor\n",
        "        self.training = training\n",
        "\n",
        "        self.audio_transforms = torch.nn.Sequential(\n",
        "            torchaudio.transforms.TimeStretch(fixed_rate=0.98),\n",
        "            torchaudio.transforms.FrequencyMasking(freq_mask_param=30),\n",
        "            torchaudio.transforms.TimeMasking(time_mask_param=100)\n",
        "        ) if training else None\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.audio_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        audio_path = self.audio_files[idx]\n",
        "        transcript = self.transcripts[idx]\n",
        "        start_time, end_time = self.timestamps[idx]\n",
        "\n",
        "        try:\n",
        "            audio, sample_rate = torchaudio.load(audio_path)\n",
        "\n",
        "            # Resample if necessary\n",
        "            if sample_rate != 16000:\n",
        "                resampler = torchaudio.transforms.Resample(sample_rate, 16000)\n",
        "                audio = resampler(audio)\n",
        "\n",
        "            # Apply audio augmentation during training\n",
        "            if self.training and self.audio_transforms:\n",
        "                audio = self.audio_transforms(audio)\n",
        "\n",
        "            # Apply timestamps to trim audio\n",
        "            start_frame = int(start_time * 16000)\n",
        "            end_frame = int(end_time * 16000)\n",
        "            audio = audio[:, start_frame:end_frame]\n",
        "\n",
        "            input_features = self.processor(\n",
        "                audio.squeeze().numpy(),\n",
        "                sampling_rate=16000,\n",
        "                return_tensors=\"pt\"\n",
        "            ).input_features\n",
        "\n",
        "            labels = self.processor(\n",
        "                transcript,\n",
        "                return_tensors=\"pt\"\n",
        "            ).input_ids\n",
        "\n",
        "            return {\n",
        "                \"input_features\": input_features.squeeze(),\n",
        "                \"labels\": labels.squeeze()\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {audio_path}: {str(e)}\")\n",
        "            # Return a zero tensor with appropriate shape as fallback\n",
        "            return {\n",
        "                \"input_features\": torch.zeros(80, 3000),\n",
        "                \"labels\": torch.zeros(100)\n",
        "            }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "g4tHWUHS-SuZ"
      },
      "outputs": [],
      "source": [
        "def create_dataloaders(audio_files, transcripts, timestamps, processor, config):\n",
        "    # Create full dataset\n",
        "    full_dataset = ProcessData(audio_files, transcripts, timestamps, processor)\n",
        "\n",
        "    # Calculate split sizes\n",
        "    val_size = int(len(full_dataset) * config[\"validation_split\"])\n",
        "    train_size = len(full_dataset) - val_size\n",
        "\n",
        "    # Split dataset\n",
        "    train_dataset, val_dataset = random_split(\n",
        "        full_dataset,\n",
        "        [train_size, val_size],\n",
        "        generator=torch.Generator().manual_seed(42)\n",
        "    )\n",
        "\n",
        "    # Create training dataloader with shuffling\n",
        "    train_dataloader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=config[\"batch_size\"],\n",
        "        shuffle=True,\n",
        "        num_workers=2,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    # Create validation dataloader without shuffling\n",
        "    val_dataloader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=config[\"batch_size\"],\n",
        "        shuffle=False,\n",
        "        num_workers=2,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    return train_dataloader, val_dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "YhrMhW5p-SuZ"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, processor, dataloader, device):\n",
        "    \"\"\"\n",
        "    Evaluate the model using multiple metrics:\n",
        "    - Loss\n",
        "    - Word Error Rate (WER)\n",
        "    - Character Error Rate (CER)\n",
        "    - Precision\n",
        "    - Recall\n",
        "    - F1-Score\n",
        "    - Accuracy\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            input_features = batch[\"input_features\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "\n",
        "            # Get model outputs and loss\n",
        "            outputs = model(input_features, labels=labels)\n",
        "            total_loss += outputs.loss.item()\n",
        "\n",
        "            # Generate predictions\n",
        "            generated_ids = model.generate(input_features)\n",
        "\n",
        "            # Decode predictions and labels\n",
        "            preds = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
        "            refs = processor.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "            # Store predictions and references for metric calculation\n",
        "            all_preds.extend(preds)\n",
        "            all_labels.extend(refs)\n",
        "\n",
        "    # Calculate average loss\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "\n",
        "    # Calculate WER and CER\n",
        "    wer_score = wer(all_labels, all_preds)\n",
        "    cer_score = cer(all_labels, all_preds)\n",
        "\n",
        "    # Calculate precision, recall, and F1-score\n",
        "    # We'll treat each word as a token for these metrics\n",
        "    tokenized_labels = [text.split() for text in all_labels]\n",
        "    tokenized_preds = [text.split() for text in all_preds]\n",
        "\n",
        "    # Flatten the lists for sklearn metrics\n",
        "    flat_labels = [word for sentence in tokenized_labels for word in sentence]\n",
        "    flat_preds = [word for sentence in tokenized_preds for word in sentence]\n",
        "\n",
        "    # Calculate precision, recall, F1\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "        flat_labels,\n",
        "        flat_preds,\n",
        "        average='weighted',\n",
        "        zero_division=0\n",
        "    )\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = accuracy_score(flat_labels, flat_preds)\n",
        "\n",
        "    # Compile all metrics\n",
        "    metrics = {\n",
        "        \"loss\": avg_loss,\n",
        "        \"WER\": wer_score,\n",
        "        \"CER\": cer_score,\n",
        "        \"Precision\": precision,\n",
        "        \"Recall\": recall,\n",
        "        \"F1-Score\": f1,\n",
        "        \"Accuracy\": accuracy\n",
        "    }\n",
        "\n",
        "    # Print metrics for monitoring\n",
        "    print(\"\\nEvaluation Metrics:\")\n",
        "    for metric_name, value in metrics.items():\n",
        "        print(f\"{metric_name}: {value:.4f}\")\n",
        "\n",
        "    return metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "78_1ayt4-Sua"
      },
      "outputs": [],
      "source": [
        "def train_model(config):\n",
        "    # Initialize wandb\n",
        "    wandb.init(project=\"taglish-whisper-fine-tuning\", config=config)\n",
        "\n",
        "    # Set device\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Load model and processor\n",
        "    processor = WhisperProcessor.from_pretrained(\"openai/whisper-base\")\n",
        "    model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-base\")\n",
        "    model.to(device)\n",
        "\n",
        "    # Load data\n",
        "    audio_files, transcripts, timestamps = load_data(\n",
        "        config[\"tsv_file\"],\n",
        "        config[\"audio_dir\"],\n",
        "        config[\"max_samples\"]\n",
        "    )\n",
        "\n",
        "    # Create dataloaders\n",
        "    train_dataloader, val_dataloader = create_dataloaders(\n",
        "        audio_files,\n",
        "        transcripts,\n",
        "        timestamps,\n",
        "        processor,\n",
        "        config\n",
        "    )\n",
        "\n",
        "    # Setup optimizer and scheduler\n",
        "    optimizer = AdamW(\n",
        "        model.parameters(),\n",
        "        lr=config[\"learning_rate\"],\n",
        "        weight_decay=config[\"weight_decay\"]\n",
        "    )\n",
        "\n",
        "    num_training_steps = config[\"epochs\"] * len(train_dataloader)\n",
        "    scheduler = get_scheduler(\n",
        "        \"cosine\",\n",
        "        optimizer=optimizer,\n",
        "        num_warmup_steps=config[\"warmup_steps\"],\n",
        "        num_training_steps=num_training_steps\n",
        "    )\n",
        "\n",
        "    # Setup mixed precision training\n",
        "    scaler = torch.cuda.amp.GradScaler() if config[\"mixed_precision\"] else None\n",
        "\n",
        "    # Setup early stopping\n",
        "    early_stopping = EarlyStopping(patience=config[\"early_stopping_patience\"])\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(config[\"epochs\"]):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        for i, batch in enumerate(train_dataloader):\n",
        "            input_features = batch[\"input_features\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "\n",
        "            # Mixed precision training\n",
        "            if config[\"mixed_precision\"]:\n",
        "                with torch.cuda.amp.autocast():\n",
        "                    outputs = model(input_features, labels=labels)\n",
        "                    loss = outputs.loss / config[\"gradient_accumulation_steps\"]\n",
        "\n",
        "                scaler.scale(loss).backward()\n",
        "\n",
        "                if (i + 1) % config[\"gradient_accumulation_steps\"] == 0:\n",
        "                    scaler.unscale_(optimizer)\n",
        "                    clip_grad_norm_(model.parameters(), config[\"max_grad_norm\"])\n",
        "                    scaler.step(optimizer)\n",
        "                    scaler.update()\n",
        "                    scheduler.step()\n",
        "                    optimizer.zero_grad()\n",
        "            else:\n",
        "                outputs = model(input_features, labels=labels)\n",
        "                loss = outputs.loss / config[\"gradient_accumulation_steps\"]\n",
        "                loss.backward()\n",
        "\n",
        "                if (i + 1) % config[\"gradient_accumulation_steps\"] == 0:\n",
        "                    clip_grad_norm_(model.parameters(), config[\"max_grad_norm\"])\n",
        "                    optimizer.step()\n",
        "                    scheduler.step()\n",
        "                    optimizer.zero_grad()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Log batch progress\n",
        "            if i % 10 == 0:\n",
        "                wandb.log({\n",
        "                    \"batch_loss\": loss.item(),\n",
        "                    \"learning_rate\": scheduler.get_last_lr()[0]\n",
        "                })\n",
        "\n",
        "        # Validation phase with complete metrics\n",
        "        val_metrics = evaluate_model(model, processor, val_dataloader, device)\n",
        "\n",
        "        # Log all metrics to WandB\n",
        "        wandb.log({\n",
        "            \"train_loss\": total_loss / len(train_dataloader),\n",
        "            \"val_loss\": val_metrics[\"loss\"],\n",
        "            \"val_wer\": val_metrics[\"WER\"],\n",
        "            \"val_cer\": val_metrics[\"CER\"],\n",
        "            \"val_precision\": val_metrics[\"Precision\"],\n",
        "            \"val_recall\": val_metrics[\"Recall\"],\n",
        "            \"val_f1\": val_metrics[\"F1-Score\"],\n",
        "            \"val_accuracy\": val_metrics[\"Accuracy\"],\n",
        "            \"epoch\": epoch + 1\n",
        "        })\n",
        "\n",
        "        # Early stopping check\n",
        "        early_stopping(val_metrics[\"loss\"])\n",
        "        if early_stopping.should_stop:\n",
        "            print(f\"Early stopping triggered at epoch {epoch + 1}\")\n",
        "            break\n",
        "\n",
        "        # Save checkpoint\n",
        "        if (epoch + 1) % config[\"checkpoint_interval\"] == 0:\n",
        "            checkpoint_dir = f\"/content/drive/Shareddrives/CS307-Thesis/Dataset/whisper_checkpoints/checkpoint_epoch_{epoch + 1}\"\n",
        "            os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "            # Save model and processor\n",
        "            model.save_pretrained(checkpoint_dir)\n",
        "            processor.save_pretrained(checkpoint_dir)\n",
        "\n",
        "            # Save training state\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'scheduler_state_dict': scheduler.state_dict(),\n",
        "                'metrics': val_metrics\n",
        "            }, os.path.join(checkpoint_dir, 'training_state.pt'))\n",
        "\n",
        "    wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 929
        },
        "id": "jPTdgrf2-Sua",
        "outputId": "86dd3796-089f-457b-84ff-cb42077ce3b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing last run (ID:v55wxdu6) before initializing another..."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">lilac-wind-1</strong> at: <a href='https://wandb.ai/idcsalvame-n-a/taglish-whisper-fine-tuning/runs/v55wxdu6' target=\"_blank\">https://wandb.ai/idcsalvame-n-a/taglish-whisper-fine-tuning/runs/v55wxdu6</a><br/> View project at: <a href='https://wandb.ai/idcsalvame-n-a/taglish-whisper-fine-tuning' target=\"_blank\">https://wandb.ai/idcsalvame-n-a/taglish-whisper-fine-tuning</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20241103_074652-v55wxdu6/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Successfully finished last run (ID:v55wxdu6). Initializing new run:<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.18.5"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20241103_075007-ukxctduy</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/idcsalvame-n-a/taglish-whisper-fine-tuning/runs/ukxctduy' target=\"_blank\">denim-dew-2</a></strong> to <a href='https://wandb.ai/idcsalvame-n-a/taglish-whisper-fine-tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/idcsalvame-n-a/taglish-whisper-fine-tuning' target=\"_blank\">https://wandb.ai/idcsalvame-n-a/taglish-whisper-fine-tuning</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/idcsalvame-n-a/taglish-whisper-fine-tuning/runs/ukxctduy' target=\"_blank\">https://wandb.ai/idcsalvame-n-a/taglish-whisper-fine-tuning/runs/ukxctduy</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Audio file not found: /content/drive/Shareddrives/CS307-Thesis/Dataset/single-speaker/single-006.wav.mp3\n",
            "Warning: Audio file not found: /content/drive/Shareddrives/CS307-Thesis/Dataset/single-speaker/single-005.wav.mp3\n",
            "Warning: Audio file not found: /content/drive/Shareddrives/CS307-Thesis/Dataset/single-speaker/single-001.wav.mp3\n",
            "Warning: Audio file not found: /content/drive/Shareddrives/CS307-Thesis/Dataset/single-speaker/single-005.wav.mp3\n",
            "Warning: Audio file not found: /content/drive/Shareddrives/CS307-Thesis/Dataset/single-speaker/single-007.wav.mp3\n",
            "Warning: Audio file not found: /content/drive/Shareddrives/CS307-Thesis/Dataset/single-speaker/single-006.wav.mp3\n",
            "Warning: Audio file not found: /content/drive/Shareddrives/CS307-Thesis/Dataset/single-speaker/single-002.wav.mp3\n",
            "Warning: Audio file not found: /content/drive/Shareddrives/CS307-Thesis/Dataset/single-speaker/single-006.wav.mp3\n",
            "Warning: Audio file not found: /content/drive/Shareddrives/CS307-Thesis/Dataset/single-speaker/single-004.wav.mp3\n",
            "Warning: Audio file not found: /content/drive/Shareddrives/CS307-Thesis/Dataset/single-speaker/single-003.wav.mp3\n",
            "Warning: Audio file not found: /content/drive/Shareddrives/CS307-Thesis/Dataset/single-speaker/single-005.wav.mp3\n",
            "Warning: Audio file not found: /content/drive/Shareddrives/CS307-Thesis/Dataset/single-speaker/single-009.wav.mp3\n",
            "Warning: Audio file not found: /content/drive/Shareddrives/CS307-Thesis/Dataset/single-speaker/single-004.wav.mp3\n",
            "Warning: Audio file not found: /content/drive/Shareddrives/CS307-Thesis/Dataset/single-speaker/single-004.wav.mp3\n",
            "Warning: Audio file not found: /content/drive/Shareddrives/CS307-Thesis/Dataset/single-speaker/single-007.wav.mp3\n",
            "Warning: Audio file not found: /content/drive/Shareddrives/CS307-Thesis/Dataset/single-speaker/single-007.wav.mp3\n",
            "Warning: Audio file not found: /content/drive/Shareddrives/CS307-Thesis/Dataset/single-speaker/single-009.wav.mp3\n",
            "Warning: Audio file not found: /content/drive/Shareddrives/CS307-Thesis/Dataset/single-speaker/single-009.wav.mp3\n",
            "Warning: Audio file not found: /content/drive/Shareddrives/CS307-Thesis/Dataset/single-speaker/single-005.wav.mp3\n",
            "Warning: Audio file not found: /content/drive/Shareddrives/CS307-Thesis/Dataset/single-speaker/single-005.wav.mp3\n",
            "Warning: Audio file not found: /content/drive/Shareddrives/CS307-Thesis/Dataset/single-speaker/single-006.wav.mp3\n",
            "Warning: Audio file not found: /content/drive/Shareddrives/CS307-Thesis/Dataset/single-speaker/single-008.wav.mp3\n",
            "Warning: Audio file not found: /content/drive/Shareddrives/CS307-Thesis/Dataset/single-speaker/single-004.wav.mp3\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "num_samples should be a positive integer value, but got num_samples=0",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-40565c4b3557>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-24-d2e51beaeb10>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m# Create dataloaders\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     train_dataloader, val_dataloader = create_dataloaders(\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0maudio_files\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mtranscripts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-22-3ab6cc46982a>\u001b[0m in \u001b[0;36mcreate_dataloaders\u001b[0;34m(audio_files, transcripts, timestamps, processor, config)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# Create training dataloader with shuffling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     train_dataloader = DataLoader(\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"batch_size\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers, pin_memory_device)\u001b[0m\n\u001b[1;32m    374\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# map-style\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m                     \u001b[0msampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m                     \u001b[0msampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequentialSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/sampler.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data_source, replacement, num_samples, generator)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_samples\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    165\u001b[0m                 \u001b[0;34mf\"num_samples should be a positive integer value, but got num_samples={self.num_samples}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m             )\n",
            "\u001b[0;31mValueError\u001b[0m: num_samples should be a positive integer value, but got num_samples=0"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    drive.mount('/content/drive')\n",
        "    train_model(config)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.6"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}