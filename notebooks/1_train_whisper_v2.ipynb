{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X8S0cMJ9-SuW"
      },
      "outputs": [],
      "source": [
        "!pip install wandb transformers torch torchaudio jiwer scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "rxIwC0z4-SuX"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import wandb\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torchaudio\n",
        "import logging\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from transformers import WhisperProcessor, WhisperForConditionalGeneration, get_scheduler\n",
        "from jiwer import wer, cer\n",
        "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
        "from torch.optim import AdamW\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "import json\n",
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add GPU verification at startup\n",
        "def verify_gpu_status():\n",
        "    \"\"\"Verify GPU availability and PyTorch CUDA configuration\"\"\"\n",
        "    print(\"\\n=== GPU Status Check ===\")\n",
        "    print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"Current Device: {torch.cuda.current_device()}\")\n",
        "        print(f\"Device Name: {torch.cuda.get_device_name()}\")\n",
        "        print(f\"Device Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "        print(f\"PyTorch CUDA Version: {torch.version.cuda}\")\n",
        "    print(\"=====================\\n\")"
      ],
      "metadata": {
        "id": "MuQ_eFZjG5d2"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "WoqRKUR_-SuX"
      },
      "outputs": [],
      "source": [
        "# Enhanced configurations\n",
        "config = {\n",
        "    \"tsv_file\": \"/content/drive/Shareddrives/CS307-Thesis/Dataset/single-speaker/validated.tsv\",\n",
        "    \"audio_dir\": \"/content/drive/Shareddrives/CS307-Thesis/Dataset/single-speaker/\",\n",
        "    \"batch_size\": 4,\n",
        "    \"learning_rate\": 5e-5,\n",
        "    \"weight_decay\": 0.01,\n",
        "    \"warmup_steps\": 500,\n",
        "    \"epochs\": 3,\n",
        "    \"max_samples\": 13,\n",
        "    \"checkpoint_interval\": 3,\n",
        "    \"validation_split\": 0.1,\n",
        "    \"max_grad_norm\": 1.0,\n",
        "    \"early_stopping_patience\": 3,\n",
        "    \"mixed_precision\": True,\n",
        "    \"gradient_accumulation_steps\": 4\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def validate_audio_file(audio_path):\n",
        "    if not os.path.exists(audio_path):\n",
        "        return False, \"File does not exist\"\n",
        "    if not audio_path.endswith(('.wav', '.mp3', '.flac')):\n",
        "        return False, \"Unsupported audio format\"\n",
        "    return True, \"Valid\""
      ],
      "metadata": {
        "id": "zJObwJQdMw2r"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "HeFoIWaN-SuY"
      },
      "outputs": [],
      "source": [
        "def load_data(tsv_file, audio_dir, max_samples=None):\n",
        "    \"\"\"\n",
        "    Load data from TSV file with timestamp handling, compatible with both \"sec\" and \"min:sec\" formats.\n",
        "    \"\"\"\n",
        "    audio_files, transcripts, languages, timestamps = [], [], [], []\n",
        "\n",
        "    # Read TSV file\n",
        "    df = pd.read_csv(tsv_file, sep='\\t')\n",
        "    required_columns = ['path', 'start_time', 'end_time', 'language', 'sentence']\n",
        "\n",
        "    # Verify all required columns are present\n",
        "    if not all(col in df.columns for col in required_columns):\n",
        "        raise ValueError(f\"TSV file must contain columns: {required_columns}\")\n",
        "\n",
        "    # Shuffle and limit samples if specified\n",
        "    df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "    if max_samples:\n",
        "        df = df.head(max_samples)\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        audio_file = row['path']\n",
        "        if not audio_file.endswith((\".mp3\", \".wav\", \".flac\")):\n",
        "            print(f\"Skipping unsupported file type: {audio_file}\")\n",
        "            continue\n",
        "\n",
        "        full_audio_path = os.path.join(audio_dir, audio_file)\n",
        "        if not os.path.exists(full_audio_path):\n",
        "            print(f\"Warning: Audio file not found: {full_audio_path}\")\n",
        "            continue\n",
        "\n",
        "        # Parse timestamps\n",
        "        def parse_time(time_str):\n",
        "            try:\n",
        "                # Check if time is already in seconds\n",
        "                return float(time_str)\n",
        "            except ValueError:\n",
        "                # Convert from \"min:sec\" format to seconds\n",
        "                minutes, seconds = map(float, time_str.split(\":\"))\n",
        "                return minutes * 60 + seconds\n",
        "\n",
        "        try:\n",
        "            start_time = parse_time(row['start_time'])\n",
        "            end_time = parse_time(row['end_time'])\n",
        "        except Exception as e:\n",
        "            print(f\"Error parsing timestamps for {audio_file}: {str(e)}\")\n",
        "            continue\n",
        "\n",
        "        audio_files.append(full_audio_path)\n",
        "        transcripts.append(row['sentence'])\n",
        "        timestamps.append((start_time, end_time))\n",
        "        languages.append(row['language'])\n",
        "\n",
        "    return audio_files, transcripts, languages, timestamps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "OvHBaGuX-SuY"
      },
      "outputs": [],
      "source": [
        "class EarlyStopping:\n",
        "    def __init__(self, patience=3, min_delta=0):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.counter = 0\n",
        "        self.best_loss = None\n",
        "        self.should_stop = False\n",
        "\n",
        "    def __call__(self, val_loss):\n",
        "        if self.best_loss is None:\n",
        "            self.best_loss = val_loss\n",
        "        elif val_loss > self.best_loss - self.min_delta:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.should_stop = True\n",
        "        else:\n",
        "            self.best_loss = val_loss\n",
        "            self.counter = 0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch):\n",
        "    \"\"\"\n",
        "    Custom collate function to handle variable length sequences\n",
        "    \"\"\"\n",
        "    # Filter out any None or empty items\n",
        "    batch = [item for item in batch if item is not None]\n",
        "\n",
        "    if len(batch) == 0:\n",
        "        return {}\n",
        "\n",
        "    # Get maximum lengths\n",
        "    max_input_length = max(item['input_features'].size(1) for item in batch)\n",
        "    max_label_length = max(item['labels'].size(0) for item in batch)\n",
        "\n",
        "    # Initialize padded tensors\n",
        "    batch_size = len(batch)\n",
        "    padded_input_features = torch.zeros(batch_size, 80, max_input_length)\n",
        "    padded_labels = torch.full((batch_size, max_label_length), -100, dtype=torch.long)  # -100 is often used for padding in transformers\n",
        "\n",
        "    # Fill padded tensors\n",
        "    for i, item in enumerate(batch):\n",
        "        # Pad input features\n",
        "        input_features = item['input_features']\n",
        "        length = input_features.size(1)\n",
        "        padded_input_features[i, :, :length] = input_features\n",
        "\n",
        "        # Pad labels\n",
        "        labels = item['labels']\n",
        "        length = labels.size(0)\n",
        "        padded_labels[i, :length] = labels\n",
        "\n",
        "    return {\n",
        "        'input_features': padded_input_features,\n",
        "        'labels': padded_labels\n",
        "    }"
      ],
      "metadata": {
        "id": "Dpw6Yh0KESQG"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "9Rd4eaXD-SuZ"
      },
      "outputs": [],
      "source": [
        "class ProcessData(Dataset):\n",
        "    def __init__(self, audio_files, transcripts, timestamps, processor, languages=None, training=True):\n",
        "        self.audio_files = audio_files\n",
        "        self.transcripts = transcripts\n",
        "        self.timestamps = timestamps\n",
        "        self.processor = processor\n",
        "        self.languages = languages or [\"tl-en\"] * len(audio_files)  # Default to tl-en if not provided\n",
        "        self.training = training\n",
        "        self.debug_stats = {\n",
        "            \"processed\": 0,\n",
        "            \"errors\": 0,\n",
        "            \"error_types\": {}\n",
        "        }\n",
        "\n",
        "        print(f\"Initializing dataset with {len(audio_files)} samples\")\n",
        "\n",
        "        if training:\n",
        "            # Initialize audio augmentation transforms\n",
        "            self.time_stretch = torchaudio.transforms.TimeStretch(fixed_rate=0.98)\n",
        "            self.freq_mask = torchaudio.transforms.FrequencyMasking(freq_mask_param=30)\n",
        "            self.time_mask = torchaudio.transforms.TimeMasking(time_mask_param=100)\n",
        "\n",
        "    def apply_audio_transforms(self, audio):\n",
        "        \"\"\"Apply audio augmentation transforms during training\"\"\"\n",
        "        if not self.training:\n",
        "            return audio\n",
        "\n",
        "        try:\n",
        "            # Convert to complex spectrogram for time stretching\n",
        "            spec = torch.stft(\n",
        "                audio,\n",
        "                n_fft=400,\n",
        "                hop_length=100,\n",
        "                win_length=400,\n",
        "                window=torch.hann_window(400),\n",
        "                return_complex=True\n",
        "            )\n",
        "\n",
        "            # Apply time stretch\n",
        "            spec_stretched = self.time_stretch(spec)\n",
        "\n",
        "            # Convert back to time domain\n",
        "            audio = torch.istft(\n",
        "                spec_stretched,\n",
        "                n_fft=400,\n",
        "                hop_length=100,\n",
        "                win_length=400,\n",
        "                window=torch.hann_window(400)\n",
        "            )\n",
        "\n",
        "            # Apply frequency and time masking\n",
        "            spec = torchaudio.transforms.MelSpectrogram()(audio)\n",
        "            spec = self.freq_mask(spec)\n",
        "            spec = self.time_mask(spec)\n",
        "\n",
        "            return audio\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Audio augmentation failed: {str(e)}\")\n",
        "            return audio\n",
        "\n",
        "    def log_processing_step(self, idx, step, info):\n",
        "        \"\"\"Structured logging for processing steps\"\"\"\n",
        "        print(f\"\\n[Sample {idx}][{step}] {info}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.audio_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        try:\n",
        "            audio_path = self.audio_files[idx]\n",
        "            transcript = self.transcripts[idx]\n",
        "            start_time, end_time = self.timestamps[idx]\n",
        "\n",
        "            self.log_processing_step(idx, \"Start\", \"Beginning processing\")\n",
        "            self.log_processing_step(idx, \"Info\", f\"Path: {audio_path}\")\n",
        "            self.log_processing_step(idx, \"Info\", f\"Transcript: {transcript}\")\n",
        "            self.log_processing_step(idx, \"Info\", f\"Timestamps: {start_time}-{end_time}\")\n",
        "\n",
        "            # Load audio\n",
        "            audio, sample_rate = torchaudio.load(audio_path)\n",
        "            self.log_processing_step(idx, \"Audio Load\", f\"Shape: {audio.shape}, Sample rate: {sample_rate}\")\n",
        "\n",
        "            # Convert to mono if stereo\n",
        "            if audio.shape[0] > 1:\n",
        "                audio = torch.mean(audio, dim=0, keepdim=True)\n",
        "                self.log_processing_step(idx, \"Mono Convert\", \"Converted stereo to mono\")\n",
        "\n",
        "            # Resample if necessary\n",
        "            if sample_rate != 16000:\n",
        "                resampler = torchaudio.transforms.Resample(sample_rate, 16000)\n",
        "                audio = resampler(audio)\n",
        "                self.log_processing_step(idx, \"Resample\", \"Resampled to 16kHz\")\n",
        "\n",
        "            # Apply timestamps\n",
        "            start_frame = int(start_time * 16000)\n",
        "            end_frame = int(end_time * 16000)\n",
        "            audio = audio[:, start_frame:end_frame]\n",
        "            self.log_processing_step(idx, \"Trim\", f\"Trimmed shape: {audio.shape}\")\n",
        "\n",
        "            # Apply audio transforms if in training mode\n",
        "            if self.training:\n",
        "                audio = self.apply_audio_transforms(audio.squeeze())\n",
        "                audio = audio.unsqueeze(0)\n",
        "                self.log_processing_step(idx, \"Augment\", \"Applied audio transforms\")\n",
        "\n",
        "            # Process audio features\n",
        "            input_features = self.processor(\n",
        "                audio.squeeze().numpy(),\n",
        "                sampling_rate=16000,\n",
        "                return_tensors=\"pt\"\n",
        "            ).input_features\n",
        "            self.log_processing_step(idx, \"Features\", f\"Processed features shape: {input_features.shape}\")\n",
        "\n",
        "            # Process labels\n",
        "            labels = self.processor(\n",
        "                text=transcript,\n",
        "                return_tensors=\"pt\"\n",
        "            ).input_ids.squeeze()\n",
        "            self.log_processing_step(idx, \"Labels\", f\"Processed labels shape: {labels.shape}\")\n",
        "\n",
        "            self.debug_stats[\"processed\"] += 1\n",
        "            return {\n",
        "                \"input_features\": input_features.squeeze(),\n",
        "                \"labels\": labels.long(),\n",
        "                \"transcript\": transcript,\n",
        "                \"language\": self.languages[idx]  # Add language information to the batch\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            self.log_error(type(e).__name__, str(e))\n",
        "            self.log_processing_step(idx, \"Error\", f\"Failed: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "    def log_error(self, error_type, details):\n",
        "        if error_type not in self.debug_stats[\"error_types\"]:\n",
        "            self.debug_stats[\"error_types\"][error_type] = []\n",
        "        self.debug_stats[\"error_types\"][error_type].append(details)\n",
        "        self.debug_stats[\"errors\"] += 1\n",
        "\n",
        "    def get_stats(self):\n",
        "        return {\n",
        "            \"total_items\": len(self),\n",
        "            \"successfully_processed\": self.debug_stats[\"processed\"],\n",
        "            \"errors\": self.debug_stats[\"errors\"],\n",
        "            \"error_breakdown\": self.debug_stats[\"error_types\"]\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "g4tHWUHS-SuZ"
      },
      "outputs": [],
      "source": [
        "def create_dataloaders(audio_files, transcripts, timestamps, processor, config, languages=None):\n",
        "    \"\"\"Create training and validation dataloaders with error checking\"\"\"\n",
        "    # Create full dataset\n",
        "    full_dataset = ProcessData(audio_files, transcripts, timestamps, processor, languages=languages, training=True)\n",
        "\n",
        "    # Calculate split sizes with minimum validation size check\n",
        "    total_samples = len(full_dataset)\n",
        "    val_size = max(int(total_samples * config[\"validation_split\"]), 1)\n",
        "    train_size = total_samples - val_size\n",
        "\n",
        "    print(f\"Splitting dataset: {train_size} training samples, {val_size} validation samples\")\n",
        "\n",
        "    # Split dataset\n",
        "    train_dataset, val_dataset = random_split(\n",
        "        full_dataset,\n",
        "        [train_size, val_size],\n",
        "        generator=torch.Generator().manual_seed(42)\n",
        "    )\n",
        "\n",
        "    # Create dataloaders\n",
        "    train_dataloader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=config[\"batch_size\"],\n",
        "        shuffle=True,\n",
        "        num_workers=2,\n",
        "        pin_memory=True,\n",
        "        collate_fn=collate_fn\n",
        "    )\n",
        "\n",
        "    val_dataloader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=config[\"batch_size\"],\n",
        "        shuffle=False,\n",
        "        num_workers=2,\n",
        "        pin_memory=True,\n",
        "        collate_fn=collate_fn\n",
        "    )\n",
        "\n",
        "    return train_dataloader, val_dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "YhrMhW5p-SuZ"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, processor, dataloader, device, log_samples=5):\n",
        "    \"\"\"Enhanced evaluation function with multilingual support and detailed prediction logging\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    batch_sizes = []\n",
        "    samples_logged = 0\n",
        "\n",
        "    # Create dictionaries to store metrics by language\n",
        "    metrics_by_language = {\n",
        "        \"en\": {\"preds\": [], \"labels\": []},\n",
        "        \"tl\": {\"preds\": [], \"labels\": []},\n",
        "        \"tl-en\": {\"preds\": [], \"labels\": []}\n",
        "    }\n",
        "\n",
        "    print(\"\\n=== Starting Evaluation ===\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, batch in enumerate(dataloader):\n",
        "            if not batch:\n",
        "                print(f\"Batch {batch_idx} is empty, skipping...\")\n",
        "                continue\n",
        "\n",
        "            batch_sizes.append(batch[\"input_features\"].size(0))\n",
        "            input_features = batch[\"input_features\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "\n",
        "            # Get language information from batch\n",
        "            languages = batch.get(\"language\", [\"tl-en\"] * input_features.size(0))\n",
        "\n",
        "            try:\n",
        "                # Create forced decoder IDs for both English and Tagalog\n",
        "                forced_decoder_ids = processor.get_decoder_prompt_ids(\n",
        "                    language=[\"en\", \"tl\"],\n",
        "                    task=\"transcribe\",\n",
        "                    no_timestamps=True\n",
        "                )\n",
        "\n",
        "                outputs = model(\n",
        "                    input_features,\n",
        "                    labels=labels,\n",
        "                    forced_decoder_ids=forced_decoder_ids\n",
        "                )\n",
        "\n",
        "                total_loss += outputs.loss.item()\n",
        "\n",
        "                # Generate predictions\n",
        "                generated_ids = model.generate(\n",
        "                    input_features,\n",
        "                    forced_decoder_ids=forced_decoder_ids,\n",
        "                    max_length=256\n",
        "                )\n",
        "\n",
        "                # Decode predictions and labels\n",
        "                decoded_preds = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
        "                labels_clean = torch.where(labels != -100, labels, processor.tokenizer.pad_token_id)\n",
        "                decoded_labels = processor.batch_decode(labels_clean, skip_special_tokens=True)\n",
        "\n",
        "                # Log predictions and store metrics by language\n",
        "                for pred, label, lang in zip(decoded_preds, decoded_labels, languages):\n",
        "                    pred = pred.strip()\n",
        "                    label = label.strip()\n",
        "                    if pred and label:\n",
        "                        # Store in overall metrics\n",
        "                        all_preds.append(pred)\n",
        "                        all_labels.append(label)\n",
        "\n",
        "                        # Store in language-specific metrics\n",
        "                        if lang in metrics_by_language:\n",
        "                            metrics_by_language[lang][\"preds\"].append(pred)\n",
        "                            metrics_by_language[lang][\"labels\"].append(label)\n",
        "\n",
        "                        # Log detailed samples\n",
        "                        if samples_logged < log_samples:\n",
        "                            print(\"\\n--- Sample #{} (Language: {}) ---\".format(samples_logged + 1, lang))\n",
        "                            print(f\"Predicted : {pred}\")\n",
        "                            print(f\"Actual    : {label}\")\n",
        "                            print(f\"WER       : {wer([label], [pred]):.4f}\")\n",
        "                            print(f\"CER       : {cer([label], [pred]):.4f}\")\n",
        "                            print(\"-\" * 50)\n",
        "                            samples_logged += 1\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error in batch {batch_idx}: {str(e)}\")\n",
        "                continue\n",
        "\n",
        "    # Calculate overall metrics\n",
        "    overall_metrics = calculate_metrics(all_preds, all_labels, total_loss, len(dataloader))\n",
        "\n",
        "    # Calculate language-specific metrics\n",
        "    language_metrics = {}\n",
        "    for lang, data in metrics_by_language.items():\n",
        "        if data[\"preds\"]:  # Only calculate if we have predictions for this language\n",
        "            lang_loss = total_loss * (len(data[\"preds\"]) / len(all_preds))  # Approximate loss distribution\n",
        "            lang_metrics = calculate_metrics(data[\"preds\"], data[\"labels\"], lang_loss, len(dataloader))\n",
        "            language_metrics[lang] = lang_metrics\n",
        "\n",
        "    # Print detailed metrics\n",
        "    print(\"\\n=== Overall Evaluation Metrics ===\")\n",
        "    for metric, value in overall_metrics.items():\n",
        "        print(f\"{metric}: {value:.4f}\")\n",
        "\n",
        "    print(\"\\n=== Language-Specific Metrics ===\")\n",
        "    for lang, metrics in language_metrics.items():\n",
        "        print(f\"\\n{lang.upper()} Metrics:\")\n",
        "        for metric, value in metrics.items():\n",
        "            print(f\"{metric}: {value:.4f}\")\n",
        "\n",
        "    # Combine metrics for return\n",
        "    return {\n",
        "        \"overall\": overall_metrics,\n",
        "        \"by_language\": language_metrics\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_metrics(all_preds, all_labels, total_loss, num_batches):\n",
        "    \"\"\"Separate function for metric calculation with error handling\"\"\"\n",
        "    metrics = {\n",
        "        \"loss\": total_loss / num_batches if num_batches > 0 else float('inf')\n",
        "    }\n",
        "\n",
        "    if len(all_preds) > 0 and len(all_labels) > 0:\n",
        "        try:\n",
        "            metrics[\"WER\"] = wer(all_labels, all_preds)\n",
        "            metrics[\"CER\"] = cer(all_labels, all_preds)\n",
        "\n",
        "            tokenized_labels = [text.split() for text in all_labels]\n",
        "            tokenized_preds = [text.split() for text in all_preds]\n",
        "\n",
        "            min_len = min(len(tokenized_labels), len(tokenized_preds))\n",
        "            tokenized_labels = tokenized_labels[:min_len]\n",
        "            tokenized_preds = tokenized_preds[:min_len]\n",
        "\n",
        "            flat_labels = [word for sentence in tokenized_labels for word in sentence]\n",
        "            flat_preds = [word for sentence in tokenized_preds for word in sentence]\n",
        "\n",
        "            if len(flat_labels) == len(flat_preds) and len(flat_labels) > 0:\n",
        "                precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "                    flat_labels,\n",
        "                    flat_preds,\n",
        "                    average='weighted',\n",
        "                    zero_division=0\n",
        "                )\n",
        "                accuracy = accuracy_score(flat_labels, flat_preds)\n",
        "\n",
        "                metrics.update({\n",
        "                    \"Precision\": precision,\n",
        "                    \"Recall\": recall,\n",
        "                    \"F1-Score\": f1,\n",
        "                    \"Accuracy\": accuracy\n",
        "                })\n",
        "        except Exception as e:\n",
        "            print(f\"Error calculating metrics: {str(e)}\")\n",
        "            metrics.update({\n",
        "                \"WER\": 1.0,\n",
        "                \"CER\": 1.0,\n",
        "                \"Precision\": 0.0,\n",
        "                \"Recall\": 0.0,\n",
        "                \"F1-Score\": 0.0,\n",
        "                \"Accuracy\": 0.0\n",
        "            })\n",
        "\n",
        "    return metrics"
      ],
      "metadata": {
        "id": "mL84UAtBHLT9"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "78_1ayt4-Sua"
      },
      "outputs": [],
      "source": [
        "def train_model(config, checkpoint_path=None):\n",
        "    # Initialize wandb\n",
        "    wandb.init(project=\"taglish-whisper-fine-tuning\", config=config)\n",
        "\n",
        "    # Set device and handle GPU unavailability\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    if not torch.cuda.is_available():\n",
        "        print(\"WARNING: CUDA is not available. Training will proceed on CPU, which will be much slower.\")\n",
        "        config[\"mixed_precision\"] = False\n",
        "\n",
        "    # Load model and processor with safe checkpoint loading\n",
        "    start_epoch = 0\n",
        "    if checkpoint_path and os.path.exists(checkpoint_path):\n",
        "        print(f\"Loading checkpoint from {checkpoint_path}\")\n",
        "        try:\n",
        "            processor = WhisperProcessor.from_pretrained(checkpoint_path)\n",
        "            model = WhisperForConditionalGeneration.from_pretrained(checkpoint_path)\n",
        "\n",
        "            # Add Tagalog language token if not present\n",
        "            special_tokens = {\"additional_special_tokens\": [\"<|tl|>\"]}\n",
        "            num_added_tokens = processor.tokenizer.add_special_tokens(special_tokens)\n",
        "            if num_added_tokens > 0:\n",
        "                model.resize_token_embeddings(len(processor.tokenizer))\n",
        "\n",
        "            # Safely load training state\n",
        "            training_state_path = os.path.join(checkpoint_path, 'training_state.pt')\n",
        "            if os.path.exists(training_state_path):\n",
        "                try:\n",
        "                    training_state = torch.load(training_state_path, map_location=device)\n",
        "                    start_epoch = training_state.get('epoch', 0) + 1\n",
        "                    print(f\"Resuming from epoch {start_epoch}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Warning: Could not load training state: {e}\")\n",
        "                    start_epoch = 0\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading checkpoint: {e}\")\n",
        "            print(\"Starting fresh training with multilingual base model\")\n",
        "            processor = WhisperProcessor.from_pretrained(\"openai/whisper-base\")\n",
        "            model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-base\")\n",
        "\n",
        "            # Add Tagalog language token\n",
        "            special_tokens = {\"additional_special_tokens\": [\"<|tl|>\"]}\n",
        "            num_added_tokens = processor.tokenizer.add_special_tokens(special_tokens)\n",
        "            if num_added_tokens > 0:\n",
        "                model.resize_token_embeddings(len(processor.tokenizer))\n",
        "    else:\n",
        "        print(\"Starting fresh training with multilingual base model\")\n",
        "        processor = WhisperProcessor.from_pretrained(\"openai/whisper-base\")\n",
        "        model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-base\")\n",
        "\n",
        "        # Add Tagalog language token\n",
        "        special_tokens = {\"additional_special_tokens\": [\"<|tl|>\"]}\n",
        "        num_added_tokens = processor.tokenizer.add_special_tokens(special_tokens)\n",
        "        if num_added_tokens > 0:\n",
        "            model.resize_token_embeddings(len(processor.tokenizer))\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    # Load data\n",
        "    print(\"\\nLoading data...\")\n",
        "    audio_files, transcripts, languages, timestamps = load_data(\n",
        "        config[\"tsv_file\"],\n",
        "        config[\"audio_dir\"],\n",
        "        config[\"max_samples\"]\n",
        "    )\n",
        "\n",
        "    print(f\"Total samples loaded: {len(audio_files)}\")\n",
        "\n",
        "    if len(audio_files) == 0:\n",
        "        raise ValueError(\"No audio files loaded. Please check your data paths and file formats.\")\n",
        "\n",
        "    # Create dataloaders with validation size check\n",
        "    total_samples = len(audio_files)\n",
        "    min_val_samples = 1  # Minimum number of validation samples\n",
        "\n",
        "    # Adjust validation split if necessary\n",
        "    if total_samples * config[\"validation_split\"] < min_val_samples:\n",
        "        adjusted_split = min_val_samples / total_samples\n",
        "        print(f\"Warning: Adjusting validation split from {config['validation_split']} to {adjusted_split} to ensure at least {min_val_samples} validation sample(s)\")\n",
        "        config[\"validation_split\"] = adjusted_split\n",
        "\n",
        "    # Create dataloaders with language information\n",
        "    train_dataloader, val_dataloader = create_dataloaders(\n",
        "        audio_files,\n",
        "        transcripts,\n",
        "        timestamps,\n",
        "        processor,\n",
        "        config,\n",
        "        languages=languages  # Pass languages to create_dataloaders\n",
        "    )\n",
        "\n",
        "    print(f\"Training batches: {len(train_dataloader)}\")\n",
        "    print(f\"Validation batches: {len(val_dataloader)}\")\n",
        "\n",
        "\n",
        "    # Setup optimizer and scheduler\n",
        "    optimizer = AdamW(\n",
        "        model.parameters(),\n",
        "        lr=config[\"learning_rate\"],\n",
        "        weight_decay=config[\"weight_decay\"]\n",
        "    )\n",
        "\n",
        "    num_training_steps = config[\"epochs\"] * len(train_dataloader)\n",
        "    scheduler = get_scheduler(\n",
        "        \"cosine\",\n",
        "        optimizer=optimizer,\n",
        "        num_warmup_steps=config[\"warmup_steps\"],\n",
        "        num_training_steps=num_training_steps\n",
        "    )\n",
        "\n",
        "    # Load optimizer and scheduler states if resuming\n",
        "    if checkpoint_path and os.path.exists(checkpoint_path) and 'training_state' in locals():\n",
        "        try:\n",
        "            optimizer.load_state_dict(training_state['optimizer_state_dict'])\n",
        "            scheduler.load_state_dict(training_state['scheduler_state_dict'])\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Could not load optimizer/scheduler states: {e}\")\n",
        "\n",
        "    # Setup mixed precision training only if CUDA is available\n",
        "    scaler = torch.amp.GradScaler() if config[\"mixed_precision\"] and torch.cuda.is_available() else None\n",
        "\n",
        "    # Setup early stopping\n",
        "    early_stopping = EarlyStopping(patience=config[\"early_stopping_patience\"])\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(start_epoch, config[\"epochs\"]):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        for i, batch in enumerate(train_dataloader):\n",
        "            if not batch:\n",
        "                continue\n",
        "\n",
        "            input_features = batch[\"input_features\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "\n",
        "            # Get language information from batch if available\n",
        "            language = batch.get(\"language\", [\"tl-en\"] * input_features.size(0))  # Default to tl-en if not provided\n",
        "\n",
        "            # Create forced decoder IDs for both English and Tagalog\n",
        "            forced_decoder_ids = processor.get_decoder_prompt_ids(\n",
        "                language=[\"en\", \"tl\"],  # Specify both languages\n",
        "                task=\"transcribe\",\n",
        "                no_timestamps=True\n",
        "            )\n",
        "\n",
        "            # Mixed precision training\n",
        "            if scaler is not None:\n",
        "                with torch.amp.autocast(device_type='cuda', dtype=torch.float16):\n",
        "                    outputs = model(\n",
        "                        input_features,\n",
        "                        labels=labels,\n",
        "                        forced_decoder_ids=forced_decoder_ids\n",
        "                    )\n",
        "                    loss = outputs.loss / config[\"gradient_accumulation_steps\"]\n",
        "\n",
        "                scaler.scale(loss).backward()\n",
        "\n",
        "                if (i + 1) % config[\"gradient_accumulation_steps\"] == 0:\n",
        "                    scaler.unscale_(optimizer)\n",
        "                    clip_grad_norm_(model.parameters(), config[\"max_grad_norm\"])\n",
        "                    scaler.step(optimizer)\n",
        "                    scaler.update()\n",
        "                    scheduler.step()\n",
        "                    optimizer.zero_grad()\n",
        "            else:\n",
        "                outputs = model(\n",
        "                    input_features,\n",
        "                    labels=labels,\n",
        "                    forced_decoder_ids=forced_decoder_ids\n",
        "                )\n",
        "                loss = outputs.loss / config[\"gradient_accumulation_steps\"]\n",
        "                loss.backward()\n",
        "\n",
        "                if (i + 1) % config[\"gradient_accumulation_steps\"] == 0:\n",
        "                    clip_grad_norm_(model.parameters(), config[\"max_grad_norm\"])\n",
        "                    optimizer.step()\n",
        "                    scheduler.step()\n",
        "                    optimizer.zero_grad()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Logging\n",
        "            if i % 10 == 0:\n",
        "                wandb.log({\n",
        "                    \"batch_loss\": loss.item(),\n",
        "                    \"learning_rate\": scheduler.get_last_lr()[0]\n",
        "                })\n",
        "\n",
        "        # Validation phase\n",
        "        val_metrics = evaluate_model(model, processor, val_dataloader, device)\n",
        "\n",
        "        # Log metrics\n",
        "        wandb.log({\n",
        "            \"train_loss\": total_loss / len(train_dataloader),\n",
        "            **val_metrics,\n",
        "            \"epoch\": epoch + 1\n",
        "        })\n",
        "\n",
        "        # Early stopping check\n",
        "        early_stopping(val_metrics[\"loss\"])\n",
        "        if early_stopping.should_stop:\n",
        "            print(f\"Early stopping triggered at epoch {epoch + 1}\")\n",
        "            break\n",
        "\n",
        "        # Save checkpoint\n",
        "        if (epoch + 1) % config[\"checkpoint_interval\"] == 0:\n",
        "            checkpoint_dir = f\"/content/drive/Shareddrives/CS307-Thesis/Dataset/whisper_checkpoints/checkpoint_epoch_{epoch + 1}\"\n",
        "            os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "            model.save_pretrained(checkpoint_dir)\n",
        "            processor.save_pretrained(checkpoint_dir)\n",
        "\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'scheduler_state_dict': scheduler.state_dict(),\n",
        "                'metrics': val_metrics\n",
        "            }, os.path.join(checkpoint_dir, 'training_state.pt'))\n",
        "\n",
        "    wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jPTdgrf2-Sua"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    # Verify CUDA installation and GPU availability before starting\n",
        "    verify_gpu_status()\n",
        "\n",
        "    # Start training\n",
        "    checkpoint_path = \"/content/drive/Shareddrives/CS307-Thesis/Dataset/whisper_checkpoints/checkpoint_epoch_2\"\n",
        "    train_model(config, checkpoint_path=checkpoint_path)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.6"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}