{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wandb transformers torch torchaudio jiwer sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import wandb\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration, get_scheduler\n",
    "from jiwer import wer, cer\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim import AdamW\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from google.colab import drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced configurations\n",
    "config = {\n",
    "    \"tsv_file\": \"/content/drive/MyDrive/path/to/train.tsv\",\n",
    "    \"audio_dir\": \"/content/drive/MyDrive/path/to/audio_files\",\n",
    "    \"batch_size\": 4,\n",
    "    \"learning_rate\": 5e-5,\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"warmup_steps\": 500,\n",
    "    \"epochs\": 3,\n",
    "    \"max_samples\": 100,\n",
    "    \"checkpoint_interval\": 2,\n",
    "    \"validation_split\": 0.1,\n",
    "    \"max_grad_norm\": 1.0,\n",
    "    \"early_stopping_patience\": 3,\n",
    "    \"mixed_precision\": True,\n",
    "    \"gradient_accumulation_steps\": 4\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(tsv_file, audio_dir, max_samples=None):\n",
    "    \"\"\"\n",
    "    Load data from TSV file with proper timestamp handling\n",
    "    \"\"\"\n",
    "    audio_files, transcripts, timestamps = [], [], []\n",
    "    \n",
    "    # Read TSV file with explicit column names\n",
    "    df = pd.read_csv(tsv_file, sep='\\t')\n",
    "    required_columns = ['path', 'start_time', 'end_time', 'sentence']\n",
    "    \n",
    "    # Verify all required columns are present\n",
    "    if not all(col in df.columns for col in required_columns):\n",
    "        raise ValueError(f\"TSV file must contain columns: {required_columns}\")\n",
    "    \n",
    "    # Shuffle the dataframe\n",
    "    df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    \n",
    "    # Limit samples if specified\n",
    "    if max_samples:\n",
    "        df = df.head(max_samples)\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        audio_file = row['path']\n",
    "        if not audio_file.endswith(\".mp3\"):\n",
    "            audio_file += \".mp3\"\n",
    "            \n",
    "        full_audio_path = os.path.join(audio_dir, audio_file)\n",
    "        if not os.path.exists(full_audio_path):\n",
    "            print(f\"Warning: Audio file not found: {full_audio_path}\")\n",
    "            continue\n",
    "            \n",
    "        audio_files.append(full_audio_path)\n",
    "        transcripts.append(row['sentence'])\n",
    "        timestamps.append((float(row['start']), float(row['end'])))\n",
    "    \n",
    "    return audio_files, transcripts, timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=3, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.should_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "        elif val_loss > self.best_loss - self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.should_stop = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProcessData(Dataset):\n",
    "    def __init__(self, audio_files, transcripts, timestamps, processor, training=True):\n",
    "        self.audio_files = audio_files\n",
    "        self.transcripts = transcripts\n",
    "        self.timestamps = timestamps\n",
    "        self.processor = processor\n",
    "        self.training = training\n",
    "        \n",
    "        self.audio_transforms = torch.nn.Sequential(\n",
    "            torchaudio.transforms.TimeStretch(fixed_rate=0.98),\n",
    "            torchaudio.transforms.FrequencyMasking(freq_mask_param=30),\n",
    "            torchaudio.transforms.TimeMasking(time_mask_param=100)\n",
    "        ) if training else None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        audio_path = self.audio_files[idx]\n",
    "        transcript = self.transcripts[idx]\n",
    "        start_time, end_time = self.timestamps[idx]\n",
    "\n",
    "        try:\n",
    "            audio, sample_rate = torchaudio.load(audio_path)\n",
    "            \n",
    "            # Resample if necessary\n",
    "            if sample_rate != 16000:\n",
    "                resampler = torchaudio.transforms.Resample(sample_rate, 16000)\n",
    "                audio = resampler(audio)\n",
    "\n",
    "            # Apply audio augmentation during training\n",
    "            if self.training and self.audio_transforms:\n",
    "                audio = self.audio_transforms(audio)\n",
    "\n",
    "            # Apply timestamps to trim audio\n",
    "            start_frame = int(start_time * 16000)\n",
    "            end_frame = int(end_time * 16000)\n",
    "            audio = audio[:, start_frame:end_frame]\n",
    "\n",
    "            input_features = self.processor(\n",
    "                audio.squeeze().numpy(), \n",
    "                sampling_rate=16000, \n",
    "                return_tensors=\"pt\"\n",
    "            ).input_features\n",
    "\n",
    "            labels = self.processor(\n",
    "                transcript, \n",
    "                return_tensors=\"pt\"\n",
    "            ).input_ids\n",
    "\n",
    "            return {\n",
    "                \"input_features\": input_features.squeeze(),\n",
    "                \"labels\": labels.squeeze()\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {audio_path}: {str(e)}\")\n",
    "            # Return a zero tensor with appropriate shape as fallback\n",
    "            return {\n",
    "                \"input_features\": torch.zeros(80, 3000),\n",
    "                \"labels\": torch.zeros(100)\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloaders(audio_files, transcripts, timestamps, processor, config):\n",
    "    # Create full dataset\n",
    "    full_dataset = ProcessData(audio_files, transcripts, timestamps, processor)\n",
    "    \n",
    "    # Calculate split sizes\n",
    "    val_size = int(len(full_dataset) * config[\"validation_split\"])\n",
    "    train_size = len(full_dataset) - val_size\n",
    "    \n",
    "    # Split dataset\n",
    "    train_dataset, val_dataset = random_split(\n",
    "        full_dataset, \n",
    "        [train_size, val_size],\n",
    "        generator=torch.Generator().manual_seed(42)\n",
    "    )\n",
    "    \n",
    "    # Create training dataloader with shuffling\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config[\"batch_size\"],\n",
    "        shuffle=True,\n",
    "        num_workers=2,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    # Create validation dataloader without shuffling\n",
    "    val_dataloader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=config[\"batch_size\"],\n",
    "        shuffle=False,\n",
    "        num_workers=2,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    return train_dataloader, val_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, processor, dataloader, device):\n",
    "    \"\"\"\n",
    "    Evaluate the model using multiple metrics:\n",
    "    - Loss\n",
    "    - Word Error Rate (WER)\n",
    "    - Character Error Rate (CER)\n",
    "    - Precision\n",
    "    - Recall\n",
    "    - F1-Score\n",
    "    - Accuracy\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_features = batch[\"input_features\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            \n",
    "            # Get model outputs and loss\n",
    "            outputs = model(input_features, labels=labels)\n",
    "            total_loss += outputs.loss.item()\n",
    "            \n",
    "            # Generate predictions\n",
    "            generated_ids = model.generate(input_features)\n",
    "            \n",
    "            # Decode predictions and labels\n",
    "            preds = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "            refs = processor.batch_decode(labels, skip_special_tokens=True)\n",
    "            \n",
    "            # Store predictions and references for metric calculation\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(refs)\n",
    "    \n",
    "    # Calculate average loss\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    \n",
    "    # Calculate WER and CER\n",
    "    wer_score = wer(all_labels, all_preds)\n",
    "    cer_score = cer(all_labels, all_preds)\n",
    "    \n",
    "    # Calculate precision, recall, and F1-score\n",
    "    # We'll treat each word as a token for these metrics\n",
    "    tokenized_labels = [text.split() for text in all_labels]\n",
    "    tokenized_preds = [text.split() for text in all_preds]\n",
    "    \n",
    "    # Flatten the lists for sklearn metrics\n",
    "    flat_labels = [word for sentence in tokenized_labels for word in sentence]\n",
    "    flat_preds = [word for sentence in tokenized_preds for word in sentence]\n",
    "    \n",
    "    # Calculate precision, recall, F1\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        flat_labels, \n",
    "        flat_preds, \n",
    "        average='weighted',\n",
    "        zero_division=0\n",
    "    )\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(flat_labels, flat_preds)\n",
    "    \n",
    "    # Compile all metrics\n",
    "    metrics = {\n",
    "        \"loss\": avg_loss,\n",
    "        \"WER\": wer_score,\n",
    "        \"CER\": cer_score,\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1-Score\": f1,\n",
    "        \"Accuracy\": accuracy\n",
    "    }\n",
    "    \n",
    "    # Print metrics for monitoring\n",
    "    print(\"\\nEvaluation Metrics:\")\n",
    "    for metric_name, value in metrics.items():\n",
    "        print(f\"{metric_name}: {value:.4f}\")\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(config):\n",
    "    # Initialize wandb\n",
    "    wandb.init(project=\"taglish-whisper-fine-tuning\", config=config)\n",
    "    \n",
    "    # Set device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Load model and processor\n",
    "    processor = WhisperProcessor.from_pretrained(\"openai/whisper-base\")\n",
    "    model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-base\")\n",
    "    model.to(device)\n",
    "    \n",
    "    # Load data\n",
    "    audio_files, transcripts, timestamps = load_data(\n",
    "        config[\"tsv_file\"],\n",
    "        config[\"audio_dir\"],\n",
    "        config[\"max_samples\"]\n",
    "    )\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_dataloader, val_dataloader = create_dataloaders(\n",
    "        audio_files,\n",
    "        transcripts,\n",
    "        timestamps,\n",
    "        processor,\n",
    "        config\n",
    "    )\n",
    "    \n",
    "    # Setup optimizer and scheduler\n",
    "    optimizer = AdamW(\n",
    "        model.parameters(),\n",
    "        lr=config[\"learning_rate\"],\n",
    "        weight_decay=config[\"weight_decay\"]\n",
    "    )\n",
    "    \n",
    "    num_training_steps = config[\"epochs\"] * len(train_dataloader)\n",
    "    scheduler = get_scheduler(\n",
    "        \"cosine\",\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=config[\"warmup_steps\"],\n",
    "        num_training_steps=num_training_steps\n",
    "    )\n",
    "    \n",
    "    # Setup mixed precision training\n",
    "    scaler = torch.cuda.amp.GradScaler() if config[\"mixed_precision\"] else None\n",
    "    \n",
    "    # Setup early stopping\n",
    "    early_stopping = EarlyStopping(patience=config[\"early_stopping_patience\"])\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(config[\"epochs\"]):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        for i, batch in enumerate(train_dataloader):\n",
    "            input_features = batch[\"input_features\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            \n",
    "            # Mixed precision training\n",
    "            if config[\"mixed_precision\"]:\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    outputs = model(input_features, labels=labels)\n",
    "                    loss = outputs.loss / config[\"gradient_accumulation_steps\"]\n",
    "                \n",
    "                scaler.scale(loss).backward()\n",
    "                \n",
    "                if (i + 1) % config[\"gradient_accumulation_steps\"] == 0:\n",
    "                    scaler.unscale_(optimizer)\n",
    "                    clip_grad_norm_(model.parameters(), config[\"max_grad_norm\"])\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "                    scheduler.step()\n",
    "                    optimizer.zero_grad()\n",
    "            else:\n",
    "                outputs = model(input_features, labels=labels)\n",
    "                loss = outputs.loss / config[\"gradient_accumulation_steps\"]\n",
    "                loss.backward()\n",
    "                \n",
    "                if (i + 1) % config[\"gradient_accumulation_steps\"] == 0:\n",
    "                    clip_grad_norm_(model.parameters(), config[\"max_grad_norm\"])\n",
    "                    optimizer.step()\n",
    "                    scheduler.step()\n",
    "                    optimizer.zero_grad()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Log batch progress\n",
    "            if i % 10 == 0:\n",
    "                wandb.log({\n",
    "                    \"batch_loss\": loss.item(),\n",
    "                    \"learning_rate\": scheduler.get_last_lr()[0]\n",
    "                })\n",
    "        \n",
    "        # Validation phase with complete metrics\n",
    "        val_metrics = evaluate_model(model, processor, val_dataloader, device)\n",
    "        \n",
    "        # Log all metrics to WandB\n",
    "        wandb.log({\n",
    "            \"train_loss\": total_loss / len(train_dataloader),\n",
    "            \"val_loss\": val_metrics[\"loss\"],\n",
    "            \"val_wer\": val_metrics[\"WER\"],\n",
    "            \"val_cer\": val_metrics[\"CER\"],\n",
    "            \"val_precision\": val_metrics[\"Precision\"],\n",
    "            \"val_recall\": val_metrics[\"Recall\"],\n",
    "            \"val_f1\": val_metrics[\"F1-Score\"],\n",
    "            \"val_accuracy\": val_metrics[\"Accuracy\"],\n",
    "            \"epoch\": epoch + 1\n",
    "        })\n",
    "        \n",
    "        # Early stopping check\n",
    "        early_stopping(val_metrics[\"loss\"])\n",
    "        if early_stopping.should_stop:\n",
    "            print(f\"Early stopping triggered at epoch {epoch + 1}\")\n",
    "            break\n",
    "        \n",
    "        # Save checkpoint\n",
    "        if (epoch + 1) % config[\"checkpoint_interval\"] == 0:\n",
    "            checkpoint_dir = f\"/content/drive/MyDrive/whisper_checkpoints/checkpoint_epoch_{epoch + 1}\"\n",
    "            os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "            \n",
    "            # Save model and processor\n",
    "            model.save_pretrained(checkpoint_dir)\n",
    "            processor.save_pretrained(checkpoint_dir)\n",
    "            \n",
    "            # Save training state\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'metrics': val_metrics\n",
    "            }, os.path.join(checkpoint_dir, 'training_state.pt'))\n",
    "    \n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    drive.mount('/content/drive')\n",
    "    train_model(config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "proj-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
